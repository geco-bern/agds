<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Regression and classification | Applied Geodata Science</title>
  <meta name="description" content="This course prepares you to benefit from the general data richness in environmental and geo-sciences." />
  <meta name="generator" content="bookdown 0.31 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Regression and classification | Applied Geodata Science" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This course prepares you to benefit from the general data richness in environmental and geo-sciences." />
  <meta name="github-repo" content="stineb/agsd" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Regression and classification | Applied Geodata Science" />
  
  <meta name="twitter:description" content="This course prepares you to benefit from the general data richness in environmental and geo-sciences." />
  

<meta name="author" content="Benjamin Stocker (lead), Koen Hufkens (contributing), Pepa Arán (contributing), Pascal Schneider (contributing)" />


<meta name="date" content="2023-02-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="figures/apple-touch-icon.png" />
  <link rel="shortcut icon" href="figures/favicon.ico" type="image/x-icon" />
<link rel="prev" href="codemgmt.html"/>
<link rel="next" href="supervisedmli.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Applied Geodata Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Overview</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-this-book"><i class="fa fa-check"></i>About this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-this-course"><i class="fa fa-check"></i>About this course</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-goal"><i class="fa fa-check"></i>Course goal</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#learning-objectives"><i class="fa fa-check"></i>Learning Objectives</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-contents"><i class="fa fa-check"></i>Course contents</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#what-is-applied-geodata-science"><i class="fa fa-check"></i>What is Applied Geodata Science?</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#the-data-science-workflow"><i class="fa fa-check"></i>The data science workflow</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#why-now"><i class="fa fa-check"></i>Why now?</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#a-new-modelling-paradigm"><i class="fa fa-check"></i>A new modelling paradigm</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#reading-and-link-collection"><i class="fa fa-check"></i>Reading and link collection</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="gettingstarted.html"><a href="gettingstarted.html"><i class="fa fa-check"></i><b>1</b> Getting started</a>
<ul>
<li class="chapter" data-level="1.1" data-path="gettingstarted.html"><a href="gettingstarted.html#learning-objectives-1"><i class="fa fa-check"></i><b>1.1</b> Learning objectives</a></li>
<li class="chapter" data-level="1.2" data-path="gettingstarted.html"><a href="gettingstarted.html#tutorial"><i class="fa fa-check"></i><b>1.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="gettingstarted.html"><a href="gettingstarted.html#working-with-r-and-rstudio"><i class="fa fa-check"></i><b>1.2.1</b> Working with R and RStudio</a></li>
<li class="chapter" data-level="1.2.2" data-path="gettingstarted.html"><a href="gettingstarted.html#r-objects"><i class="fa fa-check"></i><b>1.2.2</b> R objects</a></li>
<li class="chapter" data-level="1.2.3" data-path="gettingstarted.html"><a href="gettingstarted.html#r-environment"><i class="fa fa-check"></i><b>1.2.3</b> R environment</a></li>
<li class="chapter" data-level="1.2.4" data-path="gettingstarted.html"><a href="gettingstarted.html#libraries"><i class="fa fa-check"></i><b>1.2.4</b> Libraries</a></li>
<li class="chapter" data-level="1.2.5" data-path="gettingstarted.html"><a href="gettingstarted.html#r-scripts"><i class="fa fa-check"></i><b>1.2.5</b> R scripts</a></li>
<li class="chapter" data-level="1.2.6" data-path="gettingstarted.html"><a href="gettingstarted.html#rmarkdown"><i class="fa fa-check"></i><b>1.2.6</b> R Markdown</a></li>
<li class="chapter" data-level="1.2.7" data-path="gettingstarted.html"><a href="gettingstarted.html#wspmgmt"><i class="fa fa-check"></i><b>1.2.7</b> Workspace management</a></li>
<li class="chapter" data-level="1.2.8" data-path="gettingstarted.html"><a href="gettingstarted.html#setup"><i class="fa fa-check"></i><b>1.2.8</b> Setup</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="gettingstarted.html"><a href="gettingstarted.html#exercises"><i class="fa fa-check"></i><b>1.3</b> Exercises</a>
<ul>
<li class="chapter" data-level="" data-path="gettingstarted.html"><a href="gettingstarted.html#dimensions-of-a-circle"><i class="fa fa-check"></i>Dimensions of a circle</a></li>
<li class="chapter" data-level="" data-path="gettingstarted.html"><a href="gettingstarted.html#sequence-of-numbers"><i class="fa fa-check"></i>Sequence of numbers</a></li>
<li class="chapter" data-level="" data-path="gettingstarted.html"><a href="gettingstarted.html#gauss-sum"><i class="fa fa-check"></i>Gauss sum</a></li>
<li class="chapter" data-level="" data-path="gettingstarted.html"><a href="gettingstarted.html#magic-trick-algorithm"><i class="fa fa-check"></i>Magic trick algorithm</a></li>
<li class="chapter" data-level="" data-path="gettingstarted.html"><a href="gettingstarted.html#vectors-1"><i class="fa fa-check"></i>Vectors</a></li>
<li class="chapter" data-level="" data-path="gettingstarted.html"><a href="gettingstarted.html#data-frames-1"><i class="fa fa-check"></i>Data frames</a></li>
<li class="chapter" data-level="" data-path="gettingstarted.html"><a href="gettingstarted.html#workspace"><i class="fa fa-check"></i>Workspace</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="programmingprimers.html"><a href="programmingprimers.html"><i class="fa fa-check"></i><b>2</b> Programming primers</a>
<ul>
<li class="chapter" data-level="2.1" data-path="programmingprimers.html"><a href="programmingprimers.html#learning-objectives-2"><i class="fa fa-check"></i><b>2.1</b> Learning objectives</a></li>
<li class="chapter" data-level="2.2" data-path="programmingprimers.html"><a href="programmingprimers.html#tutorial-1"><i class="fa fa-check"></i><b>2.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="programmingprimers.html"><a href="programmingprimers.html#programming-basics"><i class="fa fa-check"></i><b>2.2.1</b> Programming basics</a></li>
<li class="chapter" data-level="2.2.2" data-path="programmingprimers.html"><a href="programmingprimers.html#style-your-code"><i class="fa fa-check"></i><b>2.2.2</b> Style your code</a></li>
<li class="chapter" data-level="2.2.3" data-path="programmingprimers.html"><a href="programmingprimers.html#findinghelp"><i class="fa fa-check"></i><b>2.2.3</b> Where to find help</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="programmingprimers.html"><a href="programmingprimers.html#exercises-1"><i class="fa fa-check"></i><b>2.3</b> Exercises</a>
<ul>
<li class="chapter" data-level="" data-path="programmingprimers.html"><a href="programmingprimers.html#gauss-variations"><i class="fa fa-check"></i>Gauss variations</a></li>
<li class="chapter" data-level="" data-path="programmingprimers.html"><a href="programmingprimers.html#nested-loops"><i class="fa fa-check"></i>Nested loops</a></li>
<li class="chapter" data-level="" data-path="programmingprimers.html"><a href="programmingprimers.html#interpolation"><i class="fa fa-check"></i>Interpolation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="datawrangling.html"><a href="datawrangling.html"><i class="fa fa-check"></i><b>3</b> Data wrangling</a>
<ul>
<li class="chapter" data-level="3.1" data-path="datawrangling.html"><a href="datawrangling.html#learning-objectives-3"><i class="fa fa-check"></i><b>3.1</b> Learning objectives</a></li>
<li class="chapter" data-level="3.2" data-path="datawrangling.html"><a href="datawrangling.html#tutorial-2"><i class="fa fa-check"></i><b>3.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="datawrangling.html"><a href="datawrangling.html#example-data"><i class="fa fa-check"></i><b>3.2.1</b> Example data</a></li>
<li class="chapter" data-level="3.2.2" data-path="datawrangling.html"><a href="datawrangling.html#tidyverse"><i class="fa fa-check"></i><b>3.2.2</b> Tidyverse</a></li>
<li class="chapter" data-level="3.2.3" data-path="datawrangling.html"><a href="datawrangling.html#reading-tabular-data"><i class="fa fa-check"></i><b>3.2.3</b> Reading tabular data</a></li>
<li class="chapter" data-level="3.2.4" data-path="datawrangling.html"><a href="datawrangling.html#variable-selection"><i class="fa fa-check"></i><b>3.2.4</b> Variable selection</a></li>
<li class="chapter" data-level="3.2.5" data-path="datawrangling.html"><a href="datawrangling.html#time-objects"><i class="fa fa-check"></i><b>3.2.5</b> Time objects</a></li>
<li class="chapter" data-level="3.2.6" data-path="datawrangling.html"><a href="datawrangling.html#variable-re--definition"><i class="fa fa-check"></i><b>3.2.6</b> Variable (re-) definition</a></li>
<li class="chapter" data-level="3.2.7" data-path="datawrangling.html"><a href="datawrangling.html#axes-of-variation"><i class="fa fa-check"></i><b>3.2.7</b> Axes of variation</a></li>
<li class="chapter" data-level="3.2.8" data-path="datawrangling.html"><a href="datawrangling.html#tidydata"><i class="fa fa-check"></i><b>3.2.8</b> Tidy data</a></li>
<li class="chapter" data-level="3.2.9" data-path="datawrangling.html"><a href="datawrangling.html#aggregating-data"><i class="fa fa-check"></i><b>3.2.9</b> Aggregating data</a></li>
<li class="chapter" data-level="3.2.10" data-path="datawrangling.html"><a href="datawrangling.html#data-cleaning"><i class="fa fa-check"></i><b>3.2.10</b> Data cleaning</a></li>
<li class="chapter" data-level="3.2.11" data-path="datawrangling.html"><a href="datawrangling.html#writing-data-to-csv"><i class="fa fa-check"></i><b>3.2.11</b> Writing data to CSV</a></li>
<li class="chapter" data-level="3.2.12" data-path="datawrangling.html"><a href="datawrangling.html#combining-relational-data"><i class="fa fa-check"></i><b>3.2.12</b> Combining relational data</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="datawrangling.html"><a href="datawrangling.html#extramaterialwrangling"><i class="fa fa-check"></i><b>3.3</b> Extra material</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="datawrangling.html"><a href="datawrangling.html#functional-programming-i"><i class="fa fa-check"></i><b>3.3.1</b> Functional programming I</a></li>
<li class="chapter" data-level="3.3.2" data-path="datawrangling.html"><a href="datawrangling.html#strings"><i class="fa fa-check"></i><b>3.3.2</b> Strings</a></li>
<li class="chapter" data-level="3.3.3" data-path="datawrangling.html"><a href="datawrangling.html#functional-programming-ii"><i class="fa fa-check"></i><b>3.3.3</b> Functional programming II</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="datawrangling.html"><a href="datawrangling.html#exerciseswrangling"><i class="fa fa-check"></i><b>3.4</b> Exercises</a>
<ul>
<li class="chapter" data-level="" data-path="datawrangling.html"><a href="datawrangling.html#star-wars"><i class="fa fa-check"></i>Star wars</a></li>
<li class="chapter" data-level="" data-path="datawrangling.html"><a href="datawrangling.html#aggregating"><i class="fa fa-check"></i>Aggregating</a></li>
<li class="chapter" data-level="" data-path="datawrangling.html"><a href="datawrangling.html#patterns-in-data-quality"><i class="fa fa-check"></i>Patterns in data quality</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="datawrangling.html"><a href="datawrangling.html#report-exercise"><i class="fa fa-check"></i><b>3.5</b> Report Exercise</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="datavis.html"><a href="datavis.html"><i class="fa fa-check"></i><b>4</b> Data visualisation</a>
<ul>
<li class="chapter" data-level="4.1" data-path="datavis.html"><a href="datavis.html#learning-objectives-4"><i class="fa fa-check"></i><b>4.1</b> Learning objectives</a></li>
<li class="chapter" data-level="4.2" data-path="datavis.html"><a href="datavis.html#tutorial-3"><i class="fa fa-check"></i><b>4.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="datavis.html"><a href="datavis.html#the-grammar-of-graphics"><i class="fa fa-check"></i><b>4.2.1</b> The grammar of graphics</a></li>
<li class="chapter" data-level="4.2.2" data-path="datavis.html"><a href="datavis.html#every-data-has-its-representation"><i class="fa fa-check"></i><b>4.2.2</b> Every data has its representation</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="datavis.html"><a href="datavis.html#exercises-2"><i class="fa fa-check"></i><b>4.3</b> Exercises</a>
<ul>
<li class="chapter" data-level="" data-path="datavis.html"><a href="datavis.html#identifying-outliers"><i class="fa fa-check"></i>Identifying Outliers</a></li>
<li class="chapter" data-level="" data-path="datavis.html"><a href="datavis.html#visualising-diurnal-and-seasonal-cycles-of-gpp"><i class="fa fa-check"></i>Visualising diurnal and seasonal cycles of GPP</a></li>
<li class="chapter" data-level="" data-path="datavis.html"><a href="datavis.html#trend-in-carbon-dioxide-concentrations"><i class="fa fa-check"></i>Trend in carbon dioxide concentrations</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="datavis.html"><a href="datavis.html#report-exercises"><i class="fa fa-check"></i><b>4.4</b> Report Exercises</a>
<ul>
<li class="chapter" data-level="" data-path="datavis.html"><a href="datavis.html#telling-a-story-from-data"><i class="fa fa-check"></i>Telling a story from data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="datavariety.html"><a href="datavariety.html"><i class="fa fa-check"></i><b>5</b> Data variety</a>
<ul>
<li class="chapter" data-level="5.1" data-path="datavariety.html"><a href="datavariety.html#learning-objectives-5"><i class="fa fa-check"></i><b>5.1</b> Learning objectives</a></li>
<li class="chapter" data-level="5.2" data-path="datavariety.html"><a href="datavariety.html#tutorial-4"><i class="fa fa-check"></i><b>5.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="datavariety.html"><a href="datavariety.html#files-and-file-formats"><i class="fa fa-check"></i><b>5.2.1</b> Files and file formats</a></li>
<li class="chapter" data-level="5.2.2" data-path="datavariety.html"><a href="datavariety.html#meta-data"><i class="fa fa-check"></i><b>5.2.2</b> Meta-data</a></li>
<li class="chapter" data-level="5.2.3" data-path="datavariety.html"><a href="datavariety.html#spatial-data-representation"><i class="fa fa-check"></i><b>5.2.3</b> Spatial data representation</a></li>
<li class="chapter" data-level="5.2.4" data-path="datavariety.html"><a href="datavariety.html#online-data-sources"><i class="fa fa-check"></i><b>5.2.4</b> Online data sources</a></li>
<li class="chapter" data-level="5.2.5" data-path="datavariety.html"><a href="datavariety.html#example-environmental-data-repositories"><i class="fa fa-check"></i><b>5.2.5</b> Example environmental data repositories</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="datavariety.html"><a href="datavariety.html#exercises-3"><i class="fa fa-check"></i><b>5.3</b> Exercises</a>
<ul>
<li class="chapter" data-level="" data-path="datavariety.html"><a href="datavariety.html#files-and-file-formats-1"><i class="fa fa-check"></i>Files and file formats</a></li>
<li class="chapter" data-level="" data-path="datavariety.html"><a href="datavariety.html#api-use"><i class="fa fa-check"></i>API use</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="openscience.html"><a href="openscience.html"><i class="fa fa-check"></i><b>6</b> Open science practices</a>
<ul>
<li class="chapter" data-level="6.1" data-path="openscience.html"><a href="openscience.html#learning-objectives-6"><i class="fa fa-check"></i><b>6.1</b> Learning objectives</a></li>
<li class="chapter" data-level="6.2" data-path="openscience.html"><a href="openscience.html#tutorial-5"><i class="fa fa-check"></i><b>6.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="openscience.html"><a href="openscience.html#project-structure"><i class="fa fa-check"></i><b>6.2.1</b> Project structure</a></li>
<li class="chapter" data-level="6.2.2" data-path="openscience.html"><a href="openscience.html#managing-workflows"><i class="fa fa-check"></i><b>6.2.2</b> Managing workflows</a></li>
<li class="chapter" data-level="6.2.3" data-path="openscience.html"><a href="openscience.html#capturing-your-session-state"><i class="fa fa-check"></i><b>6.2.3</b> Capturing your session state</a></li>
<li class="chapter" data-level="6.2.4" data-path="openscience.html"><a href="openscience.html#capturing-a-system-state"><i class="fa fa-check"></i><b>6.2.4</b> Capturing a system state</a></li>
<li class="chapter" data-level="6.2.5" data-path="openscience.html"><a href="openscience.html#readable-reporting-using-rmarkdown"><i class="fa fa-check"></i><b>6.2.5</b> Readable reporting using Rmarkdown</a></li>
<li class="chapter" data-level="6.2.6" data-path="openscience.html"><a href="openscience.html#data-retention"><i class="fa fa-check"></i><b>6.2.6</b> Data retention</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="openscience.html"><a href="openscience.html#exercises-4"><i class="fa fa-check"></i><b>6.3</b> Exercises</a>
<ul>
<li class="chapter" data-level="" data-path="openscience.html"><a href="openscience.html#external-data"><i class="fa fa-check"></i>External data</a></li>
<li class="chapter" data-level="" data-path="openscience.html"><a href="openscience.html#a-new-project"><i class="fa fa-check"></i>A new project</a></li>
<li class="chapter" data-level="" data-path="openscience.html"><a href="openscience.html#tracking-the-state-of-your-project"><i class="fa fa-check"></i>Tracking the state of your project</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="codemgmt.html"><a href="codemgmt.html"><i class="fa fa-check"></i><b>7</b> Code management</a>
<ul>
<li class="chapter" data-level="7.1" data-path="codemgmt.html"><a href="codemgmt.html#learning-objectives-7"><i class="fa fa-check"></i><b>7.1</b> Learning objectives</a></li>
<li class="chapter" data-level="7.2" data-path="codemgmt.html"><a href="codemgmt.html#tutorial-6"><i class="fa fa-check"></i><b>7.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="codemgmt.html"><a href="codemgmt.html#gitworkflow"><i class="fa fa-check"></i><b>7.2.1</b> Git and local version control</a></li>
<li class="chapter" data-level="7.2.2" data-path="codemgmt.html"><a href="codemgmt.html#remote-version-control"><i class="fa fa-check"></i><b>7.2.2</b> Remote version control</a></li>
<li class="chapter" data-level="7.2.3" data-path="codemgmt.html"><a href="codemgmt.html#location-based-code-management---github-templates"><i class="fa fa-check"></i><b>7.2.3</b> Location based code management - github templates</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="codemgmt.html"><a href="codemgmt.html#exercises-5"><i class="fa fa-check"></i><b>7.3</b> Exercises</a>
<ul>
<li class="chapter" data-level="" data-path="codemgmt.html"><a href="codemgmt.html#location-based-code-management"><i class="fa fa-check"></i>Location based code management</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="codemgmt.html"><a href="codemgmt.html#report-exercises-1"><i class="fa fa-check"></i><b>7.4</b> Report Exercises</a>
<ul>
<li class="chapter" data-level="" data-path="codemgmt.html"><a href="codemgmt.html#collaborative-work-on-github"><i class="fa fa-check"></i>Collaborative Work on Github</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regressionclassification.html"><a href="regressionclassification.html"><i class="fa fa-check"></i><b>8</b> Regression and classification</a>
<ul>
<li class="chapter" data-level="8.1" data-path="regressionclassification.html"><a href="regressionclassification.html#learning-objectives-8"><i class="fa fa-check"></i><b>8.1</b> Learning objectives</a></li>
<li class="chapter" data-level="8.2" data-path="regressionclassification.html"><a href="regressionclassification.html#tutorial-7"><i class="fa fa-check"></i><b>8.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="regressionclassification.html"><a href="regressionclassification.html#types-of-models"><i class="fa fa-check"></i><b>8.2.1</b> Types of models</a></li>
<li class="chapter" data-level="8.2.2" data-path="regressionclassification.html"><a href="regressionclassification.html#regression"><i class="fa fa-check"></i><b>8.2.2</b> Regression</a></li>
<li class="chapter" data-level="8.2.3" data-path="regressionclassification.html"><a href="regressionclassification.html#classification"><i class="fa fa-check"></i><b>8.2.3</b> Classification</a></li>
<li class="chapter" data-level="8.2.4" data-path="regressionclassification.html"><a href="regressionclassification.html#model-evaluation"><i class="fa fa-check"></i><b>8.2.4</b> Model evaluation</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="regressionclassification.html"><a href="regressionclassification.html#exercises-6"><i class="fa fa-check"></i><b>8.3</b> Exercises</a></li>
<li class="chapter" data-level="8.4" data-path="regressionclassification.html"><a href="regressionclassification.html#report-exercise-1"><i class="fa fa-check"></i><b>8.4</b> Report Exercise</a>
<ul>
<li class="chapter" data-level="" data-path="regressionclassification.html"><a href="regressionclassification.html#stepwise-regression-and-asking-for-help"><i class="fa fa-check"></i>Stepwise regression and asking for help</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="supervisedmli.html"><a href="supervisedmli.html"><i class="fa fa-check"></i><b>9</b> Supervised machine learning I</a>
<ul>
<li class="chapter" data-level="9.1" data-path="supervisedmli.html"><a href="supervisedmli.html#learning-objectives-9"><i class="fa fa-check"></i><b>9.1</b> Learning objectives</a></li>
<li class="chapter" data-level="9.2" data-path="supervisedmli.html"><a href="supervisedmli.html#tutorial-8"><i class="fa fa-check"></i><b>9.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="supervisedmli.html"><a href="supervisedmli.html#what-is-supervised-machine-learning"><i class="fa fa-check"></i><b>9.2.1</b> What is supervised machine learning?</a></li>
<li class="chapter" data-level="9.2.2" data-path="supervisedmli.html"><a href="supervisedmli.html#overfitting"><i class="fa fa-check"></i><b>9.2.2</b> Overfitting</a></li>
<li class="chapter" data-level="9.2.3" data-path="supervisedmli.html"><a href="supervisedmli.html#data-and-the-modelling-challenge"><i class="fa fa-check"></i><b>9.2.3</b> Data and the modelling challenge</a></li>
<li class="chapter" data-level="9.2.4" data-path="supervisedmli.html"><a href="supervisedmli.html#k-nearest-neighbours"><i class="fa fa-check"></i><b>9.2.4</b> K-nearest neighbours</a></li>
<li class="chapter" data-level="9.2.5" data-path="supervisedmli.html"><a href="supervisedmli.html#model-formulation"><i class="fa fa-check"></i><b>9.2.5</b> Model formulation</a></li>
<li class="chapter" data-level="9.2.6" data-path="supervisedmli.html"><a href="supervisedmli.html#data-splitting"><i class="fa fa-check"></i><b>9.2.6</b> Data splitting</a></li>
<li class="chapter" data-level="9.2.7" data-path="supervisedmli.html"><a href="supervisedmli.html#preprocessing"><i class="fa fa-check"></i><b>9.2.7</b> Pre-processing</a></li>
<li class="chapter" data-level="9.2.8" data-path="supervisedmli.html"><a href="supervisedmli.html#putting-it-all-together-half-way"><i class="fa fa-check"></i><b>9.2.8</b> Putting it all together (half-way)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="supervisedmli.html"><a href="supervisedmli.html#exercises-7"><i class="fa fa-check"></i><b>9.3</b> Exercises</a></li>
<li class="chapter" data-level="9.4" data-path="supervisedmli.html"><a href="supervisedmli.html#report-exercises-2"><i class="fa fa-check"></i><b>9.4</b> Report Exercises</a>
<ul>
<li class="chapter" data-level="" data-path="supervisedmli.html"><a href="supervisedmli.html#the-role-of-picking-a-hyperparameter"><i class="fa fa-check"></i>The role of picking a hyperparameter</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="supervisedmlii.html"><a href="supervisedmlii.html"><i class="fa fa-check"></i><b>10</b> Supervised machine learning II</a>
<ul>
<li class="chapter" data-level="10.1" data-path="supervisedmlii.html"><a href="supervisedmlii.html#learning-objectives-10"><i class="fa fa-check"></i><b>10.1</b> Learning objectives</a></li>
<li class="chapter" data-level="10.2" data-path="supervisedmlii.html"><a href="supervisedmlii.html#tutorial-9"><i class="fa fa-check"></i><b>10.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="supervisedmlii.html"><a href="supervisedmlii.html#data-and-the-modelling-challenge-1"><i class="fa fa-check"></i><b>10.2.1</b> Data and the modelling challenge</a></li>
<li class="chapter" data-level="10.2.2" data-path="supervisedmlii.html"><a href="supervisedmlii.html#training"><i class="fa fa-check"></i><b>10.2.2</b> Loss function</a></li>
<li class="chapter" data-level="10.2.3" data-path="supervisedmlii.html"><a href="supervisedmlii.html#hyperparameters"><i class="fa fa-check"></i><b>10.2.3</b> Hyperparameters</a></li>
<li class="chapter" data-level="10.2.4" data-path="supervisedmlii.html"><a href="supervisedmlii.html#resampling"><i class="fa fa-check"></i><b>10.2.4</b> Resampling</a></li>
<li class="chapter" data-level="10.2.5" data-path="supervisedmlii.html"><a href="supervisedmlii.html#validation-versus-testing-data"><i class="fa fa-check"></i><b>10.2.5</b> Validation versus testing data</a></li>
<li class="chapter" data-level="10.2.6" data-path="supervisedmlii.html"><a href="supervisedmlii.html#modeling-with-structured-data"><i class="fa fa-check"></i><b>10.2.6</b> Modeling with structured data</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="supervisedmlii.html"><a href="supervisedmlii.html#exercises-8"><i class="fa fa-check"></i><b>10.3</b> Exercises</a>
<ul>
<li class="chapter" data-level="" data-path="supervisedmlii.html"><a href="supervisedmlii.html#cross-validation-by-hand"><i class="fa fa-check"></i>Cross-validation by hand</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="supervisedmlii.html"><a href="supervisedmlii.html#report-exercises-3"><i class="fa fa-check"></i><b>10.4</b> Report Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="randomforest.html"><a href="randomforest.html"><i class="fa fa-check"></i><b>11</b> Random Forest</a>
<ul>
<li class="chapter" data-level="11.1" data-path="randomforest.html"><a href="randomforest.html#learning-objectives-11"><i class="fa fa-check"></i><b>11.1</b> Learning objectives</a></li>
<li class="chapter" data-level="11.2" data-path="randomforest.html"><a href="randomforest.html#tutorial-10"><i class="fa fa-check"></i><b>11.2</b> Tutorial</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="randomforest.html"><a href="randomforest.html#decision-trees"><i class="fa fa-check"></i><b>11.2.1</b> Decision trees</a></li>
<li class="chapter" data-level="11.2.2" data-path="randomforest.html"><a href="randomforest.html#bagging"><i class="fa fa-check"></i><b>11.2.2</b> Bagging</a></li>
<li class="chapter" data-level="11.2.3" data-path="randomforest.html"><a href="randomforest.html#from-trees-to-a-forest"><i class="fa fa-check"></i><b>11.2.3</b> From trees to a forest</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="solutions.html"><a href="solutions.html"><i class="fa fa-check"></i><b>A</b> Solutions</a>
<ul>
<li class="chapter" data-level="A.1" data-path="solutions.html"><a href="solutions.html#getting-started"><i class="fa fa-check"></i><b>A.1</b> Getting Started</a>
<ul>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#dimensions-of-a-circle-1"><i class="fa fa-check"></i>Dimensions of a circle</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#sequence-of-numbers-1"><i class="fa fa-check"></i>Sequence of numbers</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#gauss-sum-1"><i class="fa fa-check"></i>Gauss sum</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#magic-trick-algorithm-1"><i class="fa fa-check"></i>Magic trick algorithm</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#vectors-2"><i class="fa fa-check"></i>Vectors</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#data-frames-2"><i class="fa fa-check"></i>Data frames</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#workspace-1"><i class="fa fa-check"></i>Workspace</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="solutions.html"><a href="solutions.html#programming-primers"><i class="fa fa-check"></i><b>A.2</b> Programming primers</a>
<ul>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#gauss-variations-1"><i class="fa fa-check"></i>Gauss variations</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#nested-loops-1"><i class="fa fa-check"></i>Nested loops</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#interpolation-1"><i class="fa fa-check"></i>Interpolation</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="solutions.html"><a href="solutions.html#data-wrangling"><i class="fa fa-check"></i><b>A.3</b> Data wrangling</a>
<ul>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#star-wars-1"><i class="fa fa-check"></i>Star wars</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#aggregating-1"><i class="fa fa-check"></i>Aggregating</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#patterns-in-data-quality-1"><i class="fa fa-check"></i>Patterns in data quality</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#tidying-an-excel-file"><i class="fa fa-check"></i>Tidying an Excel-file</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="solutions.html"><a href="solutions.html#data-visualisation"><i class="fa fa-check"></i><b>A.4</b> Data Visualisation</a>
<ul>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#identifying-outliers-1"><i class="fa fa-check"></i>Identifying Outliers</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#diurnal-and-seasonal-cycles"><i class="fa fa-check"></i>Diurnal and seasonal cycles</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#trend-in-carbon-dioxide-concentrations-1"><i class="fa fa-check"></i>Trend in carbon dioxide concentrations</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#telling-a-story-from-data-1"><i class="fa fa-check"></i>Telling a story from data</a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="solutions.html"><a href="solutions.html#data-variety"><i class="fa fa-check"></i><b>A.5</b> Data Variety</a>
<ul>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#files-and-file-formats-2"><i class="fa fa-check"></i>Files and file formats</a></li>
</ul></li>
<li class="chapter" data-level="A.6" data-path="solutions.html"><a href="solutions.html#open-science"><i class="fa fa-check"></i><b>A.6</b> Open Science</a>
<ul>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#external-data-1"><i class="fa fa-check"></i>External data</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#a-new-project-1"><i class="fa fa-check"></i>A new project</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#tracking-the-state-of-your-project-1"><i class="fa fa-check"></i>Tracking the state of your project</a></li>
</ul></li>
<li class="chapter" data-level="A.7" data-path="solutions.html"><a href="solutions.html#code-management"><i class="fa fa-check"></i><b>A.7</b> Code Management</a>
<ul>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#location-based-code-management-1"><i class="fa fa-check"></i>Location based code management</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#collaborative-work-on-github-1"><i class="fa fa-check"></i>Collaborative Work on Github</a></li>
</ul></li>
<li class="chapter" data-level="A.8" data-path="solutions.html"><a href="solutions.html#regression-and-classification"><i class="fa fa-check"></i><b>A.8</b> Regression and Classification</a>
<ul>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#stepwise-regression-and-asking-for-help-1"><i class="fa fa-check"></i>Stepwise regression and asking for help</a></li>
</ul></li>
<li class="chapter" data-level="A.9" data-path="solutions.html"><a href="solutions.html#supervised-ml-i"><i class="fa fa-check"></i><b>A.9</b> Supervised ML I</a>
<ul>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#comparison-of-lm-and-knn"><i class="fa fa-check"></i>Comparison of LM and KNN</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#the-role-of-picking-a-hyperparameter-1"><i class="fa fa-check"></i>The role of picking a hyperparameter</a></li>
</ul></li>
<li class="chapter" data-level="A.10" data-path="solutions.html"><a href="solutions.html#supervised-ml-ii"><i class="fa fa-check"></i><b>A.10</b> Supervised ML II</a>
<ul>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#cross-validation-by-hand-1"><i class="fa fa-check"></i>Cross-validation by hand</a></li>
<li class="chapter" data-level="" data-path="solutions.html"><a href="solutions.html#testing-the-generalisability-of-a-model"><i class="fa fa-check"></i>Testing the generalisability of a model</a></li>
</ul></li>
<li class="chapter" data-level="A.11" data-path="solutions.html"><a href="solutions.html#random-forests"><i class="fa fa-check"></i><b>A.11</b> Random Forests</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>B</b> References</a>
<ul>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html#system-information-and-package-list"><i class="fa fa-check"></i>System information and package list</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied Geodata Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regressionclassification" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">Chapter 8</span> Regression and classification<a href="regressionclassification.html#regressionclassification" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><strong>Chapter lead author: Pepa Aran</strong></p>
<div id="learning-objectives-8" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> Learning objectives<a href="regressionclassification.html#learning-objectives-8" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>After completing this tutorial, you will be able to:</p>
<ul>
<li>Understand the basics of regression and classification models</li>
<li>Fit linear and logistic regression models in R</li>
<li>Choose and calculate relevant model performance metrics</li>
<li>Evaluate and compare regression models</li>
<li>Detect data outliers</li>
<li>Select best predictive variables</li>
</ul>
<p>Contents of this Chapter are inspired and partly adopted by the
excellent book <a href="https://bradleyboehmke.github.io/HOML/">Hands-On Machine Learning in R by Boehmke &amp;
Greenwell</a>.</p>
</div>
<div id="tutorial-7" class="section level2 hasAnchor" number="8.2">
<h2><span class="header-section-number">8.2</span> Tutorial<a href="regressionclassification.html#tutorial-7" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="types-of-models" class="section level3 hasAnchor" number="8.2.1">
<h3><span class="header-section-number">8.2.1</span> Types of models<a href="regressionclassification.html#types-of-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Models try to explain relationships between variables through a
mathematical formulation, particularly to predict a given target
variable using other explanatory variables, also called predictors.
Generally, we say that the target variable <span class="math inline">\(Y\)</span> is a function (denoted
<span class="math inline">\(f\)</span>) of a set of explanatory variables <span class="math inline">\(X_1, X_2, \dots, X_p\)</span> and some
model parameters <span class="math inline">\(\beta\)</span>. Models can be represented as:
<span class="math display">\[Y \sim f(X_1, X_2, \dots, X_p, \beta)\]</span></p>
<p>This is a very general notation and depending on the structure of these
components, we get to different modelling approaches.</p>
<p>The first distinction comes from the type of target variable. Whenever
<span class="math inline">\(Y\)</span> is a continuous variable, we are facing a <em>regression</em> problem. If
<span class="math inline">\(Y\)</span> is categorical, we talk about <em>classification</em>.</p>
<table>
<colgroup>
<col width="30%" />
<col width="34%" />
<col width="34%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Regression</th>
<th>Classification</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Target variable</td>
<td>Continuous</td>
<td>Categorical</td>
</tr>
<tr class="even">
<td>Common models</td>
<td>Linear regression, polynomial regression, KNN, tree-based regression</td>
<td>Logistic regression, KNN, SVM, tree classifiers</td>
</tr>
<tr class="odd">
<td>Metrics</td>
<td>RMSE, <span class="math inline">\(R^2\)</span>, adjusted <span class="math inline">\(R^2\)</span>, AIC, BIC</td>
<td>Accuracy, precision, AUC, F1</td>
</tr>
</tbody>
</table>
</div>
<div id="regression" class="section level3 hasAnchor" number="8.2.2">
<h3><span class="header-section-number">8.2.2</span> Regression<a href="regressionclassification.html#regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, we will introduce the most basic regression model:
linear regression. We’ll explain how to fit the model with R, how to
include categorical predictors and polynomial terms. Finally, several
performance metrics for regression models are presented.</p>
<div id="linear-regression" class="section level4 hasAnchor" number="8.2.2.1">
<h4><span class="header-section-number">8.2.2.1</span> Linear regression<a href="regressionclassification.html#linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Theory</strong></p>
<p>Let’s start with the simplest model: linear regression. You probably
have studied linear regression from a statistical perspective, here we
will take a data-fitting approach.</p>
<p>For example, we can try to explain the relationship between GPP and
short wave radiation, like in the Chapter <a href="datavis.html#datavis">4</a>. The figure
below shows a cloud of data points, and a straight line predicting GPP
based on observed shortwave radiation values.</p>
<div class="sourceCode" id="cb288"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb288-1"><a href="regressionclassification.html#cb288-1" aria-hidden="true" tabindex="-1"></a><span class="co"># read and format data from Ch 3</span></span>
<span id="cb288-2"><a href="regressionclassification.html#cb288-2" aria-hidden="true" tabindex="-1"></a>half_hourly_fluxes <span class="ot">&lt;-</span> readr<span class="sc">::</span><span class="fu">read_csv</span>(<span class="st">&quot;./data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006_CLEAN.csv&quot;</span>)</span>
<span id="cb288-3"><a href="regressionclassification.html#cb288-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb288-4"><a href="regressionclassification.html#cb288-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2023</span>)</span>
<span id="cb288-5"><a href="regressionclassification.html#cb288-5" aria-hidden="true" tabindex="-1"></a>plot_1 <span class="ot">&lt;-</span> half_hourly_fluxes <span class="sc">|&gt;</span></span>
<span id="cb288-6"><a href="regressionclassification.html#cb288-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sample_n</span>(<span class="dv">2000</span>) <span class="sc">|&gt;</span>  <span class="co"># to reduce the dataset</span></span>
<span id="cb288-7"><a href="regressionclassification.html#cb288-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> SW_IN_F, <span class="at">y =</span> GPP_NT_VUT_REF)) <span class="sc">+</span></span>
<span id="cb288-8"><a href="regressionclassification.html#cb288-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.75</span>, <span class="at">alpha=</span><span class="fl">0.4</span>) <span class="sc">+</span></span>
<span id="cb288-9"><a href="regressionclassification.html#cb288-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb288-10"><a href="regressionclassification.html#cb288-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;Shortwave radiation (W m&quot;</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">&quot;)&quot;</span>)), </span>
<span id="cb288-11"><a href="regressionclassification.html#cb288-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;GPP (gC m&quot;</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">&quot;s&quot;</span><span class="sc">^-</span><span class="dv">1</span>, <span class="st">&quot;)&quot;</span>))) <span class="sc">+</span></span>
<span id="cb288-12"><a href="regressionclassification.html#cb288-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>()</span>
<span id="cb288-13"><a href="regressionclassification.html#cb288-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb288-14"><a href="regressionclassification.html#cb288-14" aria-hidden="true" tabindex="-1"></a>segment_points <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x0 =</span> <span class="dv">332</span>, <span class="at">y0 =</span> <span class="fl">3.65</span>, <span class="at">y_regr =</span> <span class="fl">8.77</span>)</span>
<span id="cb288-15"><a href="regressionclassification.html#cb288-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb288-16"><a href="regressionclassification.html#cb288-16" aria-hidden="true" tabindex="-1"></a>plot_1 <span class="sc">+</span></span>
<span id="cb288-17"><a href="regressionclassification.html#cb288-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="fu">aes</span>(<span class="at">x =</span> x0, <span class="at">y =</span> y0, <span class="at">xend =</span> x0, <span class="at">yend =</span> y_regr), </span>
<span id="cb288-18"><a href="regressionclassification.html#cb288-18" aria-hidden="true" tabindex="-1"></a>               <span class="at">data =</span> segment_points,</span>
<span id="cb288-19"><a href="regressionclassification.html#cb288-19" aria-hidden="true" tabindex="-1"></a>               <span class="at">color =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="fl">1.2</span>, <span class="at">alpha =</span> <span class="fl">0.8</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-142-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We want to find the best straight line that approximates a cloud of data
points. For this, we assume a linear relationship between a single
explanatory variable <span class="math inline">\(X\)</span> and our target <span class="math inline">\(Y\)</span>: <span class="math display">\[
Y_i \sim \beta_0 + \beta_1 X_i, \;\;\; i = 1, 2, ...n \;,
\]</span> where <span class="math inline">\(Y_i\)</span> is the i-th observation of the target variable, and <span class="math inline">\(X_i\)</span>
is the i-th value of the (single) predictor variable. <span class="math inline">\(n\)</span> is the number
of observations we have and <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are constant
coefficients (model parameters). We call <span class="math inline">\(\beta_0\)</span> the intercept and
<span class="math inline">\(\beta_1\)</span> the slope of the regression line. Generally, <span class="math inline">\(\hat{Y}\)</span> denotes
the model prediction.</p>
<p>Fitting a linear regression is finding the values for <span class="math inline">\(\beta_0\)</span> and
<span class="math inline">\(\beta_1\)</span> such that, on average over all points, the distance between
the line at <span class="math inline">\(X_i\)</span>, that is <span class="math inline">\(\beta_0 + \beta_1 X_i\)</span> (blue segment in the
plot above), and the observed value <span class="math inline">\(Y_i\)</span>, is as small as possible.
Mathematically, this is minimizing the sum of the square errors, that
is: <span class="math display">\[
\min_{\beta_0, \beta_1} \sum_i (Y_i - \beta_0 - \beta_1 X_i)^2 .
\]</span> This linear model can be used to make predictions on new data, which
are obtained by <span class="math inline">\(\hat{Y}_\text{new} = \beta_0 + \beta_1 X_\text{new}\)</span>.
When the new data comes from the same distribution as the data used to
fit the regression line, this should be a good prediction.</p>
<p>It’s not hard to imagine that the univariate linear regression can be
generalized to a multivariate linear regression, where we assume that
the target variable is a linear combination of <span class="math inline">\(p\)</span> predictor variables:
<span class="math display">\[Y \sim \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \; ... \; + \beta_p X_p \;.\]</span>
Note that here, <span class="math inline">\(X_1, \dots, X_p\)</span> and <span class="math inline">\(Y\)</span> are vectors of length
corresponding to the number of observations in our data set (<span class="math inline">\(n\)</span> - as
above). Analogously, calibrating the <span class="math inline">\(p+1\)</span> coefficients
<span class="math inline">\(\beta_0, \beta_1, \beta_2, ..., \beta_p\)</span> is to minimize the sum of
square errors <span class="math inline">\(\min_{\beta} \sum_i (Y_i - \hat{Y}_i)^2\)</span>.</p>
<p>While the regression is a line in two-dimensional space for the
univariate case, it is a plane in three-dimensional space for bi-variate
regression, and hyperplanes in higher dimensions.</p>
<p><strong>Implementation in R</strong></p>
<p>To fit a univariate linear regression model in R, we can use the <code>lm()</code>
function. Already in Chapter <a href="datawrangling.html#datawrangling">3</a>, we created linear
models by doing:</p>
<div class="sourceCode" id="cb289"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb289-1"><a href="regressionclassification.html#cb289-1" aria-hidden="true" tabindex="-1"></a><span class="co"># numerical variables only, remove NA</span></span>
<span id="cb289-2"><a href="regressionclassification.html#cb289-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> half_hourly_fluxes <span class="sc">|&gt;</span></span>
<span id="cb289-3"><a href="regressionclassification.html#cb289-3" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span><span class="fu">starts_with</span>(<span class="st">&quot;TIMESTAMP&quot;</span>)) <span class="sc">|&gt;</span></span>
<span id="cb289-4"><a href="regressionclassification.html#cb289-4" aria-hidden="true" tabindex="-1"></a>  tidyr<span class="sc">::</span><span class="fu">drop_na</span>()</span>
<span id="cb289-5"><a href="regressionclassification.html#cb289-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb289-6"><a href="regressionclassification.html#cb289-6" aria-hidden="true" tabindex="-1"></a><span class="co"># fit univariate linear regression</span></span>
<span id="cb289-7"><a href="regressionclassification.html#cb289-7" aria-hidden="true" tabindex="-1"></a>linmod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(GPP_NT_VUT_REF <span class="sc">~</span> SW_IN_F, <span class="at">data =</span> df)</span></code></pre></div>
<p>Here, <code>GPP_NT_VUT_REF</code> is <span class="math inline">\(Y\)</span>, and <code>SW_IN_F</code> is <span class="math inline">\(X\)</span>. We can include
multiple predictors for a multivariate regression, for example as:</p>
<div class="sourceCode" id="cb290"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb290-1"><a href="regressionclassification.html#cb290-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fit multivariate linear regression</span></span>
<span id="cb290-2"><a href="regressionclassification.html#cb290-2" aria-hidden="true" tabindex="-1"></a>linmod2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(GPP_NT_VUT_REF <span class="sc">~</span> SW_IN_F <span class="sc">+</span> VPD_F <span class="sc">+</span> TA_F, <span class="at">data =</span> df)</span></code></pre></div>
<p>or all available features in our data set (all columns other than
<code>GPP_NT_VUT_REF</code> in <code>df</code>) as:</p>
<div class="sourceCode" id="cb291"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb291-1"><a href="regressionclassification.html#cb291-1" aria-hidden="true" tabindex="-1"></a>linmod3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(GPP_NT_VUT_REF <span class="sc">~</span> ., <span class="at">data =</span> df)</span></code></pre></div>
<p><code>linmod*</code> is now a model object of class <code>"lm"</code>. It is a list containing
the following components:</p>
<div class="sourceCode" id="cb292"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb292-1"><a href="regressionclassification.html#cb292-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ls</span>(linmod1)</span></code></pre></div>
<pre><code>##  [1] &quot;assign&quot;        &quot;call&quot;          &quot;coefficients&quot;  &quot;df.residual&quot;  
##  [5] &quot;effects&quot;       &quot;fitted.values&quot; &quot;model&quot;         &quot;qr&quot;           
##  [9] &quot;rank&quot;          &quot;residuals&quot;     &quot;terms&quot;         &quot;xlevels&quot;</code></pre>
<p>Enter <code>?lm</code> in the console for a complete documentation of these
components and other details of the linear model implementation.</p>
<p>R offers a set of generic functions that work with this type of object.
The following returns a human-readable report of the fit. Here the
<em>residuals</em> are the difference between the observed target values and
the predicted values.</p>
<div class="sourceCode" id="cb294"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb294-1"><a href="regressionclassification.html#cb294-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(linmod1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = GPP_NT_VUT_REF ~ SW_IN_F, data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -38.699  -2.092  -0.406   1.893  35.153 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.8732273  0.0285896   30.54   &lt;2e-16 ***
## SW_IN_F     0.0255041  0.0001129  225.82   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.007 on 41299 degrees of freedom
## Multiple R-squared:  0.5525, Adjusted R-squared:  0.5525 
## F-statistic: 5.099e+04 on 1 and 41299 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We can also extract coefficients <span class="math inline">\(\beta\)</span> with</p>
<div class="sourceCode" id="cb296"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb296-1"><a href="regressionclassification.html#cb296-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(linmod1)</span></code></pre></div>
<pre><code>## (Intercept)     SW_IN_F 
##  0.87322728  0.02550413</code></pre>
<p>and the residual sum of squares (which we wanted to minimize) with</p>
<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb298-1"><a href="regressionclassification.html#cb298-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">residuals</span>(linmod1)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 1035309</code></pre>
<p>Although <code>summary()</code> provides a nice, human-readable output, you may
find it unpractical to work with. A set of relevant statistical
quantities are returned in a tidy format using <code>tidy()</code> from the <code>broom</code>
package:</p>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb300-1"><a href="regressionclassification.html#cb300-1" aria-hidden="true" tabindex="-1"></a>broom<span class="sc">::</span><span class="fu">tidy</span>(linmod1)</span></code></pre></div>
<pre><code>## # A tibble: 2 × 5
##   term        estimate std.error statistic   p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)   0.873   0.0286        30.5 1.25e-202
## 2 SW_IN_F       0.0255  0.000113     226.  0</code></pre>
<p><strong>Model advantages and concerns</strong></p>
<p>An advantage of linear regression is that the coefficients provide
information that is straight-forward to interpret. We’ve seen above,
that <code>GPP_NT_VUT_REF</code> increases by 0.0255 for a unit increase in
<code>SW_IN_F</code>. Of course, the units of the coefficients depend on the units
of <code>GPP_NT_VUT_REF</code> and <code>SW_IN_F</code>. This has the advantage that the data
does not need to be normalised. That is, a linear regression model with
the same predictive skills can be found, irrespective of whether
<code>GPP_NT_VUT_REF</code> is given in g C m<span class="math inline">\(^{-2}\)</span>s<span class="math inline">\(^{-1}\)</span> or in kg C
m<span class="math inline">\(^{-2}\)</span>s<span class="math inline">\(^{-1}\)</span>.</p>
<p>Another advantage of linear regression is that it’s much less prone to
<em>overfit</em> than other algorithms. You’ll learn more about the concept of
overfitting in Chapter <a href="supervisedmli.html#supervisedmli">9</a>. Not being prone to
overfitting can also be a disadvantage: linear regression models are
often not flexible enough to be effectively fit to the data. They are
also not able to capture non-linearities in the observed relationship
and, as we’ll see later in this chapter, it exhibits a poorer
performance than more complex models (e.g. polynomial regression) also
on the validation data set.</p>
<p>A further limitation is that least squares regression requires <span class="math inline">\(n&gt;p\)</span>. In
words: the number of observations must be greater than the number of
predictors. If this is not given, one can resort to step-wise forward
regression, where predictors are sequentially added based on which
predictor adds the most additional information at each step.</p>
<p>When multiple predictors are linearly correlated, then linear regression
cannot discern individual effects and individual predictors may appear
statistically insignificant when they would be significant if covarying
predictors were not included in the model. Such instability can get
propagated to predictions. Again, stepwise regression can be used to
remedy this problem. However, when one predictor covaries with multiple
other predictors, this may not work. For many applications in Geography
and Environmental Sciences, we deal with limited numbers of predictors.
We can use our own knowledge to examine potentially problematic
covariations and make an informed pre-selection rather than throwing all
predictors we can possibly think of at our models. Such a pre-selection
can be guided by the model performance on a validation data set (more on
that below).</p>
<p>An alternative strategy is to use <em>dimension reduction</em> methods.
Principal Component regression reduces the data to capture only the
complementary axes along which our data varies and therefore collapses
covarying predictors into a single one that represents their common axis
of variation. Partial Least Squares regression works similarly but
modifies the principal components so that they are maximally correlated
to the target variable. You can read more on their implementation in R
<a href="https://bradleyboehmke.github.io/HOML/linear-regression.html#PCR">here</a>.</p>
</div>
<div id="regression-on-categorical-variables" class="section level4 hasAnchor" number="8.2.2.2">
<h4><span class="header-section-number">8.2.2.2</span> Regression on categorical variables<a href="regressionclassification.html#regression-on-categorical-variables" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In the <a href="datavis.html#vis_regr_cat">regression within categories</a> section of Chapter <a href="datavis.html#datavis">4</a>, we saw that when we separate the data into sub-plots,
hidden patterns emerge. This information is very relevant for modeling,
because it can be included in our regression model. It is crucial to
spend enough time exploring the data before you start modeling, because
it helps to understand the fit and output of the model, but also to
create models that capture the relationships between variables better.</p>
<div class="sourceCode" id="cb302"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb302-1"><a href="regressionclassification.html#cb302-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create month category</span></span>
<span id="cb302-2"><a href="regressionclassification.html#cb302-2" aria-hidden="true" tabindex="-1"></a>df_cat <span class="ot">&lt;-</span> half_hourly_fluxes <span class="sc">|&gt;</span></span>
<span id="cb302-3"><a href="regressionclassification.html#cb302-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">MONTH =</span> lubridate<span class="sc">::</span><span class="fu">month</span>(TIMESTAMP_START)) <span class="sc">|&gt;</span></span>
<span id="cb302-4"><a href="regressionclassification.html#cb302-4" aria-hidden="true" tabindex="-1"></a>  tidyr<span class="sc">::</span><span class="fu">drop_na</span>() <span class="sc">|&gt;</span></span>
<span id="cb302-5"><a href="regressionclassification.html#cb302-5" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(MONTH, GPP_NT_VUT_REF, SW_IN_F)</span></code></pre></div>
<p>So far, we have only used continuous variables as explanatory variables
in a linear regression. It is also possible to use categorical
variables. To do this in R, such variables cannot be of class <code>numeric</code>,
otherwise the <code>lm()</code> function treats them as continuous variables. For
example, although the variable <code>NIGHT</code> is categorical with values <code>0</code>
and <code>1</code>, the model <code>linmod3</code> treats it as a number. We must make sure
that categorical variables have class <code>character</code> or, even better,
<code>factor</code>.</p>
<div class="sourceCode" id="cb303"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb303-1"><a href="regressionclassification.html#cb303-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fix class of categorical variables</span></span>
<span id="cb303-2"><a href="regressionclassification.html#cb303-2" aria-hidden="true" tabindex="-1"></a>df_cat <span class="ot">&lt;-</span> df_cat <span class="sc">|&gt;</span></span>
<span id="cb303-3"><a href="regressionclassification.html#cb303-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">MONTH =</span> <span class="fu">as.factor</span>(MONTH))</span></code></pre></div>
<p>Now we can fit the linear model again:</p>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb304-1"><a href="regressionclassification.html#cb304-1" aria-hidden="true" tabindex="-1"></a>linmod_cat <span class="ot">&lt;-</span> <span class="fu">lm</span>(GPP_NT_VUT_REF <span class="sc">~</span> MONTH <span class="sc">+</span> SW_IN_F, <span class="at">data =</span> df_cat)</span>
<span id="cb304-2"><a href="regressionclassification.html#cb304-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(linmod_cat)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = GPP_NT_VUT_REF ~ MONTH + SW_IN_F, data = df_cat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -36.212  -2.346  -0.223   2.200  34.416 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.6146109  0.0893693  18.067  &lt; 2e-16 ***
## MONTH2      -1.8105447  0.1294675 -13.985  &lt; 2e-16 ***
## MONTH3      -2.8800172  0.1264177 -22.782  &lt; 2e-16 ***
## MONTH4      -2.5667281  0.1278097 -20.082  &lt; 2e-16 ***
## MONTH5      -0.0288745  0.1273491  -0.227 0.820631    
## MONTH6       0.4614556  0.1298069   3.555 0.000378 ***
## MONTH7       0.1697514  0.1283830   1.322 0.186100    
## MONTH8       1.2942463  0.1231252  10.512  &lt; 2e-16 ***
## MONTH9       0.5140562  0.1165474   4.411 1.03e-05 ***
## MONTH10     -0.4807082  0.1152536  -4.171 3.04e-05 ***
## MONTH11     -1.3370277  0.1159059 -11.535  &lt; 2e-16 ***
## MONTH12     -1.2634451  0.1151530 -10.972  &lt; 2e-16 ***
## SW_IN_F      0.0246420  0.0001169 210.810  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.865 on 41288 degrees of freedom
## Multiple R-squared:  0.5776, Adjusted R-squared:  0.5775 
## F-statistic:  4704 on 12 and 41288 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>In the fit summary, you can observe that, there are <code>MONTH2</code> to
<code>MONTH12</code> parameters. <code>MONTH</code> is a factor which can take 12 different
values: <code>1</code> to <code>12</code>. <code>lm()</code> uses one of the factor level as the
reference, in this case <code>1</code>, and fits an intercept for the other
categories. The result is a set of parallel regression lines, one for
each different month.</p>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="regressionclassification.html#cb306-1" aria-hidden="true" tabindex="-1"></a>df_cat <span class="sc">|&gt;</span></span>
<span id="cb306-2"><a href="regressionclassification.html#cb306-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">MONTH_NAME =</span> lubridate<span class="sc">::</span><span class="fu">month</span>(<span class="fu">as.integer</span>(MONTH), <span class="at">label =</span> <span class="cn">TRUE</span>)) <span class="sc">|&gt;</span></span>
<span id="cb306-3"><a href="regressionclassification.html#cb306-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> SW_IN_F, <span class="at">y =</span> GPP_NT_VUT_REF)) <span class="sc">+</span></span>
<span id="cb306-4"><a href="regressionclassification.html#cb306-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb306-5"><a href="regressionclassification.html#cb306-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">formula =</span> y <span class="sc">~</span> x <span class="sc">+</span> <span class="dv">0</span>, <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb306-6"><a href="regressionclassification.html#cb306-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;SW&quot;</span>, <span class="at">y =</span> <span class="st">&quot;GPP&quot;</span>) <span class="sc">+</span></span>
<span id="cb306-7"><a href="regressionclassification.html#cb306-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>MONTH_NAME) <span class="sc">+</span></span>
<span id="cb306-8"><a href="regressionclassification.html#cb306-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-154-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>In the grid image, we can observe that GPP does not increase with SW at
the same rate every month. For example, the increase in GPP is less
steep in February than in September. To model this, we should consider a
variable slope parameter for each month or category. In R, this is
implemented by including an <em>interaction</em> term <code>MONTH:SW_IN_F</code> in the
regression formula, like this:</p>
<div class="sourceCode" id="cb307"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb307-1"><a href="regressionclassification.html#cb307-1" aria-hidden="true" tabindex="-1"></a>linmod_inter <span class="ot">&lt;-</span> <span class="fu">lm</span>(GPP_NT_VUT_REF <span class="sc">~</span> MONTH <span class="sc">+</span> SW_IN_F <span class="sc">+</span> MONTH<span class="sc">:</span>SW_IN_F, <span class="at">data =</span> df_cat)</span>
<span id="cb307-2"><a href="regressionclassification.html#cb307-2" aria-hidden="true" tabindex="-1"></a><span class="co"># equivalently: lm(GPP_NT_VUT_REF ~ MONTH * SW_IN_F, data = df_cat)</span></span>
<span id="cb307-3"><a href="regressionclassification.html#cb307-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(linmod_inter)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = GPP_NT_VUT_REF ~ MONTH + SW_IN_F + MONTH:SW_IN_F, 
##     data = df_cat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -28.891  -2.113  -0.420   1.892  34.029 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      2.0449603  0.0944991  21.640  &lt; 2e-16 ***
## MONTH2          -1.5386938  0.1369424 -11.236  &lt; 2e-16 ***
## MONTH3          -1.5249304  0.1365863 -11.165  &lt; 2e-16 ***
## MONTH4          -1.0050639  0.1396023  -7.199 6.15e-13 ***
## MONTH5          -0.4502367  0.1412720  -3.187  0.00144 ** 
## MONTH6          -1.2559057  0.1474257  -8.519  &lt; 2e-16 ***
## MONTH7          -0.8440097  0.1446838  -5.833 5.47e-09 ***
## MONTH8          -0.2188300  0.1346734  -1.625  0.10419    
## MONTH9          -1.3407190  0.1269387 -10.562  &lt; 2e-16 ***
## MONTH10         -0.9991456  0.1235627  -8.086 6.32e-16 ***
## MONTH11         -1.2124373  0.1230946  -9.850  &lt; 2e-16 ***
## MONTH12         -1.0724209  0.1210819  -8.857  &lt; 2e-16 ***
## SW_IN_F          0.0158600  0.0008758  18.110  &lt; 2e-16 ***
## MONTH2:SW_IN_F  -0.0030373  0.0011518  -2.637  0.00837 ** 
## MONTH3:SW_IN_F  -0.0058229  0.0009713  -5.995 2.05e-09 ***
## MONTH4:SW_IN_F  -0.0038333  0.0009469  -4.048 5.17e-05 ***
## MONTH5:SW_IN_F   0.0087370  0.0009305   9.389  &lt; 2e-16 ***
## MONTH6:SW_IN_F   0.0135219  0.0009172  14.743  &lt; 2e-16 ***
## MONTH7:SW_IN_F   0.0110791  0.0009182  12.066  &lt; 2e-16 ***
## MONTH8:SW_IN_F   0.0151014  0.0009317  16.209  &lt; 2e-16 ***
## MONTH9:SW_IN_F   0.0180496  0.0009297  19.415  &lt; 2e-16 ***
## MONTH10:SW_IN_F  0.0097277  0.0009761   9.966  &lt; 2e-16 ***
## MONTH11:SW_IN_F -0.0011415  0.0010932  -1.044  0.29640    
## MONTH12:SW_IN_F -0.0099745  0.0012972  -7.689 1.52e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.593 on 41277 degrees of freedom
## Multiple R-squared:  0.6237, Adjusted R-squared:  0.6234 
## F-statistic:  2974 on 23 and 41277 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div id="polynomial-regression" class="section level4 hasAnchor" number="8.2.2.3">
<h4><span class="header-section-number">8.2.2.3</span> Polynomial regression<a href="regressionclassification.html#polynomial-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Furthermore, the relationships between variables may be non-linear. In
the previous example, we see that the increase in GPP saturates as
shortwave radiation grows, which suggests that the true relationship
could be represented by a curve. There are many regression methods that
fit this kind of relationship, like polynomial regression,
<a href="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/loess">LOESS</a>
(local polynomial regression fitting), etc.</p>
<p>Let’s fit a simple quadratic regression model, just for the month of
August. For this we use the <code>poly()</code> function which constructs
orthogonal polynomials of a given degree:</p>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb309-1"><a href="regressionclassification.html#cb309-1" aria-hidden="true" tabindex="-1"></a>quadmod <span class="ot">&lt;-</span> <span class="fu">lm</span>(GPP_NT_VUT_REF <span class="sc">~</span> <span class="fu">poly</span>(SW_IN_F, <span class="dv">2</span>), </span>
<span id="cb309-2"><a href="regressionclassification.html#cb309-2" aria-hidden="true" tabindex="-1"></a>              <span class="at">data =</span> df_cat <span class="sc">|&gt;</span></span>
<span id="cb309-3"><a href="regressionclassification.html#cb309-3" aria-hidden="true" tabindex="-1"></a>                <span class="fu">filter</span>(MONTH <span class="sc">==</span> <span class="dv">8</span>))</span>
<span id="cb309-4"><a href="regressionclassification.html#cb309-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(quadmod)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = GPP_NT_VUT_REF ~ poly(SW_IN_F, 2), data = filter(df_cat, 
##     MONTH == 8))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -26.367  -2.055  -0.253   1.801  32.375 
## 
## Coefficients:
##                     Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)          7.13084    0.07944   89.77   &lt;2e-16 ***
## poly(SW_IN_F, 2)1  447.25113    4.61907   96.83   &lt;2e-16 ***
## poly(SW_IN_F, 2)2 -151.08797    4.61907  -32.71   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.619 on 3378 degrees of freedom
## Multiple R-squared:  0.7556, Adjusted R-squared:  0.7555 
## F-statistic:  5223 on 2 and 3378 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>In the following plot, you can see how the model fit for GPP in August
improves as we consider higher degree polynomials:</p>
<div class="sourceCode" id="cb311"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb311-1"><a href="regressionclassification.html#cb311-1" aria-hidden="true" tabindex="-1"></a>df_cat <span class="sc">|&gt;</span></span>
<span id="cb311-2"><a href="regressionclassification.html#cb311-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(MONTH <span class="sc">==</span> <span class="dv">8</span>) <span class="sc">|&gt;</span></span>
<span id="cb311-3"><a href="regressionclassification.html#cb311-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> SW_IN_F, <span class="at">y =</span> GPP_NT_VUT_REF)) <span class="sc">+</span></span>
<span id="cb311-4"><a href="regressionclassification.html#cb311-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.4</span>) <span class="sc">+</span></span>
<span id="cb311-5"><a href="regressionclassification.html#cb311-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">formula =</span> y <span class="sc">~</span> x, <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="fu">aes</span>(<span class="at">color =</span> <span class="st">&quot;lm&quot;</span>), <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb311-6"><a href="regressionclassification.html#cb311-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">formula =</span> y <span class="sc">~</span> <span class="fu">poly</span>(x, <span class="dv">2</span>), <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, </span>
<span id="cb311-7"><a href="regressionclassification.html#cb311-7" aria-hidden="true" tabindex="-1"></a>              <span class="fu">aes</span>(<span class="at">color =</span> <span class="st">&quot;poly2&quot;</span>), <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb311-8"><a href="regressionclassification.html#cb311-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">formula =</span> y <span class="sc">~</span> <span class="fu">poly</span>(x, <span class="dv">3</span>), <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb311-9"><a href="regressionclassification.html#cb311-9" aria-hidden="true" tabindex="-1"></a>              <span class="fu">aes</span>(<span class="at">color =</span> <span class="st">&quot;poly3&quot;</span>), <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb311-10"><a href="regressionclassification.html#cb311-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">formula =</span> y <span class="sc">~</span> <span class="fu">poly</span>(x, <span class="dv">4</span>), <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, </span>
<span id="cb311-11"><a href="regressionclassification.html#cb311-11" aria-hidden="true" tabindex="-1"></a>              <span class="fu">aes</span>(<span class="at">color =</span> <span class="st">&quot;poly4&quot;</span>), <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb311-12"><a href="regressionclassification.html#cb311-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;SW&quot;</span>, <span class="at">y =</span> <span class="st">&quot;GPP&quot;</span>, <span class="at">color =</span> <span class="st">&quot;Regression&quot;</span>) <span class="sc">+</span></span>
<span id="cb311-13"><a href="regressionclassification.html#cb311-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-157-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="metrics-for-regression-evaluation" class="section level4 hasAnchor" number="8.2.2.4">
<h4><span class="header-section-number">8.2.2.4</span> Metrics for regression evaluation<a href="regressionclassification.html#metrics-for-regression-evaluation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Metrics measure the quality of fit between predicted and observed
values, are essential to model training (where the metric defines the
loss function, see Chapter <a href="supervisedmlii.html#supervisedmlii">10</a>), and inform model
evaluation. Different metrics measure different aspects of the
model-data agreement. In other words, a single metric never captures all aspects and
patterns of the model-data agreement. Therefore, a visual inspection of
the model fit is always a good start of the model evaluation.</p>
<p>To get an intuitive understanding of the different abilities of different metrics, compare
the scatterplots in Fig. <a href="regressionclassification.html#fig:correlationplots">8.1</a> and how different aspects of the model-data
agreement are measured by different metrics. The observed target values <span class="math inline">\(y\)</span> are plotted against the predicted values <span class="math inline">\(\hat{y}\)</span> from a
regression model, and the dashed line represents the ideal scenario: our predictions matching the data perfectly. Definitions of the metrics displayed and other metrics are given below.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:correlationplots"></span>
<img src="_main_files/figure-html/correlationplots-1.png" alt="Correlation plots between observed and fitted target values." width="672" />
<p class="caption">
Figure 8.1: Correlation plots between observed and fitted target values.
</p>
</div>
<p>Common metrics used for evaluating regression fits are:</p>
<p><strong>MSE</strong></p>
<p>The mean squared error is defined, as its name suggests,
as: <span class="math display">\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^n (Y_i - \hat{Y_i})^2
\]</span>It measures the magnitude of the errors, and is minimized to fit a
linear regression or, as we will see in Chapter
<a href="supervisedmli.html#supervisedmli">9</a>, during model training when used as a loss
function. Note that since it scales with the square of the errors,
the MSE is sensitive to large errors in single points, including
outliers.</p>
<p><strong>RMSE</strong></p>
<p>The root mean squared error is, as its name suggests, the
root of the MSE: <span class="math display">\[
\text{RMSE} = \sqrt{\text{MSE}} = \sqrt{\frac{1}{n} \sum_{i=1}^n (Y_i - \hat{Y_i})^2}
\]</span>Like the MSE, the RMSE also measures the magnitude of
the errors and is minimized during model training. By taking the
square root of mean square errors, the RMSE is in the same units as
the data <span class="math inline">\(Y\)</span> and is less sensitive to outliers than the MSE.</p>
<p><strong>MAE</strong></p>
<p>The mean absolute error is similarly defined: <span class="math display">\[
\text{MAE} = \frac{1}{n} \sum_{i = 1}^{n} |Y_i - \hat{Y_i}|
\]</span>
Measuring the discrepancies between predictions and observations using absolute errors,
instead of squared errors, gives less importance to errors of large magnitude and more importance
to small errors than the MSE would. Hence, this measures is more stable in the presence of
outliers.</p>
<p><strong><span class="math inline">\(R^2\)</span> - coefficient of determination</strong></p>
<p>describes the proportion of variation in <span class="math inline">\(y\)</span> that is captured by modelled values
<span class="math inline">\(\hat{y_i}\)</span>. It tells us how much better our fitted values <span class="math inline">\(\hat{y_i}\)</span> are than just
taking the average of the target <span class="math inline">\(\bar{y}\)</span> as predictions. In this case, the goal is to maximize the metric, thus
trying the explain as much variation as possible. In contrast to the
MSE and RMSE, <span class="math inline">\(R^2\)</span> measures <em>consistency</em>, or <em>correlation</em>, or
<em>goodness of fit</em>. It is defined as:
<span class="math display">\[
R^2 = 1 - \frac{\sum_i (\hat{Y}_i - Y_i)^2}{\sum_i (Y_i - \bar{Y})^2}\\
\]</span>
When the regression model is fitted by minimizing the MSE, the <span class="math inline">\(R^2\)</span> takes values
between 0 and 1. A perfect fit is quantified by <span class="math inline">\(R^2 = 1\)</span>. There is no generally
valid threshold of <span class="math inline">\(R^2\)</span> for a model to be considered “good”. It
depends on the application and the nature of the data and the
data-generating process. Note that the above equation can also be
written as <span class="math inline">\(R^2 = 1 - \text{MSE}/var(Y)\)</span>.</p>
<p><span class="math inline">\(r\)</span> - <strong>Pearson’s correlation</strong></p>
<p>The linear association between two variables (here <span class="math inline">\(y\)</span> and <span class="math inline">\(\hat{y}\)</span>) is measured by the <em>Pearson’s correlation coefficient</em> <span class="math inline">\(r\)</span>.
<span class="math display">\[
r = \frac{\sum_i (Y_i - \bar{Y}) (\hat{Y_i} - \bar{\hat{Y}}) }{\sqrt{ \sum_i(Y_i-\bar{Y})^2 \; (\hat{Y_i}-\bar{\hat{Y}})^2 } }
\]</span></p>
<p>The correlation calculated between the target <span class="math inline">\(Y\)</span> and a predictor <span class="math inline">\(X\)</span> can tell us about the predictive
power of <span class="math inline">\(X\)</span> in a linear regression model (the higher the correlation, the more powerful). We can also compute
the correlation between the target <span class="math inline">\(Y\)</span> and the predicted values <span class="math inline">\(\hat{Y}\)</span> by a model (multivariate, or even
not linear) to assess the adequacy of the model chosen. See Figure <a href="regressionclassification.html#fig:correlationplots">8.1</a> as an example. It is
noteworthy to mention that correlation is location and scale invariant, hence it will not detect
model deviations like the ones in the middle row plots.</p>
<p>The squared value of the Pearson’s <em>r</em> is often reported as “<span class="math inline">\(R^2\)</span>” but doesn’t
correspond to the definition of the coefficient of determination
given above. However, the square of the Pearson’s <em>r</em> is closely
related to the coefficient of determination <span class="math inline">\(R^2\)</span>. For a
linear regression, fitted minimizing the MSE, they are identical (see proof
<a href="https://statproofbook.github.io/P/slr-rsq">here</a>). In subsequent chapters, we
will use “<span class="math inline">\(R^2\)</span>” to refer to the square of the Pearson’s <em>r</em>
between the observed <span class="math inline">\(Y\)</span> and predicted <span class="math inline">\(\hat{Y}\)</span> values.</p>
<blockquote>
<p>Note the implementations in R.The <span class="math inline">\(R^2\)</span> reported by the generic
<code>summary()</code> function corresponds to the base-R function <code>cor()^2</code> , to
<code>yardstick::rsq()</code>, and to the definition of the square of the
<em>Pearson’s</em> <span class="math inline">\(r\)</span> given above. The <code>yardstick::rsq_trad()</code> returns the coefficient of determination as
traditionally defined and is not equal to the values above, unless computed on the predicted values <span class="math inline">\(\hat{y}\)</span>.</p>
</blockquote>
<blockquote>
<p>Sometimes the Person’s version is computed between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span>, and it leads to the
same number due to its “location and scale invariant” property. Nevertheless, this is
conceptually wrong, as we should look at the predictions, not the predictors: We are
not predicting <span class="math inline">\(y\)</span> by just giving the values of <span class="math inline">\(x\)</span> instead. Hence, especially when
using {yardstick} functions, make sure you compute the values on <span class="math inline">\(\hat{y}\)</span>. When we
have several predictors, it’s already clear that we should compare <span class="math inline">\(y\)</span> to <span class="math inline">\(\hat{y}\)</span>
instead of <span class="math inline">\(y\)</span> to each predictor separately.</p>
</blockquote>
<div class="sourceCode" id="cb312"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb312-1"><a href="regressionclassification.html#cb312-1" aria-hidden="true" tabindex="-1"></a><span class="co"># generate correlated random data </span></span>
<span id="cb312-2"><a href="regressionclassification.html#cb312-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1982</span>)</span>
<span id="cb312-3"><a href="regressionclassification.html#cb312-3" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)) <span class="sc">|&gt;</span> </span>
<span id="cb312-4"><a href="regressionclassification.html#cb312-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y =</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)) <span class="sc">|&gt;</span></span>
<span id="cb312-5"><a href="regressionclassification.html#cb312-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_fitted =</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)<span class="sc">$</span>fitted.values)</span>
<span id="cb312-6"><a href="regressionclassification.html#cb312-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb312-7"><a href="regressionclassification.html#cb312-7" aria-hidden="true" tabindex="-1"></a><span class="co"># implementations using Pearson&#39;s correlation</span></span>
<span id="cb312-8"><a href="regressionclassification.html#cb312-8" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> df))<span class="sc">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.6186521</code></pre>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb314-1"><a href="regressionclassification.html#cb314-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(df<span class="sc">$</span>y, df<span class="sc">$</span>x)<span class="sc">^</span><span class="dv">2</span> <span class="co"># remember: location and scale invariant</span></span></code></pre></div>
<pre><code>## [1] 0.6186521</code></pre>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb316-1"><a href="regressionclassification.html#cb316-1" aria-hidden="true" tabindex="-1"></a>yardstick<span class="sc">::</span><span class="fu">rsq</span>(df, y, x) <span class="sc">|&gt;</span> <span class="fu">pull</span>(.estimate)</span></code></pre></div>
<pre><code>## [1] 0.6186521</code></pre>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb318-1"><a href="regressionclassification.html#cb318-1" aria-hidden="true" tabindex="-1"></a>(<span class="fu">sum</span>((df<span class="sc">$</span>x <span class="sc">-</span> <span class="fu">mean</span>(df<span class="sc">$</span>x))<span class="sc">*</span>(df<span class="sc">$</span>y <span class="sc">-</span> <span class="fu">mean</span>(df<span class="sc">$</span>y))))<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span></span>
<span id="cb318-2"><a href="regressionclassification.html#cb318-2" aria-hidden="true" tabindex="-1"></a>  (<span class="fu">sum</span>((df<span class="sc">$</span>y <span class="sc">-</span> <span class="fu">mean</span>(df<span class="sc">$</span>y))<span class="sc">^</span><span class="dv">2</span>)<span class="sc">*</span><span class="fu">sum</span>((df<span class="sc">$</span>x <span class="sc">-</span> <span class="fu">mean</span>(df<span class="sc">$</span>x))<span class="sc">^</span><span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 0.6186521</code></pre>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb320-1"><a href="regressionclassification.html#cb320-1" aria-hidden="true" tabindex="-1"></a><span class="co"># implementations using coefficient of determination definition</span></span>
<span id="cb320-2"><a href="regressionclassification.html#cb320-2" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">sum</span>((df<span class="sc">$</span>x <span class="sc">-</span> df<span class="sc">$</span>y)<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> <span class="fu">sum</span>((df<span class="sc">$</span>y <span class="sc">-</span> <span class="fu">mean</span>(df<span class="sc">$</span>y))<span class="sc">^</span><span class="dv">2</span>) <span class="co"># should be \hat{y}, not x</span></span></code></pre></div>
<pre><code>## [1] 0.5993324</code></pre>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb322-1"><a href="regressionclassification.html#cb322-1" aria-hidden="true" tabindex="-1"></a>yardstick<span class="sc">::</span><span class="fu">rsq_trad</span>(df, y, x) <span class="sc">|&gt;</span> <span class="fu">pull</span>(.estimate) <span class="co"># incorrect</span></span></code></pre></div>
<pre><code>## [1] 0.5993324</code></pre>
<div class="sourceCode" id="cb324"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb324-1"><a href="regressionclassification.html#cb324-1" aria-hidden="true" tabindex="-1"></a>yardstick<span class="sc">::</span><span class="fu">rsq_trad</span>(df, y, y_fitted) <span class="sc">|&gt;</span> <span class="fu">pull</span>(.estimate) <span class="co"># correct</span></span></code></pre></div>
<pre><code>## [1] 0.6186521</code></pre>
<blockquote>
<p>An “<span class="math inline">\(R^2\)</span>” is commonly reported when evaluating the <em>agreement</em> between observed
and predicted values of a given model. When the <em>correlation</em>
between two different variables in a sample is quantified, <span class="math inline">\(r\)</span> is
commonly used to reflect also whether the correlation is positive or
negative (<span class="math inline">\(r\)</span> can attain positive or negative values in the interval <span class="math inline">\([-1, 1]\)</span>). The
coefficient of determination can return negative values when
comparing observed and predicted values for uninformative estimates (worse than just using the average of <span class="math inline">\(Y\)</span>)
and is thus not actually bound between 0 and 1. Therefore, be careful with the interpreration of “<span class="math inline">\(R^2\)</span>” and
think on which variables it was computed and with which method.</p>
</blockquote>
<p><strong>Bias</strong></p>
<p>The bias is simply the mean error:
<span class="math display">\[
\text{Bias} = \frac{1}{n} \sum_i^n{(\hat{Y}_i - Y_i)}
\]</span></p>
<p><strong>Slope</strong></p>
<p>The slope refers to the slope of the linear regression line between predicted and observed values. It is returned as the second element of the vector returned by <code>coef(lm(..))</code>:</p>
<div class="sourceCode" id="cb326"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb326-1"><a href="regressionclassification.html#cb326-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> y_fitted, <span class="at">data =</span> df))[<span class="dv">2</span>]</span></code></pre></div>
<pre><code>## y_fitted 
##        1</code></pre>
</div>
<div id="metrics-for-regression-model-comparison" class="section level4 hasAnchor" number="8.2.2.5">
<h4><span class="header-section-number">8.2.2.5</span> Metrics for regression model comparison<a href="regressionclassification.html#metrics-for-regression-model-comparison" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In general, the aim of regression modelling is to find a model that best explains
the data - but not the random errors in the data. More complex models tend to <em>overfit</em> more than simpler models. The implication of overfitting is that the model fits the data used for model fitting well, but doesn’t fit well when evaluating the predictions of the same model to new data (data not used for model fitting). In such a case, the model’s <em>generalisability</em> is poor. We’ll learn more about overfitting and generalisability in the context of supervised machine learning in later chapters. Often, simpler models generalise better than more complex model. The challenge is to strike a balance between complexity and generalisability. But how to find the “sweet spot” of this trade-off?</p>
<p>In this context it should be noted that the <span class="math inline">\(R^2\)</span> <em>always</em> increases when predictors are added to a model. Therefore, the <span class="math inline">\(R^2\)</span> is not a suitable metric for comparing models that differ with respect to their number of predictors - a factor controlling model complexity. <em>Cross-validation</em> can be regarded as the “gold-standard” for measuring model generalisability if the data is plentiful. It will be introduced in the context of supervised machine learning in Chapter <a href="supervisedmlii.html#supervisedmlii">10</a>. However, when the data size is small, cross validation estimates may not be robust. Without
resorting to cross validation, the effect of spuriously improving the
evaluation metric by adding uninformative predictors can also be
mitigated by penalizing the number of predictors <span class="math inline">\(p\)</span>. Different metrics
are available:</p>
<p><strong>Adjusted</strong> <span class="math inline">\(R^2\)</span></p>
<p>The adjusted <span class="math inline">\(R^2\)</span> discounts values of <span class="math inline">\(R^2\)</span> by the
number of predictors. It is defined as <span class="math display">\[
{R}^2_{adj} = 1 - (1-R^2) \; \frac{n-1}{n-p-1} \;,
\]</span> where <span class="math inline">\(n\)</span> (as before) is the number of observations, <span class="math inline">\(p\)</span> the
number of parameters and <span class="math inline">\(R^2\)</span> the usual coefficient of
determination. Same as for <span class="math inline">\(R^2\)</span>, the goal is to maximize
<span class="math inline">\(R^2_{adj}\)</span>.</p>
<p><strong>AIC</strong></p>
<p>The Akaike’s Information Criterion is defined in terms of
log-likelihood (covered in Quantitative Methoden) but for linear
regression it can be written as: <span class="math display">\[
\text{AIC} = n \log \Big(\frac{\text{SSE}}{n}\Big) + 2(p+2)
\]</span> where <span class="math inline">\(n\)</span> is the number of observations used for estimation, <span class="math inline">\(p\)</span>
is the number of explanatory variables in the model and SSE is the
sum of squared errors (SSE<span class="math inline">\(= \sum_i (Y_i-\hat{Y_i})^2\)</span>). Also in
this case we have to minimize it and the model with the minimum
value of the AIC is often the best model for forecasting. Since it
penalizes having many parameters, it will favor less complex models.</p>
<p><strong>AIC</strong><span class="math inline">\(_c\)</span></p>
<p>For small values of <span class="math inline">\(n\)</span> the AIC tends to select too
many predictors. A bias-corrected version of the AIC is defined as:
<span class="math display">\[
\text{AIC}_c = \text{AIC} + \frac{2(p + 2)(p + 3)}{n-p-3}
\]</span> Also AIC<span class="math inline">\(_c\)</span> is minimized for an optimal predictive model.</p>
<p><strong>BIC</strong></p>
<p>The Schwarz’s Bayesian Information Criterion is defined as
<span class="math display">\[
\text{BIC} = n \log \Big(\frac{\text{SSE}}{n}\Big) + (p+2)  \log(n)
\]</span> Also for BIC, the goal is to minimize it. This metric has the
feature that if there is a true underlying model, the BIC will
select that model given enough data. The BIC tends to select a model
with fewer predictors than AIC.</p>
<p><strong>Implementation in R</strong></p>
<p>Let’s calculate the metrics introduced above for a few of the fitted
regression models. Some of these metrics, like <span class="math inline">\(R^2\)</span> and <span class="math inline">\(R^2_{adj}\)</span> are
given by the <code>summary()</code> function. Alternatively, the {yardstick}
package provides implementations for a few of these metrics, which we
compute below:</p>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb328-1"><a href="regressionclassification.html#cb328-1" aria-hidden="true" tabindex="-1"></a>compute_regr_metrics <span class="ot">&lt;-</span> <span class="cf">function</span>(mod){</span>
<span id="cb328-2"><a href="regressionclassification.html#cb328-2" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb328-3"><a href="regressionclassification.html#cb328-3" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">&lt;-</span> <span class="fu">length</span>(mod<span class="sc">$</span>coefficients)</span>
<span id="cb328-4"><a href="regressionclassification.html#cb328-4" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="fu">length</span>(mod<span class="sc">$</span>residuals)</span>
<span id="cb328-5"><a href="regressionclassification.html#cb328-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb328-6"><a href="regressionclassification.html#cb328-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(</span>
<span id="cb328-7"><a href="regressionclassification.html#cb328-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">mse =</span> <span class="fu">mean</span>(mod<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>),</span>
<span id="cb328-8"><a href="regressionclassification.html#cb328-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">R2 =</span> <span class="fu">summary</span>(mod)<span class="sc">$</span>r.squared,</span>
<span id="cb328-9"><a href="regressionclassification.html#cb328-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">R2_adj =</span> <span class="fu">summary</span>(mod)<span class="sc">$</span>adj.r.squared,</span>
<span id="cb328-10"><a href="regressionclassification.html#cb328-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">AIC =</span> <span class="fu">extractAIC</span>(mod)[<span class="dv">2</span>],</span>
<span id="cb328-11"><a href="regressionclassification.html#cb328-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">AIC_adj =</span> <span class="fu">extractAIC</span>(mod)[<span class="dv">2</span>] <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>(p<span class="sc">+</span><span class="dv">2</span>)<span class="sc">*</span>(p<span class="sc">+</span><span class="dv">3</span>)<span class="sc">/</span>(n<span class="sc">-</span>p<span class="dv">-3</span>),</span>
<span id="cb328-12"><a href="regressionclassification.html#cb328-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">BIC =</span> <span class="fu">BIC</span>(mod) <span class="co"># this implementation is based on log-likelihood</span></span>
<span id="cb328-13"><a href="regressionclassification.html#cb328-13" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb328-14"><a href="regressionclassification.html#cb328-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb328-15"><a href="regressionclassification.html#cb328-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb328-16"><a href="regressionclassification.html#cb328-16" aria-hidden="true" tabindex="-1"></a>list_metrics <span class="ot">&lt;-</span> purrr<span class="sc">::</span><span class="fu">map</span>(</span>
<span id="cb328-17"><a href="regressionclassification.html#cb328-17" aria-hidden="true" tabindex="-1"></a>    <span class="fu">list</span>(linmod1, linmod2, linmod_cat, quadmod), </span>
<span id="cb328-18"><a href="regressionclassification.html#cb328-18" aria-hidden="true" tabindex="-1"></a>    <span class="sc">~</span><span class="fu">compute_regr_metrics</span>(.))</span>
<span id="cb328-19"><a href="regressionclassification.html#cb328-19" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(list_metrics) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Linear model&quot;</span>, </span>
<span id="cb328-20"><a href="regressionclassification.html#cb328-20" aria-hidden="true" tabindex="-1"></a>                         <span class="st">&quot;Linear model 2&quot;</span>, </span>
<span id="cb328-21"><a href="regressionclassification.html#cb328-21" aria-hidden="true" tabindex="-1"></a>                         <span class="st">&quot;Linear + categories&quot;</span>,</span>
<span id="cb328-22"><a href="regressionclassification.html#cb328-22" aria-hidden="true" tabindex="-1"></a>                         <span class="st">&quot;Quadratic model&quot;</span>)</span>
<span id="cb328-23"><a href="regressionclassification.html#cb328-23" aria-hidden="true" tabindex="-1"></a><span class="fu">bind_rows</span>(list_metrics, <span class="at">.id =</span> <span class="st">&quot;type&quot;</span>)</span></code></pre></div>
<pre><code>## # A tibble: 4 × 7
##   type                  mse    R2 R2_adj     AIC AIC_adj     BIC
##   &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
## 1 Linear model         25.1 0.553  0.553 133058. 133058. 250293.
## 2 Linear model 2       24.8 0.558  0.558 132590. 132590. 249842.
## 3 Linear + categories  23.7 0.578  0.577 130700. 130700. 248030.
## 4 Quadratic model      21.3 0.756  0.755  10350.  10350.  19972.</code></pre>
</div>
</div>
<div id="classification" class="section level3 hasAnchor" number="8.2.3">
<h3><span class="header-section-number">8.2.3</span> Classification<a href="regressionclassification.html#classification" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Classification models predict a categorical target variable. Note that
predictors may be continuous. We will introduce a classification problem
with a binary target, since it’s straightforward to generalize to
categorical variables with more than two classes. As an example, let’s
observe the <code>CO2</code> dataset from the <a href="https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/zCO2.html">{datasets}
package</a>,
which contains data from an experiment on the cold tolerance of a grass
species. We will try to classify the origin of each plant (categorical
variable <code>Type</code> with values <code>Quebec</code> or <code>Mississippi</code>) depending on the
carbon dioxide uptake rate of the plant (continuous variable <code>uptake</code>
measured in <span class="math inline">\(\mu\)</span>mol m<span class="math inline">\(^{-2}\)</span>s<span class="math inline">\(^{-1}\)</span>). More information on the dataset can be obtained by typing <code>?datasets::CO2</code> in the console.</p>
<div class="sourceCode" id="cb330"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb330-1"><a href="regressionclassification.html#cb330-1" aria-hidden="true" tabindex="-1"></a>plot_2 <span class="ot">&lt;-</span> datasets<span class="sc">::</span>CO2 <span class="sc">|&gt;</span></span>
<span id="cb330-2"><a href="regressionclassification.html#cb330-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> uptake, <span class="at">y =</span> Type, <span class="at">color =</span> Type)) <span class="sc">+</span></span>
<span id="cb330-3"><a href="regressionclassification.html#cb330-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.75</span>, <span class="at">alpha =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb330-4"><a href="regressionclassification.html#cb330-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span></span>
<span id="cb330-5"><a href="regressionclassification.html#cb330-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&quot;none&quot;</span>)</span>
<span id="cb330-6"><a href="regressionclassification.html#cb330-6" aria-hidden="true" tabindex="-1"></a>plot_2</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-161-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>At first sight, it’s easy to see that the carbon uptake is lower for the
Mississippi type. Note that other predictors can be included in the
model, but we’ll focus on a single predictor. Using this example, we’ll
cover logistic regression, its implementation in R and metrics for
classification.</p>
<div id="logistic-regression" class="section level4 hasAnchor" number="8.2.3.1">
<h4><span class="header-section-number">8.2.3.1</span> Logistic regression<a href="regressionclassification.html#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Theory</strong></p>
<p>A classification problem is a bit more difficult to write mathematically
than a regression problem. Before, the mathematical representation of
<code>GPP_NT_VUT_REF ~ SW_IN_F</code> was
<code>GPP_NT_VUT_REF</code><span class="math inline">\(\;=\; \beta_0 + \beta_1\)</span><code>SW_IN_F</code>. With the
classification model <code>Type ~ uptake</code>, we cannot just write
<code>Type</code><span class="math inline">\(\;=\; \beta_0 + \beta_1\)</span><code>uptake</code> because <code>Type</code> is not a number.
Hence, the categorical variable must be encoded, in this case 0
represents <code>Quebec</code> and 1 represents <code>Mississippi</code>.</p>
<p>The next issue is that a linear model makes continuous predictions in
the entire real numbers space <span class="math inline">\((-\inf, \inf)\)</span>, but we want the
predictions to be either 0 or 1. We can transform these values to be in
the interval <span class="math inline">\([0,1]\)</span> with a <em>link</em> function. For a binary response, it’s
common to use a <em>logit</em> link function:
<span class="math display">\[\text{logit}(z) = \frac{\exp(z)}{1+\exp(z)}.\]</span></p>
<div class="sourceCode" id="cb331"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb331-1"><a href="regressionclassification.html#cb331-1" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">exp</span>(x)<span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(x)), <span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="at">ylab =</span> <span class="st">&quot;logit(x)&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-162-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Combining a linear model (with any type of predictors, like for
regression) and a logit link function, we arrive to a <strong>logistic
regression</strong> model:
<span class="math display">\[f(X, \beta) = \text{logit}(\beta_0 + \beta_1 X_1 + ... + \beta_p X_p) = \frac{\exp(\beta_0 + \beta_1 X_1 + ... + \beta_p X_p)}{1 + \exp(\beta_0 + \beta_1 X_1 + ... + \beta_p X_p)}.\]</span>
This predicted value can be understood as the probability of belonging
to class 1 (in our example, <code>Mississippi</code>). A classification rule is
defined such that an observation <span class="math inline">\(X_{new}\)</span> with a predicted probability
of belonging to class 1 higher than a given <em>threshold</em> <span class="math inline">\(\tau\)</span> (i.e.
<span class="math inline">\(f(X_{new}, \beta) &gt; \tau\)</span>) will be classified as 1; and if the
predicted probability is smaller than the threshold, it will be
classified as 0.</p>
<p>A logistic regression model results in a linear classification rule.
This means that the <span class="math inline">\(p\)</span>-dimensional space will be divided in two by a
hyperplane, and the points falling in each side of the hyperplane will
be classified as 1 or 0. In the example above with carbon uptake as
predictor, the classification boundary would be a point dividing the
real line. If we include a second predictor, we would obtain a line
diviting the 2-dimensional plane in two.</p>
<p>Furthermore, to fit a logistic regression model means to calculate the
maximum likelihood estimator of <span class="math inline">\(\beta\)</span> with an iterative algorithm. We
will learn more about iterative model fitting, i.e. parameter
optimization, in the context of supervised machine learning (Chapter
<a href="supervisedmlii.html#supervisedmlii">10</a>).</p>
<p><strong>Implementation in R</strong></p>
<p>First, let’s see how the target variable is encoded. R directly loads
the dataframe with <code>Type</code> as a factor and <code>Quebec</code> as the reference
level. R factors work such that each factor level (here <code>Quebec</code> and
<code>Mississippi</code>) corresponds to an integer value (its position given by
<code>levels()</code>, here <code>1</code> and <code>2</code> respectively). We can fit a logistic model
in R with this encoding.</p>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb332-1"><a href="regressionclassification.html#cb332-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(datasets<span class="sc">::</span>CO2)</span></code></pre></div>
<pre><code>## Classes &#39;nfnGroupedData&#39;, &#39;nfGroupedData&#39;, &#39;groupedData&#39; and &#39;data.frame&#39;:   84 obs. of  5 variables:
##  $ Plant    : Ord.factor w/ 12 levels &quot;Qn1&quot;&lt;&quot;Qn2&quot;&lt;&quot;Qn3&quot;&lt;..: 1 1 1 1 1 1 1 2 2 2 ...
##  $ Type     : Factor w/ 2 levels &quot;Quebec&quot;,&quot;Mississippi&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ Treatment: Factor w/ 2 levels &quot;nonchilled&quot;,&quot;chilled&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ conc     : num  95 175 250 350 500 675 1000 95 175 250 ...
##  $ uptake   : num  16 30.4 34.8 37.2 35.3 39.2 39.7 13.6 27.3 37.1 ...
##  - attr(*, &quot;formula&quot;)=Class &#39;formula&#39;  language uptake ~ conc | Plant
##   .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; 
##  - attr(*, &quot;outer&quot;)=Class &#39;formula&#39;  language ~Treatment * Type
##   .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; 
##  - attr(*, &quot;labels&quot;)=List of 2
##   ..$ x: chr &quot;Ambient carbon dioxide concentration&quot;
##   ..$ y: chr &quot;CO2 uptake rate&quot;
##  - attr(*, &quot;units&quot;)=List of 2
##   ..$ x: chr &quot;(uL/L)&quot;
##   ..$ y: chr &quot;(umol/m^2 s)&quot;</code></pre>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb334-1"><a href="regressionclassification.html#cb334-1" aria-hidden="true" tabindex="-1"></a><span class="fu">levels</span>(datasets<span class="sc">::</span>CO2<span class="sc">$</span>Type)</span></code></pre></div>
<pre><code>## [1] &quot;Quebec&quot;      &quot;Mississippi&quot;</code></pre>
<p>To fit a logistic regression in R we can use the <code>glm()</code> function, which
fits a generalized linear model, indicating that our target variable is
binary and the link function is a logit function. Let’s see the model
output:</p>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb336-1"><a href="regressionclassification.html#cb336-1" aria-hidden="true" tabindex="-1"></a>logmod <span class="ot">&lt;-</span> <span class="fu">glm</span>(Type <span class="sc">~</span> uptake,</span>
<span id="cb336-2"><a href="regressionclassification.html#cb336-2" aria-hidden="true" tabindex="-1"></a>              <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> logit),</span>
<span id="cb336-3"><a href="regressionclassification.html#cb336-3" aria-hidden="true" tabindex="-1"></a>              <span class="at">data =</span> datasets<span class="sc">::</span>CO2)</span>
<span id="cb336-4"><a href="regressionclassification.html#cb336-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(logmod)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Type ~ uptake, family = binomial(link = logit), 
##     data = datasets::CO2)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.29454  -0.66966  -0.02006   0.74112   1.68565  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  3.87192    0.87273   4.437 9.14e-06 ***
## uptake      -0.14130    0.02992  -4.723 2.32e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 116.449  on 83  degrees of freedom
## Residual deviance:  83.673  on 82  degrees of freedom
## AIC: 87.673
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>This fitted model results in a linear classification boundary
(discontinued line) that splits the predictor variables space in two.
Where that line falls depends on the choice of threshold, in this case
<span class="math inline">\(\tau=0.5\)</span> (see where the grey logistic regression line meets the dashed
threshold line). You can see it plotted below:</p>
<div class="sourceCode" id="cb338"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb338-1"><a href="regressionclassification.html#cb338-1" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">coef</span>(logmod)</span>
<span id="cb338-2"><a href="regressionclassification.html#cb338-2" aria-hidden="true" tabindex="-1"></a><span class="co"># reuse previous plot with classification line</span></span>
<span id="cb338-3"><a href="regressionclassification.html#cb338-3" aria-hidden="true" tabindex="-1"></a>plot_2<span class="fl">.1</span> <span class="ot">&lt;-</span> datasets<span class="sc">::</span>CO2 <span class="sc">|&gt;</span></span>
<span id="cb338-4"><a href="regressionclassification.html#cb338-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> uptake, <span class="at">y =</span> <span class="fu">as.numeric</span>(Type)<span class="sc">-</span><span class="dv">1</span>, <span class="at">color =</span> Type)) <span class="sc">+</span></span>
<span id="cb338-5"><a href="regressionclassification.html#cb338-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.75</span>, <span class="at">alpha =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb338-6"><a href="regressionclassification.html#cb338-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;&quot;</span>) <span class="sc">+</span></span>
<span id="cb338-7"><a href="regressionclassification.html#cb338-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span></span>
<span id="cb338-8"><a href="regressionclassification.html#cb338-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_smooth</span>(<span class="at">method =</span> <span class="st">&quot;glm&quot;</span>, <span class="at">color =</span> <span class="st">&quot;grey&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>,</span>
<span id="cb338-9"><a href="regressionclassification.html#cb338-9" aria-hidden="true" tabindex="-1"></a>              <span class="at">method.args =</span> <span class="fu">list</span>(<span class="at">family=</span>binomial)) <span class="sc">+</span></span>
<span id="cb338-10"><a href="regressionclassification.html#cb338-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="sc">-</span>beta[<span class="dv">1</span>]<span class="sc">/</span>beta[<span class="dv">2</span>], <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb338-11"><a href="regressionclassification.html#cb338-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb338-12"><a href="regressionclassification.html#cb338-12" aria-hidden="true" tabindex="-1"></a>plot_2<span class="fl">.1</span></span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-165-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Most blue points fall to one side of the dashed classification line and
most red points to the other side; this is what we wanted. The points
that are in the wrong side of the line are misclassified by the logistic
regression model, we’re trying to minimize that.</p>
<p>Note that, just like for linear regression, a logistic regression model
allows to use categorical explanatory variables and polynomial
transformations of the predictors to achieve better-fitting
classification models.</p>
<p><strong>Model advantages and concerns</strong></p>
<p>One advantage of logistic regression is simplicity. It’s part of the
<em>generalized linear regression</em> family of models and the concept of a
link function used to build such a model can also be used for various
types of response variables (not only binary, but also count data…).
You can find more details in this <a href="https://en.wikipedia.org/wiki/Generalized_linear_model">Wikipedia
article</a>.</p>
<p>Furthermore, logistic regression allows for an interesting
interpretation of its model parameters: <em>odds</em> and <em>log-odds</em>. Odds
represent how much likely it is to find one class versus the other (e.g.
class 1 is twice as likely as class 0 whenever we have probabilities
<span class="math inline">\(66\%\)</span> vs <span class="math inline">\(33\%\)</span>). The <em>odds</em> are defined as the probability of <span class="math inline">\(Y\)</span>
belonging to class 1 divided by the probabiity of belonging to class 0,
and relates to the model parameters as
<span class="math display">\[\frac{P(Y_i=1)}{P(Y_i=0)} = \exp(\beta_0+\beta_1 X_i).\]</span> So the
log-odds are
<span class="math display">\[\log\left(\frac{P(Y_i=1)}{P(Y_i=0)}\right) = \beta_0+\beta_1 X_i.\]</span>
Increases in the values of the predictors affect the odds
multiplicatively and the log-odds linearly.</p>
<p>It is easy to extend a logistic regression model to more than two
classes by fitting models iteratively. For example, first you classify
class 1 against classes 2 and 3; then another logistic regression
classifies class 2 against 3.</p>
<p>Nevertheless, logistic regression relies on statistical assumptions to
fit the parameters and interpret the fitted parameters. So whenever
these assumptions are not met, one must be careful with the conclusions
drawn. Other machine learning methods, that will be covered in Chapters
<a href="supervisedmli.html#supervisedmli">9</a> and <a href="supervisedmlii.html#supervisedmlii">10</a>, can also be used for
classification tasks. These offer more flexibility than logistic
regression (are not necessarily linear) and don’t need to satisfy strict
statistical assumptions.</p>
</div>
<div id="metrics-for-classification" class="section level4 hasAnchor" number="8.2.3.2">
<h4><span class="header-section-number">8.2.3.2</span> Metrics for classification<a href="regressionclassification.html#metrics-for-classification" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Measuring the quality of a classification model is based on counting how
many observations were correctly classified, rather than the distance
between the values predicted by a regression and the true observed
values. These can be represented in a <em>confusion matrix</em>:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="center"><span class="math inline">\(Y = 1\)</span></th>
<th align="center"><span class="math inline">\(Y = 0\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\hat{Y} = 1\)</span></td>
<td align="center">True positives (TP)</td>
<td align="center">False positives (FP)</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\hat{Y} = 0\)</span></td>
<td align="center">False negatives (FN)</td>
<td align="center">True negatives (TN)</td>
</tr>
</tbody>
</table>
<p>In a confusion matrix, correctly classified observations are on the
diagonal and off-diagonal values correspond to different types of
errors. Some of these error types are more relevant for certain
applications.</p>
<p>Imagine that you want to classify whether the water of a river is safe
to drink based on measurements of certain particles or chemicals in the
water (Y=1 means safe, Y=0 means unsafe). It’s much worse to tag as
“safe” a polluted river than to tag as “unsafe” a potable water source,
one must be conservative. In this case, we would prioritize avoiding
false positives and wouldn’t care so much about false negatives.</p>
<p>The following metrics are widely used and highlight different aspects of
our modeling goals.</p>
<ul>
<li><strong>Accuracy</strong> is simply the proportion of outputs that were correctly
classified: <span class="math display">\[ \text{Accuracy}=\frac{\text{TP} + \text{TN}}{N},\]</span>
where <span class="math inline">\(N\)</span> is the number of observations. This is a very common
metric for training ML models and treats both classes as equally
important. It’s naturally extended to multi-class classification and
usually compared to the value <span class="math inline">\(\frac{1}{C}\)</span> where <span class="math inline">\(C\)</span> is the number
of classes.</li>
</ul>
<p>Classification models are usually compared to randomness: How much
better is our model compared to throwing a coin for classification? At
random, we would assign each class <span class="math inline">\(50\%\)</span> of the time. So if we assume
that both classes are as likely to appear, that is, they are <em>balanced</em>,
the accuracy of a random guess would be around <span class="math inline">\(0.5\)</span>. Hence, we want the
accuracy to be “better than random”. If there are <span class="math inline">\(C\)</span> different classes
and the observations are balanced, we want the accuracy to be above
<span class="math inline">\(1/C\)</span>.</p>
<p>A challenge is posed by <em>imbalanced</em> classes. For a dataset where <span class="math inline">\(90\%\)</span>
of the observations are from class 1 and <span class="math inline">\(10\%\)</span> from class 0, always
predicting 1 would lead to a accuracy of <span class="math inline">\(0.9\)</span>. This value may sound
good, but that model is not informative because it doesn’t use any
information from predictors. Therefore, be careful when working with
imbalanced classes and interpreting your results.</p>
<ul>
<li><p><strong>Precision</strong> measures how often our “positive” predictions are
correct:
<span class="math display">\[\text{precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}.\]</span></p></li>
<li><p>The <strong>true positive rate</strong> (TPR), also called <strong>Recall</strong> or
<strong>sensitivity</strong> measures the proportion of real “positives”
(<span class="math inline">\(Y = 1\)</span>) we are able to capture:
<span class="math display">\[ \text{TPR} = \frac{\text{TP}}{\text{TP}+\text{FN}}.\]</span></p></li>
<li><p>The <strong>false positive rate</strong> (FPR) is defined by
<span class="math display">\[\text{FPR} = \frac{\text{FP}}{\text{FP}+\text{TN}}.\]</span> and is
related to another metric called <strong>specificity</strong> by
<span class="math inline">\(\text{FPR} = 1 - \text{specificity}\)</span>.</p></li>
<li><p><strong>Receiver operating characteristic (ROC) curve</strong>: To evaluate the
performance of a binary classification model, it’s common to plot
the <em>ROC curve</em>, where the TPR is plotted against the FPR, for
varying values of the threshold <span class="math inline">\(\tau\)</span> used in the classification
rule. When we decrease the threshold, we get more positive values
(more observations are classified as 1), increasing both the true
positive and false positive rate. The following image describes
clearly how to interpret a ROC curve plot:</p></li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:roccurve"></span>
<img src="figures/Roc_curve.png" alt="ROC curves and how they compare, from [Wikipedia](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)."  />
<p class="caption">
Figure 8.2: ROC curves and how they compare, from <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">Wikipedia</a>.
</p>
</div>
<ul>
<li><p><strong>AUC</strong>: The “area under the curve” is defined as the area left
below the ROC curve. For a random classifier we would have AUC=0.5
and for the perfect classifier, AUC=1. It’s good to try to increase
the AUC and it’s used often as a reporting metric. Nevertheless, a
visual inspection of the ROC curve can say even more.</p></li>
<li><p><strong>F1</strong>: The F1 score is a more sophisticated metric, defined as the
harmonic mean of precision and sensitivity, or in terms of the
confusion matrix values: <span class="math display">\[
F1= 2 \times \frac{\text{precision} \times \text{recall}}{\text{precision} + \text{recall}} = \frac{2 \text{TP}}{2 \text{TP} + \text{FP} + \text{FN}}.
\]</span> This metric provides good results for both balanced and
imbalanced datasets and takes into account both the model’s ability
to capture positive cases (recall) and be correct with the cases it
does capture (precision). It takes values between 0 and 1, with 1
being the best and values of 0.5 and below being bad.</p></li>
</ul>
<p>These metrics can be used to compare the quality of different
classifiers but also to understand the behaviour of a single classifier
from different perspectives.</p>
<p>This was an introduction of the most basic classification metrics. For a
more information on the topic, check out <a href="https://bookdown.org/max/FES/measuring-performance.html#class-metrics">this book
chapter</a>.</p>
<p><strong>Implementation in R</strong></p>
<p>Let’s take a look at the previous metrics for the logistic regression
model we fitted before. The <code>confusionMatrix()</code> function from the
{caret} library provides most of the statistics introduced above.</p>
<div class="sourceCode" id="cb340"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb340-1"><a href="regressionclassification.html#cb340-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Make classification predictions</span></span>
<span id="cb340-2"><a href="regressionclassification.html#cb340-2" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> logmod<span class="sc">$</span>data<span class="sc">$</span>Type</span>
<span id="cb340-3"><a href="regressionclassification.html#cb340-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">round</span>(logmod<span class="sc">$</span>fitted.values)) <span class="co"># Use 0.5 as threshold</span></span>
<span id="cb340-4"><a href="regressionclassification.html#cb340-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb340-5"><a href="regressionclassification.html#cb340-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Change class names</span></span>
<span id="cb340-6"><a href="regressionclassification.html#cb340-6" aria-hidden="true" tabindex="-1"></a><span class="fu">levels</span>(Y) <span class="ot">&lt;-</span> <span class="fu">levels</span>(x) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Quebec&quot;</span>, <span class="st">&quot;Mississippi&quot;</span>)</span>
<span id="cb340-7"><a href="regressionclassification.html#cb340-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb340-8"><a href="regressionclassification.html#cb340-8" aria-hidden="true" tabindex="-1"></a><span class="co"># plot confusion matrix</span></span>
<span id="cb340-9"><a href="regressionclassification.html#cb340-9" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">confusionMatrix</span>(<span class="at">data =</span> x, <span class="at">reference =</span> Y)</span>
<span id="cb340-10"><a href="regressionclassification.html#cb340-10" aria-hidden="true" tabindex="-1"></a>conf_matrix</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##              Reference
## Prediction    Quebec Mississippi
##   Quebec          32          13
##   Mississippi     10          29
##                                          
##                Accuracy : 0.7262         
##                  95% CI : (0.618, 0.8179)
##     No Information Rate : 0.5            
##     P-Value [Acc &gt; NIR] : 2.039e-05      
##                                          
##                   Kappa : 0.4524         
##                                          
##  Mcnemar&#39;s Test P-Value : 0.6767         
##                                          
##             Sensitivity : 0.7619         
##             Specificity : 0.6905         
##          Pos Pred Value : 0.7111         
##          Neg Pred Value : 0.7436         
##              Prevalence : 0.5000         
##          Detection Rate : 0.3810         
##    Detection Prevalence : 0.5357         
##       Balanced Accuracy : 0.7262         
##                                          
##        &#39;Positive&#39; Class : Quebec         
## </code></pre>
<p>Now we can visualize the confusion matrix as a mosaic plot. This is
quite helpful when we work with many classes.</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb342-1"><a href="regressionclassification.html#cb342-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mosaicplot</span>(conf_matrix<span class="sc">$</span>table,</span>
<span id="cb342-2"><a href="regressionclassification.html#cb342-2" aria-hidden="true" tabindex="-1"></a>           <span class="at">main =</span> <span class="st">&quot;Confusion matrix&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-167-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="model-evaluation" class="section level3 hasAnchor" number="8.2.4">
<h3><span class="header-section-number">8.2.4</span> Model evaluation<a href="regressionclassification.html#model-evaluation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Model evaluation refers to several techniques that help you understand
how the model performs, whether this behavior is what you expect and how
you can improve it. You can use metrics and plots to get an overview of
the weaknesses of your model. This section covers model comparison,
variable selection and outlier detection, and more concepts related to
model evaluation (overfitting, data pre-processing, cross-validation…)
are explained in the remaining chapters. Concepts will be explained
using regression as an example, but are directly translated to
classification problems.</p>
<div id="model-comparison" class="section level4 hasAnchor" number="8.2.4.1">
<h4><span class="header-section-number">8.2.4.1</span> Model comparison<a href="regressionclassification.html#model-comparison" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Be systematic with your model comparisons. Three key ideas in model
selection are:</p>
<ul>
<li>Comparisons should be hierarchical: compare a model to another that
“contains it”, i.e. compare <code>GPP_NT_VUT_REF ~ SW_IN_F</code> to
<code>GPP_NT_VUT_REF ~ SW_IN_F + LW_IN_F</code>, not <code>GPP_NT_VUT_REF ~ SW_IN_F</code>
to <code>GPP_NT_VUT_REF ~ NIGHT + TA_F</code>.</li>
<li>Complexity must be increased slowly: add one variable at a time, not
three variables all at once. This helps avoid collinearity in the
predictors.</li>
<li>Choose the most appropriate metric: if possible, a metric that
accounts for model complexity and represents the goal of your
analysis (e.g., recall for a classification where you don’t want to
miss any positives).</li>
</ul>
<p>If you’re considering different model approaches for the same task, you
should first fit the best possible model for each approach, and then
compare those optimized models to each other. For example, fit the best
linear regression with your available data, the best KNN non-parametric
regression model and a random forest; then compare those three final
models and choose the one that answers your research question the best.</p>
<p>One must be careful not to keep training or improving models until they
fit the data perfectly, but maintain the models’ ability to generalize
to newly available data. Chapter <a href="supervisedmli.html#supervisedmli">9</a> introduces the
concept of overfitting, which is central to data science. Think of model
interpretation and generalization when comparing them, not only of
performance. Simple models can be more valuable than very complex ones
because they tell a better story about the data (e.g., by having few
very good predictors rather than thousands of mediocre ones, from which
we cannot learn the underlying relationships).</p>
</div>
<div id="variable-selection-1" class="section level4 hasAnchor" number="8.2.4.2">
<h4><span class="header-section-number">8.2.4.2</span> Variable selection<a href="regressionclassification.html#variable-selection-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let’s think of variable selection in the context of linear regression. A
brute force approach to variable selection would be: Fit a linear
regression for each combination of available predictors, calculate a
metric (e.g., AIC) and choose the best one (lowest AIC). The problem is,
if you have 8 predictors, you would fit 40320 different regression
models. This can be very computationally expensive.</p>
<p>Instead, take a hierarchical, or “greedy”, approach, starting with an
empty model (just an intercept) and adding one variable at a time. This
is called <strong>stepwise (forward) regression</strong>. The algorithm goes as
follows:</p>
<ul>
<li>First, you fit all regression models with just one variable and
compute the <span class="math inline">\(R^2\)</span>.</li>
<li>Then, select the one predictor leading to a model with the greatest
<span class="math inline">\(R^2\)</span> (best fitting model) and compute the AIC (or BIC).</li>
<li>In the next step, compare remaining predictors to be added as a
second variable in the model and calculating their <span class="math inline">\(R^2\)</span>.</li>
<li>Choose as second predictor the one leading to the best <span class="math inline">\(R^2\)</span>. Then,
compute the AIC. If the AIC (which accounts for model fit and
complexity) is worse, that is, bigger, stop and keep the univariate
linear model. If the AIC is better, that is, smaller, add the second
variable and repeat the previous steps to include a third variable.
The method finishes once you cannot reduce the AIC anymore, or when
you run out of variables. In the end, you’ll have more or less the
best possible linear regression model. The function <code>step()</code>
implements the stepwise algorithm in R.</li>
</ul>
<p>This stepwise approach can also be done backwards, starting with a full
model (all available variables) and removing one at a time. Or even with
a back-and-forth approach, where you look at both including a new or
removing an existing variable at each step (optimizing AIC).
Furthermore, this algorithm can be applied to fitting a polynomial
regression. We want to increase the degree of the polynomials unit by
unit. For a model with categorical variables, interaction terms should
only be considered after having the involved variables as “intercept
only”.</p>
<p><strong>Multicollinearity</strong> exists when there is a correlation between
multiple predictors in a multivariate regression model. This is
problematic because it makes the estimated coefficients corresponding to
the variables that are highly correlated very unstable. Since two highly
correlated variables explain almost the same, it doesn’t matter whether
we include one or the other in the model (the performance metrics will
be similar) or even if we include both of them. Hence, it becomes
difficult to say which variables actually influence the target.</p>
<p>The <strong>variance inflation factor (VIF)</strong> is a score from economics that
measures the amount of multicollinearity in regression, based on how the
estimated variance of a coefficient is inflated due to its correlation
with another predictor. It’s calculated as
<span class="math display">\[\text{VIF}_i = \frac{1}{1 - R^2_i},\]</span> where <span class="math inline">\(R^2_i\)</span> is the coefficient
of determination for regressing the i<span class="math inline">\(^{th}\)</span> predictor on the remaining
ones. A VIF<span class="math inline">\(_i\)</span> is computed for each predictor in the multivariate
regression model we are evaluating, meaning: if <span class="math inline">\(\text{VIF}_i = 1\)</span>
variables are not correlated; if <span class="math inline">\(1 &lt; \text{VIF}_i &lt; 5\)</span> there is
moderate collinearity; and if <span class="math inline">\(\text{VIF}_i \geq 5\)</span> they are highly
correlated. Because they can be almost fully explained by all the other
predictors (high <span class="math inline">\(R^2_i\)</span>), these variables are redundant in our final
model.</p>
<p>To remedy collinearity, you may choose to use only one or two of those
correlated variables. Another option would be to use <em>Principal
Component Analysis</em> (PCA), which you may read more about
<a href="https://www.r-bloggers.com/2021/05/principal-component-analysis-pca-in-r/">here</a>.
What PCA does is to map the space of predictors into another of smaller
dimension, leading to a smaller set of predictor variables
<span class="math inline">\(\{Z_1, ... , Z_q\}\)</span>, each of them being a linear combination of all the
initial available predictors, that is
<span class="math inline">\(Z_1 = \alpha^1_0 + \alpha^1_1 X_1 + ... + \alpha^1_p X_p\)</span>, etc. If you
have collinearity, those highly correlated variables would be summarized
into one single new variable, called <em>principal component</em>.</p>
<p>When we work with high-dimensional data (that is, we have more variables
than observations) there are better techniques to do variable selection
than stepwise regression. Since the predictors space is so large, we
could fit a line that passes through all the observations (a perfect
fit), but does the model generalize? We don’t know. For example, Lasso
and Ridge regression incorporate variable selection in the fitting
process (you can check <a href="https://www.r-bloggers.com/2020/06/understanding-lasso-and-ridge-regression/">this
post</a>
if you’re curious).</p>
</div>
<div id="outlier-detection" class="section level4 hasAnchor" number="8.2.4.3">
<h4><span class="header-section-number">8.2.4.3</span> Outlier detection<a href="regressionclassification.html#outlier-detection" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Detecting outliers is important, because they can affect the fit of a
model and render the model fitting not robust. When the data is large,
individual points have less influence on the model fitting. Therefore
only outliers that are very far from normal values will affect the model
fit (see below). Outliers are particularly problematic in the context of
small data, because every value has a big influence on the fitted model.</p>
<p>Take a look at the two linear regressions below and how one single weird
observation can throw off the fit. Whenever an observation is very
distant from the center of the predictor’s distribution, it becomes very
influential (it has a big <em>leverage</em>). If the observed response for that
data point is in harmony with the rest of points, nothing happens, but
if it’s also off, the regression model will be affected greatly.</p>
<div class="sourceCode" id="cb343"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb343-1"><a href="regressionclassification.html#cb343-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2023</span>)</span>
<span id="cb343-2"><a href="regressionclassification.html#cb343-2" aria-hidden="true" tabindex="-1"></a>half_hourly_fluxes_small <span class="ot">&lt;-</span> half_hourly_fluxes <span class="sc">|&gt;</span></span>
<span id="cb343-3"><a href="regressionclassification.html#cb343-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sample_n</span>(<span class="dv">100</span>) <span class="sc">|&gt;</span> <span class="co"># reduce dataset</span></span>
<span id="cb343-4"><a href="regressionclassification.html#cb343-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(SW_IN_F, GPP_NT_VUT_REF)</span>
<span id="cb343-5"><a href="regressionclassification.html#cb343-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb343-6"><a href="regressionclassification.html#cb343-6" aria-hidden="true" tabindex="-1"></a>plot_3 <span class="ot">&lt;-</span> half_hourly_fluxes_small <span class="sc">|&gt;</span></span>
<span id="cb343-7"><a href="regressionclassification.html#cb343-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> SW_IN_F, <span class="at">y =</span> GPP_NT_VUT_REF)) <span class="sc">+</span></span>
<span id="cb343-8"><a href="regressionclassification.html#cb343-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.75</span>) <span class="sc">+</span></span>
<span id="cb343-9"><a href="regressionclassification.html#cb343-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>, <span class="at">fullrange =</span> <span class="cn">TRUE</span>) <span class="sc">+</span></span>
<span id="cb343-10"><a href="regressionclassification.html#cb343-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;Shortwave radiation (W m&quot;</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">&quot;)&quot;</span>)), </span>
<span id="cb343-11"><a href="regressionclassification.html#cb343-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;GPP (gC m&quot;</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">&quot;s&quot;</span><span class="sc">^-</span><span class="dv">1</span>, <span class="st">&quot;)&quot;</span>))) <span class="sc">+</span></span>
<span id="cb343-12"><a href="regressionclassification.html#cb343-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span></span>
<span id="cb343-13"><a href="regressionclassification.html#cb343-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="sc">-</span><span class="dv">20</span>, <span class="dv">40</span>) <span class="sc">+</span> </span>
<span id="cb343-14"><a href="regressionclassification.html#cb343-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">0</span>, <span class="dv">2000</span>)</span>
<span id="cb343-15"><a href="regressionclassification.html#cb343-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb343-16"><a href="regressionclassification.html#cb343-16" aria-hidden="true" tabindex="-1"></a>plot_4 <span class="ot">&lt;-</span> half_hourly_fluxes_small <span class="sc">|&gt;</span></span>
<span id="cb343-17"><a href="regressionclassification.html#cb343-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_row</span>(<span class="at">SW_IN_F =</span> <span class="dv">2000</span>, <span class="at">GPP_NT_VUT_REF =</span> <span class="sc">-</span><span class="dv">20</span>) <span class="sc">|&gt;</span> <span class="co"># add outlier</span></span>
<span id="cb343-18"><a href="regressionclassification.html#cb343-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> SW_IN_F, <span class="at">y =</span> GPP_NT_VUT_REF)) <span class="sc">+</span></span>
<span id="cb343-19"><a href="regressionclassification.html#cb343-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.75</span>) <span class="sc">+</span></span>
<span id="cb343-20"><a href="regressionclassification.html#cb343-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>, <span class="at">fullrange =</span> <span class="cn">TRUE</span>) <span class="sc">+</span></span>
<span id="cb343-21"><a href="regressionclassification.html#cb343-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;Shortwave radiation (W m&quot;</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">&quot;)&quot;</span>)), </span>
<span id="cb343-22"><a href="regressionclassification.html#cb343-22" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;GPP (gC m&quot;</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">&quot;s&quot;</span><span class="sc">^-</span><span class="dv">1</span>, <span class="st">&quot;)&quot;</span>))) <span class="sc">+</span></span>
<span id="cb343-23"><a href="regressionclassification.html#cb343-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span></span>
<span id="cb343-24"><a href="regressionclassification.html#cb343-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="dv">2000</span>, <span class="at">y =</span> <span class="sc">-</span><span class="dv">20</span>), <span class="at">colour=</span><span class="st">&#39;blue&#39;</span>) <span class="sc">+</span></span>
<span id="cb343-25"><a href="regressionclassification.html#cb343-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="sc">-</span><span class="dv">20</span>, <span class="dv">40</span>) <span class="sc">+</span> </span>
<span id="cb343-26"><a href="regressionclassification.html#cb343-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">0</span>, <span class="dv">2000</span>)</span>
<span id="cb343-27"><a href="regressionclassification.html#cb343-27" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb343-28"><a href="regressionclassification.html#cb343-28" aria-hidden="true" tabindex="-1"></a>cowplot<span class="sc">::</span><span class="fu">plot_grid</span>(plot_3, plot_4)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;
## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-168-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The first step to identifying outliers is to look at your data, one
variable at a time. Plot a histogram to see the rough distribution of a
variable. This will help identify what kind of values to expect. In
Chapters <a href="datawrangling.html#datawrangling">3</a> and <a href="datavis.html#datavis">4</a> it was introduced how
to identify values that fell out of this distribution using histograms
and boxplots. Checking in the histogram if the distribution has fat
tails helps to discern whether the values that pop out of a boxplot
should be considered outliers or not.</p>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb345-1"><a href="regressionclassification.html#cb345-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create an outlier for demonstration purposes</span></span>
<span id="cb345-2"><a href="regressionclassification.html#cb345-2" aria-hidden="true" tabindex="-1"></a>half_hourly_fluxes_outlier <span class="ot">&lt;-</span> half_hourly_fluxes_small <span class="sc">|&gt;</span></span>
<span id="cb345-3"><a href="regressionclassification.html#cb345-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_row</span>(<span class="at">SW_IN_F =</span> <span class="dv">2000</span>, <span class="at">GPP_NT_VUT_REF =</span> <span class="sc">-</span><span class="dv">20</span>)</span>
<span id="cb345-4"><a href="regressionclassification.html#cb345-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb345-5"><a href="regressionclassification.html#cb345-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Various ways to identify the outlier using graphs</span></span>
<span id="cb345-6"><a href="regressionclassification.html#cb345-6" aria-hidden="true" tabindex="-1"></a>plot_5 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(</span>
<span id="cb345-7"><a href="regressionclassification.html#cb345-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> half_hourly_fluxes_outlier,</span>
<span id="cb345-8"><a href="regressionclassification.html#cb345-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x =</span> GPP_NT_VUT_REF, <span class="at">y =</span> <span class="fu">after_stat</span>(density))) <span class="sc">+</span></span>
<span id="cb345-9"><a href="regressionclassification.html#cb345-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">fill =</span> <span class="st">&quot;grey70&quot;</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span></span>
<span id="cb345-10"><a href="regressionclassification.html#cb345-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">color =</span> <span class="st">&#39;red&#39;</span>)<span class="sc">+</span></span>
<span id="cb345-11"><a href="regressionclassification.html#cb345-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&#39;Histogram, density and boxplot&#39;</span>, </span>
<span id="cb345-12"><a href="regressionclassification.html#cb345-12" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;GPP (gC m&quot;</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">&quot;s&quot;</span><span class="sc">^-</span><span class="dv">1</span>, <span class="st">&quot;)&quot;</span>))) <span class="sc">+</span></span>
<span id="cb345-13"><a href="regressionclassification.html#cb345-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>()</span>
<span id="cb345-14"><a href="regressionclassification.html#cb345-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb345-15"><a href="regressionclassification.html#cb345-15" aria-hidden="true" tabindex="-1"></a>plot_6 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(</span>
<span id="cb345-16"><a href="regressionclassification.html#cb345-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> half_hourly_fluxes_outlier,</span>
<span id="cb345-17"><a href="regressionclassification.html#cb345-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x =</span> <span class="st">&quot;&quot;</span>, <span class="at">y =</span> GPP_NT_VUT_REF)) <span class="sc">+</span></span>
<span id="cb345-18"><a href="regressionclassification.html#cb345-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>(<span class="at">fill =</span> <span class="st">&quot;grey70&quot;</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span></span>
<span id="cb345-19"><a href="regressionclassification.html#cb345-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>() <span class="sc">+</span></span>
<span id="cb345-20"><a href="regressionclassification.html#cb345-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span></span>
<span id="cb345-21"><a href="regressionclassification.html#cb345-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.text.y=</span><span class="fu">element_blank</span>(),</span>
<span id="cb345-22"><a href="regressionclassification.html#cb345-22" aria-hidden="true" tabindex="-1"></a>        <span class="at">axis.ticks.y=</span><span class="fu">element_blank</span>()) <span class="sc">+</span></span>
<span id="cb345-23"><a href="regressionclassification.html#cb345-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;GPP (gC m&quot;</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">&quot;s&quot;</span><span class="sc">^-</span><span class="dv">1</span>, <span class="st">&quot;)&quot;</span>)))</span>
<span id="cb345-24"><a href="regressionclassification.html#cb345-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb345-25"><a href="regressionclassification.html#cb345-25" aria-hidden="true" tabindex="-1"></a>plot_7 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(</span>
<span id="cb345-26"><a href="regressionclassification.html#cb345-26" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> half_hourly_fluxes_outlier,</span>
<span id="cb345-27"><a href="regressionclassification.html#cb345-27" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x =</span> SW_IN_F, <span class="at">y =</span> <span class="fu">after_stat</span>(density))) <span class="sc">+</span></span>
<span id="cb345-28"><a href="regressionclassification.html#cb345-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">fill =</span> <span class="st">&quot;grey70&quot;</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span></span>
<span id="cb345-29"><a href="regressionclassification.html#cb345-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">color =</span> <span class="st">&#39;red&#39;</span>)<span class="sc">+</span></span>
<span id="cb345-30"><a href="regressionclassification.html#cb345-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&#39;Histogram, density and boxplot&#39;</span>, </span>
<span id="cb345-31"><a href="regressionclassification.html#cb345-31" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;Shortwave radiation (W m&quot;</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">&quot;)&quot;</span>))) <span class="sc">+</span></span>
<span id="cb345-32"><a href="regressionclassification.html#cb345-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>()</span>
<span id="cb345-33"><a href="regressionclassification.html#cb345-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb345-34"><a href="regressionclassification.html#cb345-34" aria-hidden="true" tabindex="-1"></a>plot_8 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(</span>
<span id="cb345-35"><a href="regressionclassification.html#cb345-35" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> half_hourly_fluxes_outlier,</span>
<span id="cb345-36"><a href="regressionclassification.html#cb345-36" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x =</span> <span class="st">&quot;&quot;</span>, <span class="at">y =</span> SW_IN_F)) <span class="sc">+</span></span>
<span id="cb345-37"><a href="regressionclassification.html#cb345-37" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>(<span class="at">fill =</span> <span class="st">&quot;grey70&quot;</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span></span>
<span id="cb345-38"><a href="regressionclassification.html#cb345-38" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>() <span class="sc">+</span></span>
<span id="cb345-39"><a href="regressionclassification.html#cb345-39" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>() <span class="sc">+</span></span>
<span id="cb345-40"><a href="regressionclassification.html#cb345-40" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.text.y=</span><span class="fu">element_blank</span>(),</span>
<span id="cb345-41"><a href="regressionclassification.html#cb345-41" aria-hidden="true" tabindex="-1"></a>        <span class="at">axis.ticks.y=</span><span class="fu">element_blank</span>()) <span class="sc">+</span></span>
<span id="cb345-42"><a href="regressionclassification.html#cb345-42" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;Shortwave radiation (W m&quot;</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">&quot;)&quot;</span>)))</span>
<span id="cb345-43"><a href="regressionclassification.html#cb345-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb345-44"><a href="regressionclassification.html#cb345-44" aria-hidden="true" tabindex="-1"></a>cowplot<span class="sc">::</span><span class="fu">plot_grid</span>(plot_5, plot_7, plot_6, plot_8,</span>
<span id="cb345-45"><a href="regressionclassification.html#cb345-45" aria-hidden="true" tabindex="-1"></a>                   <span class="at">ncol =</span> <span class="dv">2</span>, <span class="at">rel_heights =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">1</span>),</span>
<span id="cb345-46"><a href="regressionclassification.html#cb345-46" aria-hidden="true" tabindex="-1"></a>                   <span class="at">align =</span> <span class="st">&#39;v&#39;</span>, <span class="at">axis =</span> <span class="st">&#39;lr&#39;</span>)</span></code></pre></div>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-169-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>A <strong>Q-Q Plot</strong> depicts the sample quantiles of a variable against the
theoretical quantiles of a distribution of our choice, usually a normal
distribution. In the histograms above, GPP looks somewhat Gaussian but
with fatter tails and slightly skewed to the right, while shortwave
radiation is skewed to the right, resembling an exponential
distribution. This is also visible in the Q-Q plots below, because
outliers deviate greatly from the straight line (which represents a
match between the observed values and the theoretical distribution):</p>
<div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb347-1"><a href="regressionclassification.html#cb347-1" aria-hidden="true" tabindex="-1"></a>plot_9 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(</span>
<span id="cb347-2"><a href="regressionclassification.html#cb347-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> half_hourly_fluxes_outlier,</span>
<span id="cb347-3"><a href="regressionclassification.html#cb347-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">sample =</span> GPP_NT_VUT_REF)) <span class="sc">+</span></span>
<span id="cb347-4"><a href="regressionclassification.html#cb347-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_qq</span>() <span class="sc">+</span></span>
<span id="cb347-5"><a href="regressionclassification.html#cb347-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_qq_line</span>() <span class="sc">+</span></span>
<span id="cb347-6"><a href="regressionclassification.html#cb347-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;GPP (gC m&quot;</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">&quot;s&quot;</span><span class="sc">^-</span><span class="dv">1</span>, <span class="st">&quot;)&quot;</span>)),</span>
<span id="cb347-7"><a href="regressionclassification.html#cb347-7" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;Theoretical normal quantiles&quot;</span>) <span class="sc">+</span></span>
<span id="cb347-8"><a href="regressionclassification.html#cb347-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>()</span>
<span id="cb347-9"><a href="regressionclassification.html#cb347-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb347-10"><a href="regressionclassification.html#cb347-10" aria-hidden="true" tabindex="-1"></a>plot_10 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(</span>
<span id="cb347-11"><a href="regressionclassification.html#cb347-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> half_hourly_fluxes_outlier,</span>
<span id="cb347-12"><a href="regressionclassification.html#cb347-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">sample =</span> SW_IN_F)) <span class="sc">+</span></span>
<span id="cb347-13"><a href="regressionclassification.html#cb347-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_qq</span>() <span class="sc">+</span></span>
<span id="cb347-14"><a href="regressionclassification.html#cb347-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_qq_line</span>() <span class="sc">+</span></span>
<span id="cb347-15"><a href="regressionclassification.html#cb347-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;Shortwave radiation (W m&quot;</span><span class="sc">^-</span><span class="dv">2</span>, <span class="st">&quot;)&quot;</span>)),</span>
<span id="cb347-16"><a href="regressionclassification.html#cb347-16" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;Theoretical normal quantiles&quot;</span>) <span class="sc">+</span></span>
<span id="cb347-17"><a href="regressionclassification.html#cb347-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_classic</span>()</span>
<span id="cb347-18"><a href="regressionclassification.html#cb347-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb347-19"><a href="regressionclassification.html#cb347-19" aria-hidden="true" tabindex="-1"></a>cowplot<span class="sc">::</span><span class="fu">plot_grid</span>(plot_9, plot_10, <span class="at">ncol=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-170-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>For linear (and logistic) regression, we would like predictor variables
to look as normal, i.e. gaussian, as possible. You’ve probably learned some of the
reasons for this in quantitative methods courses, but are beyond the
scope of this class. It’s common to study the distribution of the
regression residuals with QQ-plots to assess if model assumptions are
met.</p>
<p>Above, you can see the distributions of our target and predictor (with
outliers). And it’s very easy to see the weird value for the shortwave
radiation but for GPP it doesn’t stick out so much. This already points
to how important it is to check their multivariate distribution. R
provides some useful plots from the fitted regression objects, in
particular the “Residuals vs Leverage” plot:</p>
<div class="sourceCode" id="cb348"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb348-1"><a href="regressionclassification.html#cb348-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit regression with outlier</span></span>
<span id="cb348-2"><a href="regressionclassification.html#cb348-2" aria-hidden="true" tabindex="-1"></a>linmod_outlier <span class="ot">&lt;-</span> <span class="fu">lm</span>(GPP_NT_VUT_REF <span class="sc">~</span> SW_IN_F, </span>
<span id="cb348-3"><a href="regressionclassification.html#cb348-3" aria-hidden="true" tabindex="-1"></a>                     <span class="at">data =</span> <span class="fu">add_row</span>(half_hourly_fluxes_small, <span class="at">SW_IN_F =</span> <span class="dv">2000</span>, <span class="at">GPP_NT_VUT_REF =</span> <span class="sc">-</span><span class="dv">20</span>))</span>
<span id="cb348-4"><a href="regressionclassification.html#cb348-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb348-5"><a href="regressionclassification.html#cb348-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(linmod_outlier, <span class="dv">5</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-171-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>This plot shows the leverage (see the mathematical definition
<a href="https://en.wikipedia.org/wiki/Leverage_(statistics)">here</a>) of each
observation against the corresponding residual from the fitted linear
regression. Points with high leverage, i.e., points that are far from
the center of the predictor distribution, and large residuals, i.e.,
points that are far from the fitted regression line, are very
influential. <strong>Cook’s distance</strong> (definition
<a href="https://en.wikipedia.org/wiki/Cook%27s_distance">here</a>) is an estimate
of the influence of a data point in a linear regression and observations
with Cook’s distance &gt; 1 are candidates for being outliers. See in the
plot above how the point with index 101 (our added outlier) has a very
large Cook’s distance. Boundary regions for Cook’s distance equal to 0.5
(suspicious) and 1 (certainly influential) are drawn with a dashed line.</p>
<p>Finally, it’s very important that, before you remove a value because it
may be an outlier, you understand where the data came from and if such
an abnormal observation is possible. If it depicts an extraordinary but
possible situation, this information can be very valuable and it’s wiser
to keep it in the model. Interesting research questions arise when data
doesn’t align with our preconceptions, so keep looking into it and
potentially collect more data.</p>
</div>
</div>
</div>
<div id="exercises-6" class="section level2 hasAnchor" number="8.3">
<h2><span class="header-section-number">8.3</span> Exercises<a href="regressionclassification.html#exercises-6" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are no exercises with provided solutions for this Chapter.</p>
<blockquote>
<p><em>Hint</em>: For all exercises remember the resources we provided on finding help in section <a href="programmingprimers.html#findinghelp">2.2.3</a>.</p>
</blockquote>
<!-- -   Implement the formula for RMSE using simple "low-level" functions -->
<!--     like `sqrt()` and `mean()`. Confirm that the function `rmse()` from -->
<!--     the yardstick package computes the RMSE the same way. -->
<!--     ```{r} -->
<!--     ## generate random data  -->
<!--     df <- tibble(x = rnorm(100)) |>  -->
<!--       mutate(y = x + rnorm(100), -->
<!--              x = x) -->
<!--     yardstick::rmse(df, y, x) |> pull(.estimate) -->
<!--     sqrt(mean((df$x - df$y)^2)) -->
<!--     ``` -->
</div>
<div id="report-exercise-1" class="section level2 hasAnchor" number="8.4">
<h2><span class="header-section-number">8.4</span> Report Exercise<a href="regressionclassification.html#report-exercise-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="stepwise-regression-and-asking-for-help" class="section level3 unnumbered hasAnchor">
<h3>Stepwise regression and asking for help<a href="regressionclassification.html#stepwise-regression-and-asking-for-help" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="context" class="section level4 unnumbered hasAnchor">
<h4>Context<a href="regressionclassification.html#context" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Although there is a lot of helpful information out there, you will often have to write your own data analysis routine. This requires good understanding of statistical knowledge, algorithmic thinking and problem-solving skills. While writing your code, you will face many questions and bugs that you need to solve. And knowing where and how to ask for help properly are crucial parts of this process (see Chapter <a href="programmingprimers.html#programmingprimers">2</a> for more on getting help). To learn these skills, you will attempt to write your own step-wise forward regression from scratch using the theory and code provided in this and previous chapters.</p>
</div>
<div id="deliverables-for-the-report" class="section level4 unnumbered hasAnchor">
<h4>Deliverables for the report<a href="regressionclassification.html#deliverables-for-the-report" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Important:</strong> For your final report you have to hand in the following two parts:
1. A comparison of all bivariate models, showing which predictor is the best single predictor and an interpretation of these results.
2. A minimum reproducible example of an error that you faced while solving the full stepwise regression.</p>
<div id="find-the-single-best-predictor" class="section level5 unnumbered hasAnchor">
<h5>Find the single best predictor<a href="regressionclassification.html#find-the-single-best-predictor" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The key ingredient of a stepwise forward regression is to find the single most powerful predictor that improves your linear regression model the best. To do so, you first have to find a way to compare all possible models and then pick the best predictor. Your task is to write a reproducible script that includes the following steps:</p>
<ul>
<li>Download and load this data <code>https://raw.githubusercontent.com/geco-bern/agds/main/data/df_for_stepwise_regression.csv</code></li>
<li>Define the target variable <code>GPP_NT_VUT_REF</code> and use all remaining numerical variables as predictors.</li>
<li>Calculate all possible bivariate linear regression models and compare them using your model metric of choice.</li>
<li>Communicate your finding with your diplay item of choice.</li>
<li>Write an interpretation of your results, discussing why some predictors are better than others.</li>
</ul>
<blockquote>
<p>Hint: Some variables may hold more <code>NA</code> than others, messing with your model metrics. What do you need to do to be able to make the next best model comparable to the current one, when adding a new variable?</p>
</blockquote>
<blockquote>
<p>Hint: To better discuss your results, you might want to look up what each variable stands for.</p>
</blockquote>
</div>
<div id="full-stepwise-regression" class="section level5 unnumbered hasAnchor">
<h5>Full stepwise regression<a href="regressionclassification.html#full-stepwise-regression" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The next part of this exercise is to expand your algorithm to perform a full stepwise regression. We do not expect that all of you solve this exercise without any issues - which is completely fine! Therefore, in your report, you will add the code up to the point you got stuck <strong>and</strong> you will have to add a minimum reproducible example (MRE), following <a href="https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example">this guideline</a>. Write this MRE as if you were addressing an online audience; describe in detail what you goal is, where you got stuck, what the error message is, provide a code example that is runnable without needing the .csv files that you have locally (e.g., <code>dput()</code> or reading data from the web), etc. If you can solve this exercise without issues, congratulations! But it is likely that faced some error messages along the way. So, take one of these errors to write the MRE you have to submit.</p>
<p>Code errors can break knitting your RMarkdown to HTML. To avoid having issues, make sure that you set the chunks that hold erroneous code, to <code>eror = TRUE</code>, or skip running them entirely using <code>eval = FALSE</code> (more info <a href="https://bookdown.org/yihui/rmarkdown-cookbook/opts-error.html">here</a>).</p>
<p>Now, re-read the section on stepwise regression of this tutorial and write a “pseudo-code” for how you would encode a stepwise regression, write-out using coding terms like “I have to loop over all predictors and for each predictor I have to check…”. Find a way to sequentally add the next best variable to your model and save that variable’s name and the model metric. Visualise the result of the step-wise regression so that it shows how the chosen metric varies with the number of predictors, preferably showing the name of the additional predictor. What does your result suggest? Did you expect that form the interpetation of finding the best single predictor? What dynamics of the model metrics can you see and can you explain them using the theory from the tutorial?</p>
<blockquote>
<p>Hint: Consider using loops and <a href="https://stackoverflow.com/questions/4951442/formula-with-dynamic-number-of-variables">dynamic formula creation</a> to make your code shorter and easier to read.</p>
</blockquote>
<!-- ```{r} -->
<!-- df <- readr::read_csv("data/df_for_stepwise_regression.csv") -->
<!-- ``` -->
<!-- *Hints*: -->
<!-- -   Really try to think of the blueprint (*pseudo-code*) first: How to go through different models in each forward step? How to store predictors added to the model and how to update candidate predictors? -->
<!-- -   You will have to add predictors sequentally to your final model as you search through all predictors. You can use `new_list <- list()` to create an empty vector, and then add elements to that list by `new_list <- c(new_list, new_element)`. -->
<!-- -   It may be helpful to explicitly define a set of "candidate predictors" that are potentially added to the model (e.g., `preds_candidate`), and define predictors retained in the model from the previous step in a separate vector (e.g., `preds_retained`). In each step, search through `preds_candidate`, select the best predictor, add it to `preds_retained` and remove it from `preds_candidate`. -->
<!-- -   At each step, record the metrics and store them in a data frame for later plots. As in the first "warm-up" exercise, you may record metrics at each step as a single-row data frame and sequentially stack (bind) them together. -->
<!-- -   You will have to create new formulas for models dynamically, based on the candidate predictors. A clever way how to construct formulas dynamically is described in [this stackoverflow post](https://stackoverflow.com/questions/4951442/formula-with-dynamic-number-of-variables). -->
<!-- <!-- -   The metrics for each model are assessed *after* the order of added variables is determined. To be able to determine the metrics, the models can be saved by constructing a list of models and sequentially add elements to that list (`mylist[[ name_new_element ]] <- new_element`). You can also fit the model again after determining which predictor worked best. -->
<p>–&gt;</p>
<!-- <!-- -   Your code will most certainly have bugs at first. To debug efficiently, write code first in a simple R script and use the debugging options in RStudio (see [here](https://support.rstudio.com/hc/en-us/articles/205612627-Debugging-with-RStudio)). -->
<p>–&gt;</p>
<!-- -   For visualising: To display a table nicely as part of the RMarkdown html output, use the function `knitr::kable()`. To avoid reordering the list of variable names when plotting, change the type of variable names from "character" to "factor" by `pred <- factor(pred, levels = pred)` -->
<!-- #### Warm-Up Exercises {-} -->
<!-- #### Nested loops and if-statements -->
<!-- Given a matrix A and a vector B (see below), do the following tasks: -->
<!-- -   Replace the missing values (`NA`) in the first row of A by the largest value of B. After using that element of B for imputing A, drop that element from the vector B and proceed with imputing the second row of A, using the (now) largest value of the updated vector B, and drop that element from B after using it for imputing A. Repeat the same procedure for all four rows in A. -->
<!-- -   After imputing (replacing) in each step, calculate the mean of the remaining values in B and record it as a single-row data frame with two columns `row_number` and `avg`, where `row_number` is the row number of A where the value was imputed, and `avg` is the mean of remaining values in B. As the algorithm proceeds through rows in A, sequentially bind the single-row data frame together so that after completion of the algorithm, the data frame contains four rows (corresponding to the number of rows in A). -->
<!-- ```{r} -->
<!-- A <- matrix(c(6, 7, 3, NA, 15, 6, 7,  -->
<!--               8, 9, 12, 6, 11, NA, 3,  -->
<!--               9, 4, 7, 3, 21, NA, 6,  -->
<!--               7, 19, 6, NA, 15, 8, 10), -->
<!--             nrow = 4, byrow = TRUE) -->
<!-- B <- c(8, 4, 12, 9, 15, 6) -->
<!-- ``` -->
<!-- Before implementing these tasks, try to write down a pseudo code. This is code-like text that may not be executable, but describes the structure of real code and details where and how major steps are implemented. Next, you'll need to write actual R code. For this, you will need to find answers to the following questions: -->
<!-- -   How to go through each of the element in matrix? -->
<!-- -   How to detect NA value? -->
<!-- -   How to drop an element of a given value from a vector? -->
<!-- -   How to add a row to an existing data frame? -->

</div>
</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="codemgmt.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="supervisedmli.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
