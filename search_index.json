[["index.html", "Applied Geodata Science Overview About this book About this course", " Applied Geodata Science Benjamin Stocker, Koen Hufkens, Pepa Arán, and Pascal Schneider 2023-11-07 Overview About this book This book serves as the basis for the series of courses in Applied Geodata Science, taught at the Institute of Geography, University of Bern. The starting point of this book were the tutorials edited by Benjamin Stocker, Loïc Pellissier, and Joshua Payne for the course Environmental Systems Data Science (D-USYS, ETH Zürich). The present book was written as a collaborative effort led by Benjamin Stocker, with contributions by Pepa Arán and Koen Hufkens, and exercises by Pascal Schneider. The target of this book are people interested in applying data science methods for research. Methods, example data sets, and prediction challenges are chosen to make the book most relatable to scientists and students in Geography and Environmental Sciences. No prior knowledge of coding is required. Respective essentials are briefly introduced as primers. The focus of this book is not on the theoretical basis of the methods. Other “classical” statistics courses serve this purpose. Instead, this book introduces essential concepts, methods, and tools for applied data science in Geography and Environmental Sciences with an emphasis on covering a wide breadth. It is written with a hands-on approach using the R programming language and should enable an intuitive understanding of concepts with only a minimal reliance on mathematical language. Worked examples are provided for typical steps of data science applications in Geography and Environmental Sciences. The aim of this book is to teach the diverse set of skills needed as a basis for data-intensive research in academia and outside. We also use this book as a reference and on-boarding resource for group members of Geocomputation and Earth Observation (GECO), at the Institute of Geography, University of Bern. Links Browse the source code Report an issue License Images and other materials used here were made available under non-restrictive licenses. Original sources are attributed. Content without attribution is our own and shared under the license below. If there are any errors or any content you find concerning with regard to licensing or other, please contact us or report an issue. Any feedback, positive or negative, is welcome. This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License. How to cite this book Benjamin Stocker, Koen Hufkens, Pepa Arán, &amp; Pascal Schneider. (2023). Applied Geodata Science (v1.0). Zenodo. About this course This book contains the lecture notes and exercises for the following courses, offered for Geography and for Climate Sciences students at the University of Bern, Switzerland: Applied Geodata Science I This is a course for Bachelors students in their second or third year of studies in Geography. Chapters 1 to Chapter 11 Applied Geodata Science II This is a course for Master students in Geography or Climate Sciences. Chapters are in development. Course goal The overall goal of this set of courses is that students and other readers learn to tell a story with (environmental and geo-) data. Learning Objectives The overall learning objectives are: Design and communicate your research project as a reproducible workflow. Find, access, process, and visualise large environmental and geographic data. Write legible code and manage collaborative code and data-centered projects. Manage analysis code for long-term reproducibility. Identify, quantify, and interpret patterns in large environmental and geographic data. Devise suitable data visualisations. Determine suitable model formulations and implement effective model training. Describe the challenges of model fitting with large data. Implement and make use of Open Science practices and resources to support data science projects in Geography and Environmental Sciences. Course contents This course covers all steps along the data science workflow (see Fig. 0.1) and introduces methods and tools to learn the most from data, to effectively communicate insights, and to make your workflow reproducible. By following this course, you will be well equipped for joining the Open Science movement. Figure 0.1: The data science workflow and keywords of contents covered in Applied Geodata Science I. Figure adapted from: Wickham and Grolemund R for Data Science This chapter starts by providing the context for this course: Why Applied Geodata Science? Why now? Chapters 1 and 2 serve as primers to get readers with a diverse background and varying data science experience up to speed with the basics for programming in R, which we rely on in later chapters. Chapter 3 introduces efficient handling and cleaning of large tabular data with the R tidyverse “programming dialect”. The focus is on non-geospatial data. Closely related to transforming data and its multiple axes of variation is data visualisation, covered in Chapter 4. Chapters 5, 7, and 6 introduce essential tools for the daily work with diverse data, for collaborative code development, and for an Open Science practice. With Chapters 8, Chapter 9, Chapter 10, and Chapter 11, we will get into modelling and identifying patterns in the data. Chapters 1-11 serve as lecture notes for Applied Geodata Science I and as learning material for students and scientists in any data-intensive research domain. These chapters are not explicitly dealing with geospatial data and modelling. Modelling with geospatial and temporal data is the subject of the course Applied Geodata Science II and will be introduced with a focus on typical applications and modelling tasks in Geography and Environmental Sciences. Respective materials are not currently contained in this book but will be added here later. All tutorials use the R programming language, and a full list of the packages used in this course are provided in Appendix B. "],["introduction.html", "Introduction What is Applied Geodata Science? The data science workflow Why now? A new modelling paradigm Reading and link collection", " Introduction The sheer volume of data that is becoming available today bears a huge potential for answering long-standing questions in all fields of environmental and geo-sciences. This gives rise to a new set of tools that can be used and a new set of challenges when applying them. What is Applied Geodata Science? Data science is interdisciplinary by nature. It sits at the intersection between domain expertise, Statistics and Mathematics knowledge, and coding skills. Data science generates new insights for applications in different fields by combining these three realms (Fig. 0.2). Combining only two of the three realms falls short of what data science is (Conway, 2013). Figure 0.2: The Venn diagram of data science. Adapted from Conway, 2013. Dealing with data requires coding (but not a degree in computer science). Coding skills are essential for file and data manipulation and for thinking algorithmically. Basic knowledge in Statistics and Mathematics are needed for extracting insights from data and for applying appropriate statistical methods. An overview of methods, a general familiarity, and an intuitive understanding of the basics are more important for most data science projects than having a PhD in Statistics. Statistics plus data yields machine learning, but not “data science”. In data science, questions and hypotheses are motivated by the scientific endeavor in different domains or by applications in the public or private sectors. To emphasize the distinctively applied and domain-oriented approach to data science of this course, we call it Applied Geodata Science. Of course, empirical research has always relied on data. The essential ingredient of a course in (Applied Geo-) data science is that it emphasizes the methodological aspects that are unique and critical for data-intensive research in Geography and Environmental Sciences, and for putting Open Science into practice. This course is also supposed to teach you how to stay out of the “danger zone” - where data is handled and models are fitted with a blind eye to fundamental assumptions and relations. The aim of data science projects is to yield credible (“trustworthy”) and robust results. The data science workflow The red thread of this course is the data science workflow (Fig. 0.3). Applied (geo-) data science projects typically start with research questions and hypotheses, and some data at hand, and (ideally) end with an answer to the research questions and the communication of results in textual, visual, and reproducible forms. What lies in between is not a linear process, but a cycle. One has to “understand” the data in order to identify appropriate analyses for answering the research questions. Before we’ve visualized the data, we don’t know how to transform it. And before we’ve modeled it, we don’t know the most appropriate visualization. In practice, we approach answers to our research questions gradually, through repeated cycles of exploratory data analysis - repeated cycles of transforming the data, visualizing it, and modelling relationships. More often than not, the exploratory data analysis generates insights about missing pieces in the data puzzle that we’re trying to solve. In such cases, the data collection and modelling task may have to be re-defined (dashed line in Fig. 0.3), and the exploratory data analysis cycle re-initiated. Figure 0.3: The data science workflow. Figure adapted from: Wickham and Grolemund R for Data Science As we work our way through repeated cycles of exploratory data analysis, we take decisions based on our data analysis, modelling, and visualizations. And we write code. The final conclusions we draw, the answers to research questions we find, and the results we communicate rest on the combination of all steps of our data processing, analysis, and visualization. Simply put, it rests on the reproducibility (and legibility) of our code (encapsulated by ‘Program’ in Fig. 0.3). Why now? Three general developments set the stage for this course. First, Geography and Environmental Sciences (as many other realms of today’s world) have entered a data-rich era (Chapters 5). Second, machine learning algorithms have revolutionized the way we can extract information from large volumes of data (this Chapter and Chapters 10 - 11). Third, Open Science principles (Chapter 6) - essential for inclusive research, boundless progress, and for diffusing science to society - are becoming a prerequisite for getting research funded and published. The skill set required to make use of the potentials of a data-rich world is diverse and is often not taught as part of the curriculum in the natural sciences (as of year 2023). This course fills this open space. A new modelling paradigm What is ‘modelling’? Models are an essential part of the scientific endeavor. They are used for describing the world, explaining observed phenomena, and for making predictions that can be tested with data. Models are thus a device for translating hypotheses of how the world operates into a form that can be confronted with how the world is observed. Models can be more or less explicit and more or less quantitative. Models can come in the form of vague mental notions that underpin our view of the world and our interpretation of observations. Towards the more specific end of this spectrum, models can be visualizations. For example a visualization of how elements in a system are connected. At the arguably most explicit and quantitative end of the spectrum are models that rely on mathematical descriptions of how elements of a system are connected and how processes operate. Examples of such models include General Circulation Models of the climate system or models used for Numerical Weather Prediction. Such models are often referred to as mechanistic models. A further distinction within mechanistic models can be made between dynamic models that describe a temporal evolution of a system (e.g., the dynamics of the atmosphere and the ocean in a General Circulation Model) and “static” models (e.g., a model for estimating the power generation of a solar photovoltaics station). In a dynamic model, we need to specify an initial state and the model (in many cases given additional inputs) predicts the evolution of the system from that. In a static model, the prediction can be described as a function of a set of inputs, without temporal dependencies between the inputs and the model prediction. Often, mechanistic and empirical models (or, here used as synonym, statistical models) are distinguished. Empirical models can be viewed as somewhere closer towards the less explicit end of the spectrum described above. In mechanistic models, the mathematical descriptions of relationships are informed by theory or by independently determined relationships (e.g., laboratory measurements of metabolic rates of an enzyme). In contrast, empirical models rely on no, or only a very limited amount of a priori knowledge that is built into the model formulation. However, it should be noted that mechanistic models often also rely on empirical or statistical descriptions for individual components (e.g., the parametrisation of convection in a climate model), and statistical models may, in some cases, also be viewed as a representation of mechanisms that reflects our theoretical understanding. For example, depending on whether a relationship between two variables is linear or saturating by nature, we would chose a different structure of an empirical model. An specific example is the light use efficiency model (Monteith, 1972) that linearly relates vegetation productivity to the amount of absorbed solar radiation. It simply has the form of a bivariate linear regression model. Vice-versa, traditional statistical models also rely on assumptions regarding the data generating process(es) and the resulting distribution of the data. Supervised machine learning models can be regarded as empirical models that are even more “assumption free” than traditional statistical models. In contrast to mechanistic models where rules and hypotheses are explicitly and mathematically encoded, and in contrast to statistical models where assumptions of the data distribution are made for specifying the model, machine learning approaches modelling from the flip side: from the data to the insight (Breiman, 2001). Rules are not encoded by a human, but discovered by the machine. Machine learning models learn from patterns in the data for making new predictions, rather than relying on theory and a priori knowledge of the system. In that sense, machine learning follows a new modelling paradigm. The learning aspect in machine learning refers to the automatic search process and the guidance of the model fitting by some feedback signal (loss function) that are employed in machine learning algorithms (see also Chapter 10). The aspect of “patterns in the data” is key here. Often, these patterns are fuzzy. Rule-based algorithms have a limited capacity for dealing with such problems. Symbolic artificial intelligence is based on rules and underlies, for example, a computer playing chess (Chollet &amp; Allaire, 2018). However, where rules cannot be encoded from the outset, symbolic artificial intelligence has reached its limits. A breakthrough in learning from fuzzy patterns in the data has been enabled by deep learning. Through multiple layers of abstraction of the data, deep learning models identify underlying, abstract, relationships and use them for prediction. Deep learning has been extremely successful in solving problems, e.g., in image classification, speech recognition, or language translation. However, the abstraction comes at the cost of interpretability. Deep learning models and machine learning models in general are used with an emphasis on prediction and have seen particularly wide adoption in fields where a false prediction has acceptable consequences (An inappropriate book recommendation based on your previous purchases is not grave.) (Knüsel et al., 2019). The model itself remains a black box and its utility for hypothesis testing is limited. This challenge has spurred the field of interpretable machine learning, where solutions are sought for uncovering the black box and probe the model for its trustworthiness. Chapters 8-11 lead into the world of machine learning and introduce the essential steps of the modelling workflow without delving into deep learning. Together with its preceeding chapters, this completes the toolbox required for making the first data scientific steps for applications in Geography and Environmental Sciences. This may be only just the beginning… Reading and link collection Foundations Leo Breiman: Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author).” Statist. Sci. 16 (3) 199 - 231, 2001. https://doi.org/10.1214/ss/1009213726 A paper describing the paradigm shift in statistical modelling - from traditional approaches to machine learning. Written in accessible language by the inventor of the Random Forest algorithm. Marquet, P. A., Allen, A. P., Brown, J. H., Dunne, J. A., Enquist, B. J., Gillooly, J. F., Gowaty, P. A., Green, J. L., Harte, J., Hubbell, S. P., O’Dwyer, J., Okie, J. G., Ostling, A., Ritchie, M., Storch, D., &amp; West, G. B.: On Theory in Ecology. BioScience, 64(8), 701–710, 2014. https://doi.org/10.1093/biosci/biu098 A key statement of this paper summarises its content: “Some have suggested that theories are irrelevant in the big data era—that correlations are sufficient to build a vigorous science […]. We disagree.” Data wrangling Hadley Wickham and Garrett Grolemund: “R for Data Science”, https://r4ds.had.co.nz/ A comprehensive resource for data analysis (and visualisation) using R tidyverse. Covers contents of Chapters 1, 2, and 3 - but in more depth. Max Kuhn and Kjell Johnson: “Feature Engineering and Selection: A Practical Approach for Predictive Models”, http://www.feat.engineering/ A practical guide to all steps of the data science workflow, with particularly valuable chapters on data wrangling and feature engineering. This free online book is not accompanied by code and is thus useful for any data science practitioner, irrespective of their preferred programming language. Data visualisation Claus O. Wilke: “Fundamentals of Data Visualization”, https://clauswilke.com/dataviz/ A comprehensive resource for data visualisation - not specific to any programming language, but specific about the grammar of graphics. Covers concepts of Chapter 4 - but in more depth. The go-to resource for the implementation of data visualisation using the {ggplot2} R library is Wickham and Grolemund (see above). Crameri, F., Shephard, G.E. &amp; Heron, P.J. The misuse of colour in science communication. Nature Communications 11, 5444 (2020). https://doi.org/10.1038/s41467-020-19160-7 A systematic approach to why some color scales are better than others. https://exts.ggplot2.tidyverse.org/gallery/ An overview of {ggplot2} extensions for special visualisations and solutions for publication-ready graphs. Machine learning Bradley Boehmke and Brandon Greenwell: “Hands-On Machine Learning with R”, https://bradleyboehmke.github.io/HOML/ A great entry point for machine learning in R. It demonstrates concepts and a range of algorithms of varying complexity - from linear regression to Random Forest - with examples in R. This book served as an inspiration and starting point for the (in some ways reduced) contents of this course, covered in Chapters 8-11. Pichler, M., &amp; Hartig, F. (2023). Machine learning and deep learning—A review for ecologists. Methods in Ecology and Evolution, 00, 1– 23. https://doi.org/10.1111/2041-210X.14061 A review paper that provides an instructive overview of machine learning and deep learning for a general readership (not just scientists in Ecology and Evolution). The paper explains also the fundamental assumptions of different methodological approaches and provides an intuitive understanding of the sometimes surprising power of machine learning algorithms. Chollet &amp; Allaire “Deep learning with R”, Manning Publications, Accessed February 17, 2023. https://www.manning.com/books/deep-learning-with-r. This is the next step after you’ve studied Applied Geodata Science I. It introduces machine learning with deep neural networks using the {keras} machine learning library (with its wrapper in R). "],["gettingstarted.html", "Chapter 1 Getting started 1.1 Learning objectives 1.2 Tutorial 1.3 Exercises", " Chapter 1 Getting started Chapter lead author: Pepa Arán 1.1 Learning objectives This chapter provides a start at the very beginning of your journey in Applied Geodata Science with two main aims. First, it introduces the very basics for readers with no experience using R. You will be able to: Work with R and RStudio. Know basic R objects and classes. Understand how R interacts with files in your computer. Second, the tutorial of this chapter prepares you for your work on the remainder of this course by setting up all the necessary infrastructure. Also experienced R users should follow respective tutorial sections: Sec. 1.2.6, 1.2.7, and 1.2.8). You will: Organize your workspace for efficient data science projects. Have installed git and all R packages used throughout the book. Completing these points are essential before we go deeper into project management topics in Chapters 6 and 7. Chapter 2 will focus on how to code. 1.2 Tutorial 1.2.1 Working with R and RStudio R is a free, open-source programming language and software environment for statistical computing and graphics. It is widely used, not only among statisticians and data miners for developing statistical software, but also by scientist in various domains for data analysis, visualisation, and modelling. RStudio is an integrated development environment (IDE) that provides a user-friendly “center stage” for your work in R (and Python, see here). 1.2.1.1 Installing R and RStudio To use R and RStudio, you will first need to download and install them on your computer. To install R, go to the CRAN website and download the latest version of R for your operating system. Once the download is complete, follow the on-screen installation instructions for your operating system to install R. To install RStudio, go to the RStudio website and download the latest version of RStudio for your operating system. Once the download is complete, follow the installation instructions for your operating system to install RStudio. 1.2.1.2 The RStudio interface RStudio provides a user-friendly interface for writing, running, and debugging R code. When you open RStudio, you will see the following: Figure 1.1: RStudio interface. The interface is divided into four main panels: The source editor is where you can write, edit, and save your R code. The console is where you can enter R commands and see the output. The environment panel shows you the objects (variables, data frames, etc.) that are currently in your R session, as well as their values. The files, plots, help, etc. panel shows you the files, plots, and other items that are currently in your R workspace, as well as help and documentation for R functions and packages. We will cover this in more detail later in this course. 1.2.1.3 Running R code Once you have both programs installed, you can open RStudio and begin a new R session. To run R code using R Studio, follow these steps: In the source editor panel, type your R code. To run the code, you can either press the Run button or use the keyboard shortcut Ctrl + Enter (Windows) or Command + Enter (Mac). The code will be executed in the console panel, and any output will be displayed there. Alternatively, you can directly type single-statement R commands in the console and run them by pressing Enter. For example, let’s say you want to calculate the sum of the numbers 1, 2, and 3. You can write the following code in the console or in the source editor: # Calculate the sum of 1, 2, and 3 1 + 2 + 3 ## [1] 6 If you’ve entered it in the console, press Enter. If you’ve entered it in the source editor, you can press the Run button or use the keyboard shortcut to run the code. The output will be displayed in the console: &gt; 1 + 2 + 3 [1] 6 1.2.1.4 Base R operations The R {base} package contains the basic functions which let R function as a programming language: arithmetic, input/output, basic programming support, etc. Its contents are always available when you start an R session. Here we introduce the main binary operators, which work on vectors, matrices and scalars. Arithmetic operators: + addition - subtraction * multiplication / division ^ or ** exponentiation %% modulo operator (returns remainder of a division) Logical operators: &gt; greater than &gt;= greater than or equal to == exactly equal to &lt; less than &lt;= less than or equal to != not equal 1.2.2 R objects In addition to running single statements in the R console, the output of a statement can be saved as a new object. There are many kinds of R objects, some of which are covered here and in future chapters. 1.2.2.1 Types of data First, we will introduce the different types of data that one can encounter. We can classify variables according to what values they take. Numerical: These variables can be measured quantitatively and their value is a number. Continuous: We say that a variable is continuous when it can take an infinite number of real values within an interval. One could consider unbounded variables (height above sea level) or restricted variables, like positive variables (weight of a person) or an interval (a proportion between 0 and 1). Discrete: When the variable can only take a finite number of values in an interval, we say it is discrete. A common example is count data, like the population of a city. Categorical: The values are characteristics that cannot be quantified. Binary: These variables have two possible values: TRUE or FALSE (a variable indicating whether the person has siblings or not). Nominal: They describe a name, label, or category without a natural order (for example, the name of a person). Ordinal: Like their name indicates, ordinal variables are categorical and follow a natural order. For example, “terrible”, “bad”, “neutral”, “good”, “great”. A numerical variable can sometimes be discretized and put into categories, like dividing a person’s age into age groups (bins) “toddler”, “child”, “teenager”, “adult”. Next, we will see how these different types of variables can be treated in R. 1.2.2.2 Variables and classes In R, a variable is a named location in memory that stores a value. To create a variable, you simply assign a value to a name using the &lt;- operator (or the = operator, which has an equivalent role when assigning values to a variable, but &lt;- is preferred). For example: my_variable &lt;- 5 This code creates a variable called my_variable and assigns the value 5 to it. You can access the value of a variable or any other object by simply referring to its name, like this: my_variable ## [1] 5 When you run this code, the value of my_variable will be printed to the console. Running print(my_variable) is an alternative syntax, using the print() function. In R, every object and value has a class that determines how it is stored and how it behaves. For example, the 5 in our example above is a number, so its class is numeric. To find out the class of a value or a variable, you can use the class() function, like this: class(5) ## [1] &quot;numeric&quot; class(my_variable) ## [1] &quot;numeric&quot; The most basic classes are: numeric (num) - any real number, e.g. 2.375 integer (int) - integer numbers, e.g. 2 character (chr) - any string, e.g., \"fluxes\" logical (logi) - binary, i.e., either TRUE or FALSE. factor (Factor) - categorical data, the variable can only be one of a defined number of options, e.g., one of C3, C4, or CAM (the three pathways of photosynthesis). Factors may also be given an order. function - a set of statements organized to perform a specific task, for example mean() By default, any number is coerced as \"numeric\". So if you want an integer value to have class \"integer\", you need to specify it like this: my_variable &lt;- as.integer(5) class(my_variable) ## [1] &quot;integer&quot; Sometimes, you need to convert the class of an object, for example turning an \"integer\" number into a \"character\". You can do so as follows: my_variable &lt;- as.character(my_variable) my_variable ## [1] &quot;5&quot; class(my_variable) ## [1] &quot;character&quot; Note that now, the values are in quotes \"5\". This way, R interprets it as a text and you will not be able to do any numeric calculations with it anymore. 1.2.2.3 Vectors A vector in R is a sequence of data elements of the same class. Vectors can be created with the c() function, which stands for concatenate, i.e., to link together in a series or chain. For example, the following code creates a numeric vector: x &lt;- c(1, 2, 3, 4, 5) To access the elements of a vector, you can use the square bracket notation. For example, the following code retrieves the second element of the vector x: x[2] ## [1] 2 You can also use the square bracket notation to extract a sub-vector from a larger vector. For example, you can extract the second to fourth elements of the vector x: x[2:4] ## [1] 2 3 4 Another useful property of vectors in R is that they can be easily combined using arithmetic operators. For example, adding the elements of two vectors x and y element-wise: x &lt;- c(1, 2, 3) y &lt;- c(4, 5, 6) x + y ## [1] 5 7 9 R also supports vectors of other classes, for example character vectors. Since all elements must be of the same class, the most general class will be adopted. The following code concatenates the vectors x and y, followed by new character elements: z &lt;- c(x, y, &quot;seven&quot;, &quot;eight&quot;) z ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;seven&quot; &quot;eight&quot; class(z) ## [1] &quot;character&quot; Operations on vectors are performed element-wise. For example, if we ask what numbers in x are greater than 2, we obtain a vector of logical values (and class \"logical\"): x &gt; 2 ## [1] FALSE FALSE TRUE Vectors that contain sequences of numbers are often needed in programming. They are easily created in R, e.g., by: 1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 More flexibility is offered by the seq() function: seq(from = 0, to = 10, by = 2) ## [1] 0 2 4 6 8 10 Often, we need to evaluate multiple elements in a vector. We’ve learned that the operator &gt; tests whether the value left to it is greater than the value on its right and returns a logical. We can subset a vector based on a vector of equal length that contains logicals. x &gt; 1 ## [1] FALSE TRUE TRUE x ## [1] 1 2 3 x[x &gt; 1] ## [1] 2 3 We can also determine the indices (positions of elements in the vector) that evaluate to TRUE, or that have the lowest value: which(x &gt; 1) ## [1] 2 3 which.min(x) ## [1] 1 Elements can be dropped from vectors by referring to indices: y[-2] ## [1] 4 6 1.2.2.4 Lists Lists are R objects, of class \"list\". They are a bit like vectors, but more flexible. They allow us to store different types of data, even if they are of different lengths or of different classes. They are created with the function list() and can be named or not. Here is an example where each element of the list is named. mylist &lt;- list( temperatures = c(2.234, 1.987, 4.345), my_favourite_function = mean, my_favourite_course = &quot;Applied Geodata Science&quot; ) Similar to vectors, we can extract elements from lists, either by index [[1]] or by the name using [[\"temperatures\"]] or $temperatures. Note the double [[]] here, indicating an element of a list as opposed to [] indicating an element of a vector. To get the entire vector of temperatures, do either of the three: mylist[[1]] ## [1] 2.234 1.987 4.345 mylist[[&quot;temperatures&quot;]] ## [1] 2.234 1.987 4.345 mylist$temperatures ## [1] 2.234 1.987 4.345 Note below how, if we index the list like we would index a vector, a list with just one element would be returned, rather than the element itself. [ is used to subset a list (and a list is returned). In contrast, [[ or $ extract a single element from a list. A thorough explanation of these differences is given here and here. mylist[1] # returns a subset of the list as a new list ## $temperatures ## [1] 2.234 1.987 4.345 mylist[[1]] # extracts the first element of the list (a vector) ## [1] 2.234 1.987 4.345 To get the first temperature value, which is an element of the vector (at the same time an element of the list), we can run: mylist[[&quot;temperatures&quot;]][1] ## [1] 2.234 You can also append elements to the list (either way is possible): mylist[[&quot;my_second_favourite_function&quot;]] &lt;- median mylist$my_second_favourite_function &lt;- median This was a very condensed introduction to vectors and lists. A more complete introduction is given here. 1.2.2.5 Data frames A data frame, an object of class \"data.frame\", is essentially a table, consisting of named columns and rows. A data frame can be created as follows: df &lt;- data.frame(name = c(&quot;Maria&quot;, &quot;Peter&quot;, &quot;Alex&quot;, &quot;Charlie&quot;), age = c(13, 56, 30, 42), is_married = c(FALSE, TRUE, TRUE, FALSE)) df ## name age is_married ## 1 Maria 13 FALSE ## 2 Peter 56 TRUE ## 3 Alex 30 TRUE ## 4 Charlie 42 FALSE A data frame can also be understood as a list of vectors of equal length, whereby each vector vector makes up a column and each of these vectors (columns) contains values of the same type. This notion makes it also evident that the elements of a data frame can be accessed the same way like we access elements of lists. To get the vector corresponding to the column named age, we can do: df$age ## [1] 13 56 30 42 Data frames can be also be treated as a matrix. Note that the first index refers to rows and the second to columns. For example: df[, 1] # first column (returns a vector) ## [1] &quot;Maria&quot; &quot;Peter&quot; &quot;Alex&quot; &quot;Charlie&quot; df[2, ] # second row (returns a data frame) ## name age is_married ## 2 Peter 56 TRUE df[2,2] # age of Peter (returns a scalar) ## [1] 56 The method of selecting parts of a data frame by index is quite flexible. For example, we may require the information in the third column for the first three rows. Putting a colon between two numbers, e.g. [1:3,], indicates we want to select the rows numbers starting at the first and ending with the second number. So here [1:3,] will give us rows one, two and three. This can be combined with subsetting for the other dimension as well: df[1:3, 3] ## [1] FALSE TRUE TRUE To reduce the data frame to fewer columns/rows that are not contiguous, the function c() is used. This outputs the data frame reduced to the selected row or column numbers inside c(). Another method is to select the columns by column names, i.e. giving as input a string vector with the name of each column we want to select. For example, the following commands give the same output: df[, c(1,3)] # select by column index ## name is_married ## 1 Maria FALSE ## 2 Peter TRUE ## 3 Alex TRUE ## 4 Charlie FALSE df[, c(&quot;name&quot;, &quot;is_married&quot;)] # select by column name ## name is_married ## 1 Maria FALSE ## 2 Peter TRUE ## 3 Alex TRUE ## 4 Charlie FALSE There are several base R functions to help you understand the structure of a data frame. Here is a non-exhaustive list of of them: Size dim() - Returns the dimensions of an object (here: number of rows and columns). nrow() - Returns the number of rows of an object. ncol() - Returns the number of columns of an object. Content head() - Returns the first 6 rows. tail() - Returns the last 6 rows. View() - Opens a window in the source panel in RStudio where you can look at the entire data set in the form of a table. Names names() - Returns the column names (for data.frame objects it is synonymous to colnames()). rownames() - Returns the row names. For example, the data frame df has 4 rows and 3 columns: dim(df) ## [1] 4 3 There are many more things you can do with data frames. Since they are central to analyzing data with R, we have dedicated all of Chapter 3 to teach you how to work with data frames in a tidy way with the {tidyverse} collection of packages. 1.2.2.6 Functions R functions can be applied to an object (or several objects) and return another object. For example, the mean() function can take a numeric vector as input and output the mean of its elements. mean(df$age) ## [1] 35.25 Functions are also R objects and have class \"function\". Writing your own functions is an essential part of good programming and will be introduced in Chapter 2. 1.2.2.7 Missing values R has two representations for missing values: NA and NULL. Similar objects also exist in other programming languages. NA is an identifier to mark missing data and stands for not available. You will encounter this when reading data into a data frame, and some of its cells show NA because that value is missing. Also, if you ask for the fourth element of a vector of length 3, R returns NA. x[4] ## [1] NA In general, operations on vectors that contain at least one NA value return NA. For example: mean(c(1, 2, NA)) ## [1] NA To remove all missing values in the function evaluation, the common argument to set in the respective function call is na.rm. By default, it is usually set to FALSE, but we can do: mean(c(1, 2, NA), na.rm = TRUE) ## [1] 1.5 Furthermore, NA counts as an element in vectors. A variable assigned just NA would have length 1 (of class \"logical\") and the vector above has length 3, as can be determined using the length() function, and has class \"numeric\". Whether a value is missing can be tested by: is.na(c(1, 2, NA)) ## [1] FALSE FALSE TRUE By contrast, NULL is the R null object or empty space. You can also assign NULL to a variable, which will then have length zero because it is empty. Functions may return NULL when no output was defined, or if an error occurred. 1.2.2.8 Read and save objects The function save() allows to save multiple R objects of any form as a single .RData file. This is how the environment of your R session is saved. This is how we would save several R objects: save(df, df_small, file = &quot;./data/data_frames.RData&quot;) To tell the function where the data is located, pass the data’s path as an argument. You can either use an absolute path, starting from C:/ on a Windows computer or ~/ on a Mac or Linux. Or, alternatively, you can provide a relative path, where ./ points to the present working directory (like above) and ../ is one level up, or ../../ is two levels up, etc. We recommend that you work with R projects and use relative paths, because the working directory is set to the root directory of the R project and relative paths will also work on another person’s computer, helping with reproducibility. In the example above, we provided a relative path, starting at the current working directory (./). The file data_frames.RData sits in a sub-directory of the current working directory, called data/. What is the current working directory? Unfortunately, the answer is “it depends…”. You’ll learn more about this in Section 1.2.7.1. .RData files are read into your environment using the load() function. This function loads the objects with the name that they were saved with. load(&quot;./data/data_frames.RData&quot;) Alternatively, the function saveRDS() allows you save single R objects that can then be read into R with a specific (potentially new) variable name. This is more transparent than using save() and load(), and thus is preferred, as it gives the user more control over variable names. saveRDS(df_small, file = &quot;./data/df_small.rds&quot;) df_small_2 &lt;- readRDS(&quot;./data/df_small.rds&quot;) save() and saveRDS() create binary files that are fast to write and read, but only intelligible to R (and not to the human eye or another program). Such files are commonly identified by the suffix .rds. It is recommended to name the .rds files according to the single object they contain. When publishing and sharing data, follow Open Science principles (Chapter 6) and avoid file formats that are not readable across different platforms and programming languages. We will learn more about human-readable tabular data files like CSV in Chapter 3 and other open source binary file formats in Chapter 5. 1.2.3 R environment The set of objects (variables, data frames, etc.) defined during an R session are referred to as the environment. You can view the objects in RStudio in the environment panel in R Studio, grouped as Data, Values and Functions. After closing an existing R session (e.g., after quitting RStudio), the environment defined by the used during that session will not be saved automatically and will not be available in your next R session. You should avoid saving your entire environment. Rather, save individual objects into files and read them in explicitly in the next R session, as described above. This gives you control, transparency, and ensures better reproducibility. 1.2.4 Libraries Packages, also called libraries, are collections of R functions, data, and complied code in a well-defined format. R comes with a standard set of packages (including {base} R, {utils}, {stats}…) and other packages targeted for specific applications are available for download and installation. Once installed, you need to load them each time you start a new R session to use them. For example, the {tidyverse} package is used for data wrangling and will be covered in this course. This is a special package which loads many other packages in the background (like {readr}, {ggplot2}, etc.). You can install a new package as follows: install.packages(&quot;tidyverse&quot;) Then, you can load it with the following code. Note that now the name of the package is not in quotation marks. library(tidyverse) You can now use the functions and features provided by the {tidyverse} package in your R scripts. Imagine that you have loaded two packages named {lib1} and {lib2} with the library() function. Now, all the functions in those two packages are available for use in R. But if both of them have a function called fun() and you run the command fun(my_vector), how does R know from which package that function comes? Normally, the package loaded last will “mask” the previous packages, such that their functions (if names are repeated) are used by R. If you want to specify from which library to take a function, you can use the :: notation. So in our imaginary example we would use lib1::fun(). Furthermore, using the :: notation allows to access a function without the package not being loaded. In general, it’s a good practice to specify the package from which a function comes with ::. At any time, you can see a list of your installed packages on the source panel with the following command: library() And a list of the packages currently loaded: search() ## [1] &quot;.GlobalEnv&quot; &quot;package:stats&quot; &quot;package:graphics&quot; ## [4] &quot;package:grDevices&quot; &quot;package:datasets&quot; &quot;renv:shims&quot; ## [7] &quot;package:utils&quot; &quot;package:methods&quot; &quot;Autoloads&quot; ## [10] &quot;package:base&quot; This information can also be found on the Packages panel in RStudio. The loaded packages are shown with a tick mark. 1.2.5 R scripts Usually, multiple statements are needed to get, e.g., from reading data into R to final numbers and figures that make up a further analysis. Together, these multiple statements constitute a workflow. It is essential that all workflows that underlie results of publications are reproducible, that is, that another person can replicate your results using your code and certain data. To make a workflow reproducible, the sequence of statements that you needed to carry out your analysis and produce outputs can be saved as an R script. A script is a text file named with the suffix .R to indicate that it is executable by R. It contains a sequence of R commands, which you can be executed, line by line, starting from the top. To create a new script in RStudio, go to the File menu and select New File &gt; R Script. This will open a new script file in the source editor. You can then type your R code in the script file and save it to your computer. To run a script, you can either use the Source button in the source editor or use the keyboard shortcut Ctrl + Shift + Enter (Windows) or Command + Shift + Enter (Mac). This will run all of the commands in the script file, in the order they are written, in the console. Alternatively, you can type into the console: &gt; source(&quot;my_r_script.R&quot;) Note that, to be able to run the code above, the file my_r_script.R must be in your current working directory. You must always specify the path to the file, also when sourcing code. 1.2.6 R Markdown R Markdown files are an enhanced version of scripts. They combine formatted text and executable code chunks. They can either be compiled (knitted) into an HTML or PDF output, where code chunks are executed upon compilation and visualization outputs are directly placed into the output, or they can be run like a script entirely or each code chunk separately. Rmarkdown is ideal for reporting, i.e., writing your final document presenting your analysis results. When opened in RStudio, RMarkdown files appear in the Editor like this: Figure 1.2: R Markdown document opened in the source panel. As shown in Fig. 1.2, an RMarkdown file consists of a header that specifies the document properties, such as how it should be rendered (as an html page, a docx file or a pdf). --- title: &quot;Simple global map&quot; author: Alex Humboldt output: html_document --- Below the header follow the contents as either text or code chunks. Text is formatted using the Markdown syntax. Nice brief guides are provided here or here. For example, a top-level section title is specified by # and a title of a section one level lower by ##. Code chunks that contain executable R code are opened by a line ```{r}. The document can be rendered by calling rmarkdown::render() on the command line or hitting the Knit button in the RStudio IDE. Depending on your settings a html file, pdf or docx file will be generated in your current directory (and or displayed in the IDE viewer). The RMarkdown source file shown in Fig. 1.2 is rendered to: Figure 1.3: Rendered HTML output. Note that the code chunk produced a figure as an output which was placed directly in the rendered html. This demonstrates the usefulness of RMarkdown as notebooks to document entire workflows and make their outputs reproducible. To create a new RMarkdown file, select from the drop-down menu in the top-left corner of RStudio as shown below: Figure 1.4: Create a new RMarkdown file 1.2.7 Workspace management Workspace management is crucial for efficient data science projects. Two aspects are essential for workspace management: The structure of directories and the types of files they contain; and code management and version control. To address these two points, we will briefly introduce you to R projects and git. 1.2.7.1 R projects Think of all files that are linked to a certain data science workflow as belonging together into one project. A project is therefore a directory that contains the code and the files it produces. Different types of files go in different sub-directories with descriptive names. To keep an order and make your code useful also for others, always keep files of the same type in sub-directories that are always named the same way across projects. When creating a new project, you will find the same overall structure. A common structure is: Keep .R files that implement a user-defined function (see Chapter 2) in a sub-directory ./R/, and .R files that implement a workflow in a sub-directory ./analysis/. Workflow implementations with a view to communication should be implemented in RMarkdown files. RMarkdown files are commonly kept in a sub-directory ./vignettes/ or in the main (top-level) project directory. Keep figures produced by your code in a sub-directory ./figures/ Keep data files produced by your code in a sub-directory ./data/ Data obtained from external sources (e.g., published datasets, data shared by collaborators) and potentially used across multiple projects should be kept outside a project directory. Set up your own directory of data obtained from external sources, for example in your home as ~/data/ To read and write from/to files you should use relative paths (relative to the project’s top-level directory), like any of the two equivalent following options: &gt; source(&quot;./R/my_r_script.R&quot;) &gt; source(&quot;R/my_r_script.R&quot;) … and not: &gt; source(&quot;~/my_project/my_r_script.R&quot;) Note that when a path is specified, e.g,. in a script or RMarkdown file that sits itself in a sub-directory, then the effective current working directory is that sub-directory, and not the top-level project directory. Hint: Use here::here() to obtain the path to the top-level. Paths can then be specified relative to that, irrespective of where the code lives that uses that path. For example, a file in sub-directory data/ can be read by read.csv(paste0(here::here(), \"/data/a_file.rds\")). R sessions in RStudio that are connected to such project directories are R projects. To create a new R project, go to the File menu and select New Project…. This will open the New Project dialog, where you can choose where to save your project and what type of project to create. The current project that you are working on is shown on the upper right corner of the RStudio interface. Here you can also switch between existing projects or create a new one. Figure 1.5: R project menu. When starting a new project, named project_name, a file project_name.Rproj is created in the top-level project directory. It stores information about your the session (settings, open files, etc.), automatically enables useful features in RStudio for easy package, website, or book building, lets you manage code version control (see next sub-section), and optionally (not recommended) stores the environment of your current session. It is not recommended to save and restore entire environments. Instead read and write objects from and into files to guarantee control, transparency, and reproducibility. When you want to continue working on an existing R project, you can start a new session by clicking on your project_name.Rproj file. This restores In Chapters 6 and 7, we will learn more about efficient code management. 1.2.7.2 git git is a version control system that keeps track of all changes on your code, who made it, and when. It allows you to go back to previous changes, and lets you control when to iterate from one state of the code to the next. The basic unit of git is a git repository - all the code contained in a project directory. That’s why R projects work well with git. git can also keep track of changes in data, so long as the data is small and in text format (see 5). git also lets you sync your local copy of a git repository with git service in the cloud, for example GitHub. This allows you to manage the same repository across computers and collaborate with multiple partners on the same repository. We will learn more about git in Chapter 7. For now, follow instructions in Section 1.2.8 to set yourself up with installing git and other required stuff on your computer and with a GitHub account. You will use this later in this course (and maybe throughout your career). 1.2.8 Setup 1.2.8.1 Packages You either have installed R and RStudio at the start of this chapter or you had them in your computer already. If you belong to the second group, make sure that you update both R and RStudio to get the latest versions. Working with the latest version of R, RStudio and packages helps to avoid errors due to dependency conflicts. Now, let’s install all the required packages for this course and load them. use_pkgs &lt;- c(&quot;dplyr&quot;, &quot;tidyr&quot;, &quot;readr&quot;, &quot;lubridate&quot;, &quot;stringr&quot;, &quot;purrr&quot;, &quot;ggplot2&quot;, &quot;tidyverse&quot;, &quot;visdat&quot;, &quot;terra&quot;, &quot;hexbin&quot;, &quot;jsonlite&quot;, &quot;MODISTools&quot;, &quot;forcats&quot;, &quot;yardstick&quot;, &quot;recipes&quot;, &quot;caret&quot;, &quot;broom&quot;, &quot;skimr&quot;, &quot;cowplot&quot;, &quot;scico&quot;, &quot;hwsdr&quot;, &quot;usethis&quot;, &quot;renv&quot;, &quot;rsample&quot;, &quot;modelr&quot;, &quot;rmarkdown&quot;, &quot;rpart&quot;, &quot;rpart.plot&quot;, &quot;ranger&quot;, &quot;sessioninfo&quot;, &quot;ncdf4&quot;) new_pkgs &lt;- use_pkgs[!(use_pkgs %in% installed.packages()[, &quot;Package&quot;])] if (length(new_pkgs) &gt; 0) install.packages(new_pkgs) invisible(lapply(use_pkgs, require, character.only = TRUE)) If the installation failed, there should be an error message “Installation of package … had non-zero exit status”. If so, check with teaching assistance. 1.2.8.2 Other libraries and applications Some R packages depend on one another (for example, all depend on {base} R), but they can also depend on software external to the R ecosystem. For Chapter @ref{datavariety}, we will need the {ncdf4} package to work with netCDF files in R. NetCDF (network Common Data Form) is a file format for storing multidimensional scientific data (variables) such as temperature, humidity, pressure, wind speed, and direction. The package {ncdf4} depends on software that is not available as an R package, the netCDF command-line tools, which you need to install additionally on your computer. To install the netCDF command-line tools, follow these instructions: For MacOS users, via homebrew: You first need to install the package manager Homebrew. Copy-paste the following code into the terminal: /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\". Type into terminal brew --version. If version number is shown, you are good to go. If nothing pops up, restart Mac and type in again. If still nothing shows, check with teaching assistance. Type in brew install netcdf following this. Restart RStudio if it was open during the steps above. Enter install.packages(\"ncdf4\") and see if it installs it correctly. If installation failed, there should be a message “Installation of package … had non-zero exit status”. If so, check with teaching assistance. Note: The programm Terminal allows you to interact with your Mac through the command line and is installed automatically. You can open it through the Finder if you go to Applications &gt; Utilities &gt; Terminal. For MacOS users, via MacPorts: Install xcode via the Terminal by typing in xcode-select --install (if not installed already). Then, install the package manager Homebrew via the terminal code /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\". Then, install netcdf via the terminal code brew install netcdf. It is possible that you still cannot install {terra} because you are missing gdal. If so, run brew install gdal in the terminal. For Linux users: Since the package is pre-installed in Linux, just type sudo apt install gdal-bin libgdal-dev in the terminal. For Windows users: Download the “netCDF-4 64-bit (Windows)” .exe file on the netCDF website. Once downloaded, click on the .exe file and follow the installation dialog. You can check if the installation was successful by running the following code in the RStudio console: terra::rast(&quot;https://raw.githubusercontent.com/geco-bern/agds/main/data/demo_data.nc&quot;) ## class : SpatRaster ## dimensions : 41, 71, 1 (nrow, ncol, nlyr) ## resolution : 0.1, 0.1 (x, y) ## extent : 4.95, 12.05, 43.95, 48.05 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 ## source : demo_data.nc ## varname : t2m (2 metre temperature) ## name : t2m ## unit : K ## time : 2022-01-01 12:00:00 UTC Or, to be independent of R and the {terra} package, you can test it by running the following code in your bash terminal (on Mac or Linux): # download file demo_data.nc curl --output demo_data.nc https://raw.githubusercontent.com/geco-bern/agds/main/data/demo_data.nc # show header of file ncdump -h demo_data.nc 1.2.8.3 git In this section, you will install git, create a GitHub account and connect it to RStudio. This should set up your laptop so that you can do version control of your code (with an easy to use interface) and upload changes to an online copy of your R project (on GitHub) directly from RStudio. Follow these instructions: If you don’t have one already, create a GitHub account on https://github.com and sign in. We encourage you to use a personal email (not your student email) so you can keep using this account for years to come, thus compiling your “code knowledge base”. Next, follow the Git download and installation instructions for your OS outlined here. Once Git is installed, reboot RStudio. Set up RStudio for version control by going to Tools &gt; Global Options &gt; Git/SVN and tick “Enable version control interface…”. The path to your Git installation should be correct automatically. If it says “(Not Found)”, try the following: For Windows, it’s probably C:\\\\Program Files\\\\Git\\\\bin\\\\git.exe. For MacOS and Linux, you can type which git in the Terminal and it will show the installation location. If you get an error after trying this, ask the teaching assistance. Figure 1.6: Set up Git for RStudio. Connect your GitHub account with Git locally, using SSH keys (which are cryptographic authentication credentials). You can do this from RStudio following the steps: Go to Tools &gt; Global Options &gt; Git/SVN and select Create SSH Key…. Create an SSH key with the default options (you can ignore the passphrase and click “Create”). This will create a new SSH in your default ~/.ssh folder. Figure 1.7: Create an SSH key from RStudio. Next, click on View public key (above your new SSH key) and copy the public key (all the text). Figure 1.8: Copy the public key for the SSH connection Go back to your browser and open your GitHub account. At the top right, open your profile’s menu and choose Settings &gt; SSH and GPG Keys. Then select Add new SSH key. Give your SSH connection a name, for example “My laptop” and paste the public key that you copied from RStudio. If you have problems, check out these instructions to add a new SSH key to your GitHub account. Finally, you can test your SSH connection following these instructions. Note that, at the top of these websites, there’s an option to choose your OS because the steps vary depending on the type of computer your work with. 1.2.8.4 Preparation for your report Your performance assessment for Applied Geodata Science will be in the form of a report. You will approach the writing of this report as if you were working on a clean, structured and reproducible data science project. This section is meant to guide you through the preparation of your report workspace, such that it’s in the form that you will submit it. Following the next steps, you will set up a git repository containing an R project and an R Markdown file. Log into GitHub in your browser and create a new repository. Name it agds_report_yourname, give it a short description and set it to be public, so we can read it for the evaluation. Include a README file (it’s a sort of menu which you’ll edit later), a .gitignore template for R (select in the drop-down menu) and a Creative Commons license (for code sharing rights, etc). Figure 1.9: Create a Git repository for your report. Now that you have a GitHub repository, open it and you should see a green button saying Code. Go to SSH and copy that url. You will use it to clone the repository. Figure 1.10: Copy the SSH url to clone your reporitory. Open RStudio. Create a new R project (opening the New Project dialog) from a GitHub repository. Paste the repository’s url, give it a name (by default the same as the repository) and select in which folder you want to keep it locally (for example the base directory ~). This will clone all the files from GitHub onto the project directory (i.e. folder) in your computer. Figure 1.11: Clone your repository and create an R project. With this, you’re set up. If your R project agds_report_yourname is open, you should see a Git panel in RStudio (next to Environment) and also be able to use Git in the Terminal panel in RStudio. As mentioned above, we will start working with these tools in a few weeks and they will be a crucial part of the course. You’ll see a detailed explanation of the git workflows in Chapter 7. For now, add your solutions to each report exercise into a separate RMarkdown file, placed in a subdirectory named ./vignettes/. The first Report Exercise will be the one for Chapter 3 where you will be asked to implement your solutions in a file called re_tidy.Rmd. At that stage, your repository should look like this: Figure 1.12: At first, the R Markdown for your report will look something like this. In total you should have two R projects related to this course: a local project containing solutions to general exercises (for yourself, created in the exercises below) and the report project + git repository (that you just created and will share with us at the end of the course). Your report project should not include the solutions to general exercises. 1.3 Exercises Dimensions of a circle Given the radius of a circle r, write a few lines of code that calculates its area and its circumference. Run your code with different values assigned to r. Print the solution as text. Hint: Enter pi in your console. Hint: Entering print(\"agds\") in your console returns \"agds\". Combining (concatenating) multiple strings into a single one can be done using paste(). Sequence of numbers Generate a sequence of numbers from 0 and \\(\\pi\\) as a vector with length 5. Hint: Consult the manual of the function seq() by entering ?seq in your terminal. Gauss sum Rumors have it that young Carl Friedrich Gauss was asked in primary school to calculate the sum of all natural numbers between 1 and 100. He did it in his head in no time. We’re very likely not as intelligent as young Gauss. But we have R. What’s the solution? Gauss calculated the sum with a trick. The sum of 100 and 1 is 101. The sum of 99 and 2 is 101. You do this 50 times, and you get \\(50 \\times 101\\). Demonstrate Gauss’ trick with vectors in R. Magic trick algorithm Define a variable named x that contains an integer value and perform the following operations in sequence: Redefine x by adding 1. Double the resulting number, over-writing x. Add 4 to x and save the result as x. Redefine x as half of the previous value of x. Subtract the originally chosen arbitrary number from x. Print x. Restart the algorithm defined above by choosing a new arbitrary natural number. Vectors Print the object datasets::rivers and consult the manual of this object. What is the class of the object? What is the length of the object? Calculate the mean, median, minimum, maximum, and the 33%-quantile across all values. Hint: If you don’t know how to solve a problem, help yourself on the internet. Data frames Print the object datasets::quakes and consult the manual of this object. Determine the dimensions of the data frame using the respective function in R. Extract the vector of values in the data frame that contain information about the Richter Magnitude. Determine the value largest value in the vector of event magnitudes. Determine the geographic position of the epicenter of the largest event. Workspace Create a new R project and create sub-directories in a meaningful way (as described in this Chapter). Create an RMarkdown file in your new project which implements your solutions to above exercises. Give the file a title, implement some structure in the document, and write some text explaining what your code does. "],["programmingprimers.html", "Chapter 2 Programming primers 2.1 Learning objectives 2.2 Tutorial 2.3 Exercises", " Chapter 2 Programming primers Chapter lead author: Pepa Aran 2.1 Learning objectives After you’ve gone over the lecture and solved the exercises, you should be able to: Use loops, conditional statements and functions in your code Write clean, stylized and structured code Look for help 2.2 Tutorial 2.2.1 Programming basics In this section, we will review the most basic programming elements (conditional statements, loops, functions…) for the R syntax. 2.2.1.1 Conditional statements In cases where we want certain statements to be executed or not, depending on a criterion, we can use conditional statements if, else if, and else. Conditionals are an essential feature of programming and available in all languages. The R syntax for conditional statements looks like this: if (temp &lt; 0.0){ is_frozen &lt;- TRUE } The evaluation of the criterion inside the round brackets (here (temp &lt; 0.0)) has to return either TRUE or FALSE. Whenever the statement between brackets is TRUE, the chunk of code between the subsequent curly brackets is executed. You can also write a conditional that covers all possibilities, like this: temp &lt;- -0.5 if (temp &lt; 0.0){ is_frozen &lt;- TRUE } else { is_frozen &lt;- FALSE } When the temperature is below 0, the first chunk of code is executed. Whenever it is greater or equal that 0 (i.e. the condition returns FALSE) the second chunk of code is evaluated. You can also write more than two conditions, covering several cases: is_frozen &lt;- FALSE just_cold &lt;- FALSE if (temp &lt; 0.0){ is_frozen &lt;- TRUE } else if (temp &lt; 10){ just_cold &lt;- TRUE } Note: In the code chunks above, an indentation was used to highlight which parts go together, which makes the code easy to understand. Indentations are not evaluated by R per se (unlike in other programming languages, e.g., Matlab, Python), but help to make the code easier to read. 2.2.1.2 Loops Loops are essential for solving many common tasks. for and while loops let us repeatedly execute the same set of commands, while changing an index or counter variable to take a sequence of different values. The following example calculates the sum of elements in the vector vec_temp by iteratively adding them together. vec_temp &lt;- seq(10) # equivalent to 1:10 temp_sum &lt;- 0 # initialize sum for (idx in seq(length(vec_temp))){ temp_sum &lt;- temp_sum + vec_temp[idx] } temp_sum ## [1] 55 Of course, this is equivalent to just using the sum() function. sum(vec_temp) Instead of directly telling R how many iterations it should do we can also define a condition and use a while-loop. As long as the condition is TRUE, R will continue iterating. As soon as it is FALSE, R stops the loop. The following lines of code do the same operation as the for loop above. What is different? What is the same? idx = 1 # initialize counter temp_sum &lt;- 0 # initialize sum while (idx &lt;= 10){ temp_sum &lt;- temp_sum + vec_temp[idx] idx = idx + 1 } temp_sum 2.2.1.3 Functions Often, analyses require many steps and your scripts may get excessively long. An important aspect of good programming is to avoid duplicating code. If the same sequence of multiple statements or functions are to be applied repeatedly, then it is usually advisable to bundle them into a new function and apply this single function to each object. This also has the advantage that if some requirement or variable name changes, it has to be edited only in one place. A further advantage of writing functions is that you can give the function an intuitively understandable name, so that your code reads like a sequence of orders given to a human. For example, the following code, converting temperature values provided in Fahrenheit to degrees Celsius, could be turned into a function. # not advisable temp_soil &lt;- (temp_soil - 32) * 5 / 9 temp_air &lt;- (temp_air - 32) * 5 / 9 temp_leaf &lt;- (temp_leaf - 32) * 5 / 9 Functions are a set of instructions encapsulated within curly brackets ({}) that generate a desired outcome. Functions contain four main elements: They start with a name to describe their purpose, then they need arguments, which are a list of the objects being input, enclosed by curly brackets function(x){ ... } for the code making up the body of the function, and lastly, within the body, a return statement indicating the output of the function. Below, we define our own function f2c(): # advisable f2c &lt;- function(temp_f){ temp_c &lt;- (temp_f - 32) * 5 / 9 return(temp_c) } temp_soil &lt;- f2c(temp_soil) temp_air &lt;- f2c(temp_air) temp_leaf &lt;- f2c(temp_leaf) Functions are essential for efficient programming. Functions have their own environment, which means that variables inside functions are only defined and usable within that function and are not saved to the global environment. Functions restrict the scope of the domain in which variables are defined. Information flows inside the function only through its arguments, and flows out of the function only through its returned variable. Functions (particularly long ones) can be written to separate source files with a suffix .R and saved in your ./R directory - “written” as in copy and paste the function text as in the code chunk above into a text file with .R suffix. Preferably, the file has the same name as the function. We can save the previous function in a script ./R/f2c.R and load it later by running source(\"./R/f2c\"). It’s good practice to keep one file per function, unless a function calls another function that is called nowhere else. In that case, the “sub-ordinate” function can be placed inside the same .R file. 2.2.2 Style your code Nice code is clean, readable, consistent, and extensible (easily modified or adapted). Ugly code works, but is hard to work with. There is no right or wrong about coding style, but certain aspects make it easier to read and use code. Here are a few points to consider. 2.2.2.1 Spaces and breaks Adding enough white spaces and line breaks in the right locations greatly helps the legibility of code. Cramming variables, operators, and brackets without spaces leaves an unintelligible sequence of characters and it will not be clear what parts go together. Therefore, consider the following points: Use spaces around operators (=, +, -, &lt;-, &gt;, etc.). Use &lt;-, not =, for allocating a value to a variable. Code inside curly brackets should be indented (recommended: two white spaces at the beginning of each line for each indentation level - don’t use tabs). For example: if (temp &gt; 5.0){ growth_temp &lt;- growth_temp + temp } 2.2.2.2 Variable naming It is recommended to use concise and descriptive variable names. Different variable naming styles are being used. In this course, we use lowercase letters, and underscores (_) to separate words within a variable name (_). Avoid (.) as they are reserved for certain types of objects in R. Also, avoid naming your objects with names of common functions and variables since your re-definition will mask already defined object names. For example, df_daily is a data frame with data at a daily resolution. Or clean_daily is a function that cleans daily data. Note that a verb is used as a name for a function and an underscore (_) is used to separate words. It is also recommended to avoid variable names consisting of only one character. Single-letter names make it practically impossible to search for that variable. # Good day_01 # Bad DayOne day.one first_day_of_the_month djm1 # Very bad mean &lt;- function(x) sum(x)/length(x) # mean() itself is already a function T &lt;- FALSE # T is an abbreviation of TRUE c &lt;- 10 # c() is used to create a vector (example &lt;- c(1, 2, 3)) 2.2.2.3 Script style Load libraries at the very beginning of a script, followed, by reading data or functions from files. Functions should be defined in separate .R files, unless they are only a few lines long. Then, place the sequence of statements. The name of the script should be short and concise and indicate what the script does. Use comments to describe in human-readable text what the code does. Comments are all that appears to the right of a # and are code parts that are not interpreted by R and not executed. Adding comments in the code greatly helps you and others to read your code, understand what it does, modify it, and resolve errors (bugs). To visually separate parts of a script, use commented lines (e.g., #----). The RStudio text editor automatically recognizes ---- added to the right end of a commented line and interprets it as a block of content which can be navigated by using the document (Outline button in the top right corner of the editor panel). Avoid reading entire workspace environments (e.g., load(old_environment.RData)), deleting environments rm(list=ls()), loading hidden dependencies (e.g., .Rprofile), or changing the working directory (setwd(\"~/other_dir/\") as part of a script. Note that information about the author of the script, its creation date, and modifications, etc. should not be added as commented text in the file. In Chapter 7, we will learn about the code versioning control system git, which keeps track of all this as meta information associated with files. A good and comprehensive best practices guide is given by the tidyverse style guide. 2.2.3 Where to find help The material covered in this course will give you a solid basis for your future projects. Even more so, it provides you with code examples that you can adapt to your own purposes. Naturally, you will face problems we did not cover in the course and you will learn more as you go. Different approaches to getting help can be taken for different types of problems and questions. 2.2.3.1 Within R “I know the name of a function that might help solve the problem but I do not know how to use it.” Typing a ? in front of the function will open the documentation of the function, giving information about a function’s purpose and method, arguments, the returned object, and examples. You have learned a few things about plots but you may not know how to make a boxplot: ?graphics::boxplot Running the above code will open the information on making boxplots in R. “There must be a function that does task X but I do not know which one.” Typing ?? will call the function help.search(). Maybe you want to save a plot as a JPEG but you do not know how: ??jpeg Note that it only looks through your installed packages. 2.2.3.2 Online To search in the entire library of R go to the website rdocumentation.org or turn to a search engine of your choice. It will send you to the appropriate function documentation or a helpful forum where someone has already asked a similar question. Most of the time you will end up on stackoverflow.com, a forum where most questions have already been answered. 2.2.3.3 Error messages If you do not understand the error message, start by searching for it on the web. Be aware that this is not always useful as developers rely on the error catching provided by R. To be more specific, add the name of the function and package you are using, to get a more detailed answer. 2.2.3.4 Worked examples Worked examples are implementations of certain workflows that may serve as a template for your own purpose. It is often simpler to adjust existing code to fulfill your purpose than to write it from scratch. Vignettes are provided for many packages and serve as example workflows that demonstrate the utility of package functions. You can type … vignette(&quot;caret&quot;, package = &quot;caret&quot;) … to get information about how to use the {caret} package in an easily digestible format. (You will learn more about caret in Chapters 9 and 10). Several blogs serve similar purposes and are a great entry point to learn about new topics. Examples are the Posit Blog (Posit is the company developing and maintaining RStudio and several R packages), R-bloggers, R-Ladies, etc. 2.2.3.5 Asking for help If you cannot find a solution online, start by asking your friends and colleagues. Someone with more experience than you might be able and willing to help you. When asking for help it is important to think about how you state the problem. The key to receiving help is to make it as easy as possible to understand the issue you are facing. Try to reduce what does not work to a simple example. Reproduce a problem with a simple data frame instead of one with thousands of rows. Generalize it in a way that people who do not do research in your field can understand the problem. If you are asking a question online in a forum include the output of sessionInfo() (it provides information about the R version, packages your using,…) and other information that can be helpful to understand the problem. Stackoverflow has its own guidelines on how to ask a good question, which you should follow. Here’s a great template you should use for R-specific question. If your question is well crafted and has not been answered before you can sometimes get an answer within 5 minutes. 2.3 Exercises Gauss variations Use a for loop to compute the sum of all natural numbers from 1 to 100. Print the result to the screen. Repeat this exercise but use a while loop. Add up all numbers between 1 and 100 that are at the same time a multiple of 3 and a multiple of 7. Print the result to the screen in the form of: The sum of multiples of 3 and 7 within 1-100 is: {your result}. Nested loops Given a matrix mymat and a vector myvec (see below), implement the following algorithm: Start with the first row in mymat. Fill all missing values in the current row of mymat with the maximum value in myvec. Drop the maximum value from myvec. Proceed to the next row of mymat and repeat steps 2-4. mymat and myvec are defined as: mymat &lt;- matrix(c(6, 7, 3, NA, 15, 6, 7, NA, 9, 12, 6, 11, NA, 3, 9, 4, 7, 3, 21, NA, 6, rep(NA, 7)), nrow = 4, byrow = TRUE) myvec &lt;- c(8, 4, 12, 9, 15, 6) Interpolation Define a vector \\(\\vec{v}\\) of length 100. Define the vector so that \\(v_i = 6\\), for \\(i = 1 : 25\\) and \\(v_i = -20\\), for \\(i = 66 : 100\\). Remaining elements are to be defined as ‘missing’. Linearly interpolate missing values that are not defined. Plot the values of \\(\\vec{v}\\) using plot(vec). "],["datawrangling.html", "Chapter 3 Data wrangling 3.1 Learning objectives 3.2 Tutorial 3.3 Extra material 3.4 Exercises 3.5 Report Exercise", " Chapter 3 Data wrangling Chapter lead author: Benjamin Stocker 3.1 Learning objectives In this chapter you will learn how to manipluate and transform data, a curcial part of the data science workflow. You will learn how to: read and transform tabulated data understand the ‘tidy’ data concept select variables Aggregate data handle bad and/or missing data 3.2 Tutorial Exploratory data analysis - the transformation, visualization, and modelling of data - is the central part of any (geo-) data science workflow and typically takes up a majority of the time we spend on a research project. The transformation of data often turns out to be particularly (and often surprisingly) time-demanding. Therefore, it is key to master typical steps of data transformation, and to implement them in a transparent fashion and efficiently - both in terms of robustness against coding errors (“bugs”) and in terms of code execution speed. We refer to data wrangling here to encompass the steps for preparing the data set prior to modelling - including, the combination of variables from different data sources, the removal of bad data, and the aggregation of data to the desired resolution or granularity (e.g., averaging over all time steps in a day, or over all replicates in a sample). In contrast, pre-processing refers to the additional steps that are either required by the the specific machine learning algorithm used with the data (e.g., centering and scaling for K-Nearest Neighbors or Neural Networks), the gap-filling of variables, or the transformation of variables guided by the resulting improvement of the predictive power of the machine learning model. Pre-processing is part of the modelling workflow and includes all steps that apply transformations that use parameters derived from the data. We will introduce and discuss data pre-processing in Chapter 9. 3.2.1 Example data The example data used in this chapter are parallel time series of (gaseous) CO\\(_2\\) and water vapor exchange fluxes between the vegetation and the atmosphere, along with various meteorological variables measured in parallel. Quasi-continuous measurements of temporally changing gas exchange fluxes are obtained with the eddy covariance technique which relies on the parallel quantification of vertical wind speeds and gas concentrations. The data are provided at half-hourly resolution for the site CH-Lae, located on the south slope of the Lägern mountain on the Swiss Plateau at 689 m a.s.l. in a mixed forest with a distinct seasonal course of active green leaves (a substantial portion of the trees in the measured forest are deciduous). The dataset is generated and formatted following standard protocols (FLUXNET2015). For more information of the variables in the dataset, see the FLUXNET2015 website and Pastorello et al., 2020 for a comprehensive documentation of variable definitions and methods. For our demonstrations, the following variables are the most relevant: TIMESTAMP_START: Hour and day of the start of the measurement period for which the respective row’s data are representative. Provided in a format of “YYYYMMDDhhmm”. TIMESTAMP_END: Hour and day of the end of the measurement period for which the respective row’s data are representative. Provided in a format of “YYYYMMDDhhmm”. TA_* (°C): Air temperature. SW_IN_* (W m\\(^{-2}\\)): Shortwave incoming radiation LW_IN_* (W m\\(^{-2}\\)): Longwave incoming radiation VPD_* (hPa): Vapor pressure deficit (the difference between actual and saturation water vapor pressure) PA_* (kPa): Atmospheric pressure P_* (mm): Precipitation WS_* (m \\(^{-1}\\)): Wind speed SWC_* (%): Volumetric soil water content GPP_* (\\(\\mu\\)mol CO\\(_2\\) m\\(^{-2}\\) s\\(^{-1}\\)): Gross primary production (the ecosystem-level gross CO\\(_2\\) uptake flux driven by photosynthesis) *_QC: Quality control information for the variable *. Important for us: NEE_*_QC is the quality control information for the net ecosystem CO\\(_2\\) exchange flux (NEE_*) and for GPP derived from the corresponding NEE estimate (GPP_*). 0 = measured, 1 = good quality gap-filled, 2 = medium, 3 = poor. Suffixes _* indicate that multiple estimates for respective variables are available and distinguished by different suffixes. For example, variables TA_* contain the same information, but are derived with slightly different assumptions and gap-filling techniques. The meanings of suffixes are described in Pastorello et al., 2020. 3.2.2 Tidyverse The tidyverse is a collection of R packages and functions that share a common design philosophy, enabling a particularly efficient implementation of transformation steps on tabular data. The most important data and function design principle of the tidyverse is that each function takes a data frame as its first argument and returns a data frame as its output. From this design principles, even the most convoluted code and implementation of data transformation steps fall into place and fast and error-free progression through exploratory data analysis is facilitated. Therefore, you will be introduced to the R {tidyverse} here and we heavily rely on this dialect of the R language throughout the remainder of this course. 3.2.3 Reading tabular data Tabular data are organised in rows and columns. R data frames are tabular data. As introduced in Chapter 1, each column can be regarded as a vector of a certain type. Each row contains the same number of columns and each column contains the same type of values (for example numeric, or characters). Each row can be regarded as a separate instance or data record - for example a record of simultaneously taken measurements of different variables, along with some attributes and meta information (e.g., the date). In Chapter 5, you will be introduced to other types of data. The most common format for tabular data are CSV (comma-separated-values), typically indicated by the file name suffix .csv. CSV is a text-based file format, readable across platforms and does not rely on proprietary software (as opposed to, for example, .xlsx). The first row in a CSV file typically specifies the name of the variable provided in the respective column. Let’s get started with working with our example data set and read it into R, as the variable half_hourly_fluxes. Note that the naming of variables can be important for keeping code legible. Chose intuitively understandable names that describe what the object represents (as done here). To import the data into the R environment, we use the function read_csv() from the {readr} package (part of tidyverse). In other R code, you will also encounter the base R read.csv() function. However, read_csv() is much faster and reads data into a tidyverse-data frame (a tibble) which has some useful additional characteristics, on top of a common R data frame. For example, tibbles generate a nicely readable output when printing the object as is done below. half_hourly_fluxes &lt;- readr::read_csv(&quot;./data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv&quot;) half_hourly_fluxes ## # A tibble: 52,608 × 235 ## TIMESTAMP_START TIMESTAMP_END TA_F_MDS TA_F_MDS_QC TA_ERA TA_F TA_F_QC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 200401010000 200401010030 -9999 -9999 -2.22 -2.22 2 ## 2 200401010030 200401010100 -9999 -9999 -2.25 -2.25 2 ## 3 200401010100 200401010130 -9999 -9999 -2.28 -2.28 2 ## 4 200401010130 200401010200 -9999 -9999 -2.50 -2.50 2 ## 5 200401010200 200401010230 -9999 -9999 -2.72 -2.72 2 ## 6 200401010230 200401010300 -9999 -9999 -2.94 -2.94 2 ## 7 200401010300 200401010330 -9999 -9999 -3.17 -3.17 2 ## 8 200401010330 200401010400 -9999 -9999 -3.39 -3.39 2 ## 9 200401010400 200401010430 -9999 -9999 -3.61 -3.61 2 ## 10 200401010430 200401010500 -9999 -9999 -3.59 -3.59 2 ## # ℹ 52,598 more rows ## # ℹ 228 more variables: SW_IN_POT &lt;dbl&gt;, SW_IN_F_MDS &lt;dbl&gt;, ## # SW_IN_F_MDS_QC &lt;dbl&gt;, SW_IN_ERA &lt;dbl&gt;, SW_IN_F &lt;dbl&gt;, SW_IN_F_QC &lt;dbl&gt;, ## # LW_IN_F_MDS &lt;dbl&gt;, LW_IN_F_MDS_QC &lt;dbl&gt;, LW_IN_ERA &lt;dbl&gt;, LW_IN_F &lt;dbl&gt;, ## # LW_IN_F_QC &lt;dbl&gt;, LW_IN_JSB &lt;dbl&gt;, LW_IN_JSB_QC &lt;dbl&gt;, LW_IN_JSB_ERA &lt;dbl&gt;, ## # LW_IN_JSB_F &lt;dbl&gt;, LW_IN_JSB_F_QC &lt;dbl&gt;, VPD_F_MDS &lt;dbl&gt;, ## # VPD_F_MDS_QC &lt;dbl&gt;, VPD_ERA &lt;dbl&gt;, VPD_F &lt;dbl&gt;, VPD_F_QC &lt;dbl&gt;, PA &lt;dbl&gt;, … To reproduce this code chunk, you can download the file FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv from here and read it from the local path where the file is stored on your machine. All data files used in this tutorials are stored here. Since the file is properly formatted, with variable names given in the first line of the file, the function read_csv() identifies them correctly as column names and interprets values in each column as values of a consistent type (as numeric &lt;dbl&gt;). The file is also automatically machine-readable because it has no merged cells and only one value per cell. 3.2.4 Variable selection For our further data exploration, we will reduce the data frame we are working with and select a reduced set of variables. Reducing the dataset can have the advantage of speeding up further processing steps, especially when the data are large. For the further steps in this chapter we will now subset our original data. We select the following variants of variables described above, plus some additional variables (further information in Pastorello et al., 2020): All variables with names starting with TIMESTAMP) All meteorological variables derived following the “final gap-filled method”, as indicated with names ending with _F. GPP estimates that are based on the nighttime decomposition method, using the “most representative” of different gap-filling versions, after having applied the variable u-star filtering method (GPP_NT_VUT_REF) and the corresponding quality control information (NEE_VUT_REF_QC) Soil water measured at different depths (variables starting with SWC_F_MDS_) Do not use any radiation variables derived with the “JSBACH” algorithm (not with a name that contains the string JSB) Flag indicating whether a time step is at night (NIGHT) This is implemented by: half_hourly_fluxes &lt;- select( half_hourly_fluxes, starts_with(&quot;TIMESTAMP&quot;), ends_with(&quot;_F&quot;), GPP_NT_VUT_REF, NEE_VUT_REF_QC, starts_with(&quot;SWC_F_MDS_&quot;), -contains(&quot;JSB&quot;), NIGHT ) This reduces our dataset from 235 available variables to 20 variables. As you can see, select() is a powerful tool to apply multiple selection criteria on your data frame in one step. It takes many functions that make filtering the columns easier. For example, criteria can be formulated based on the variable names with starts_with(), ends_with(), contains(), matches(), etc. Using these functions within select() can help if several column names start with the same characters or contain the same pattern and all need to be selected. If a minus (-) is added in front of a column name or one of the mentioned functions within select(), then R will not include the stated column(s). Note that the selection criteria are evaluated in the order we write them in the select() function call. You can find the complete reference for selecting variables here. 3.2.5 Time objects The automatic interpretation of the variables TIMESTAMP_START and TIMESTAMP_END by the function read_csv() is not optimal: class(half_hourly_fluxes$TIMESTAMP_START[[1]]) ## [1] &quot;numeric&quot; as.character(half_hourly_fluxes$TIMESTAMP_START[[1]]) ## [1] &quot;200401010000&quot; As we can see, it is considered by R as a numeric variable with 12 digits (“double-precision”, occupying 64 bits in computer memory). After printing the variable as a string, we can guess that the format is: YYYYMMDDhhmm. The {lubridate} package is designed to facilitate processing date and time objects. Knowing the format of the timestamp variables in our dataset, we can use ymd_hm() to convert them to actual date-time objects. dates &lt;- ymd_hm(half_hourly_fluxes$TIMESTAMP_START) dates[1] ## [1] &quot;2004-01-01 UTC&quot; Working with such date-time objects facilitates typical operations on time series. For example, adding one day can be done by: nextday &lt;- dates + days(1) nextday[1] ## [1] &quot;2004-01-02 UTC&quot; The following returns the month of each date object: month(dates[1]) ## [1] 1 The number 1 stands for the month of the year, i.e., January. You can find more information on formatting dates and time within the {tidyverse} here, and a complete reference of the {lubridate} package is available here. 3.2.6 Variable (re-) definition Since read_csv() did not interpret the TIMESTAMP_* variables as desired, we may convert the entire column in the data frame into a date-time object. In base-R, we would do this by: half_hourly_fluxes$TIMESTAMP_START &lt;- ymd_hm(half_hourly_fluxes$TIMESTAMP_START) Modifying existing or creating new variables (columns) in a data frame is done in the {tidyverse} using the function mutate(). The equivalent statement is: half_hourly_fluxes &lt;- mutate( half_hourly_fluxes, TIMESTAMP_START = ymd_hm(TIMESTAMP_START) ) Note: Avoid using whitespaces (‘Leerzeichen’) to name columns in a dataframe because it can cause troubles down the line. Instead, use _ to separate words in one name. In the code chunk above, the function mutate() is from the tidyverse package {dplyr}. It takes a dataframe as its first argument (here half_hourly_fluxes) and returns a dataframe as its output. You will encounter an alternative, but equivalent, syntax in the following form: half_hourly_fluxes &lt;- half_hourly_fluxes |&gt; mutate(TIMESTAMP_START = ymd_hm(TIMESTAMP_START)) Here, the pipe operator |&gt; is used. It “pipes” the object evaluated on its left side into the function on its right side, where the object takes the place of (but is not spelled out as) the first argument of that function. Using the pipe operator can have the advantage of facilitating the separation, removal, inserting, or re-arranging of individual transformation steps. Arguably, it facilitates reading code, especially for complex data transformation workflows. Therefore, you will encounter the pipe operator frequently throughout the remainder of this course. Note: The pipe operator is so popular that has been recently included in the latest versions of base R (version 4.1.0 and beyond). This is the |&gt; pipe we just introduced. Nevertheless, you may encounter the %&gt;% operator, which is the original pipe from the {magrittr} package (part of the {tidyverse}). Note: If you cannot update R and have a version older than 4.1.0, just use the magrittr pipe %&gt;% throughout. This can happen if you have an older Macbook that can’t operate the latest Operating System version. Mutating both our timestamp variables could be written as mutate(TIMESTAMP_START = ymd_hm(TIMESTAMP_START), TIMESTAMP_END = ymd_hm(TIMESTAMP_END)). Sometimes, such multiple-variable mutate statements can get quite long. A handy short version of this can be implemented using across(): half_hourly_fluxes &lt;- half_hourly_fluxes |&gt; mutate(across(starts_with(&quot;TIMESTAMP_&quot;), ymd_hm)) We will encounter more ways to use mutate later in this tutorial. A complete reference to mutate() is available here. If you only want to change the name of a variable, but not modify its values, you can do so with the {dplyr} function rename(). 3.2.7 Axes of variation Tabular data are two-dimensional (rows \\(\\times\\) columns), but not all two-dimensional data are tabular. For example, raster data are a two-dimensional array of data (a matrix) representing variables on an evenly spaced grid, for example pixels in remotely sensed imagery. For example the volcano data (provided as an example dataset in R) is a matrix - each column contains the same variable, and no variable names are provided. volcano[1:5, 1:5] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 100 100 101 101 101 ## [2,] 101 101 102 102 102 ## [3,] 102 102 103 103 103 ## [4,] 103 103 104 104 104 ## [5,] 104 104 105 105 105 In the volcano dataset, rows and columns represent different geographic positions in latitude and longitude, respectively. The volcano data are not tabular data. Another typical example for non-tabular data are climate model outputs. They are typically given as arrays with more than two dimensions. Typically, this is longitude, latitude, and time, and sometimes a vertical dimension representing, for example, elevation. Such data are multi-dimensional and, as such, not tabular. Tabular data, although formatted in two dimensions by rows and columns, may represent data that varies along multiple axes. Most environmental data are structured, that is, values of “nearby” observations tend to be more similar than values of “distant” observations. Here, “nearby” and “distant” may refer to a spatial distance, but not necessarily so. Structure in data arises from similarity of the subjects generating the data (e.g., evapotranspiration over two croplands may be more similar than evapotranspiration over a forest), or from temporal proximity. In biological data, there may be a genetic structure arising from evolutionary relatedness (Roberts et al., 2016). Note also that temporal proximity is more complex than than being governed by a single dimension - time. In environmental data, time is often expressed through periodically varying conditions (the diurnal and seasonal cycles). It’s often critical to understand and account for the structure in data when analysing it and using it for model fitting. Challenges are posed when structure is not apparent or not known. Note also that some structures are hierarchical. For example, data may be structured by postal codes within cantons; or by hours within a day within a year. Biological data may be generated by species within genera within families. Data from experiments is typically structured as samples within treatments. You see, structure in data is rather the rule than the exception. Our example data contains values recorded at each half-hourly time interval over the course of eleven years (check by nrow(half_hourly_fluxes)/(2*24*365)). The data are recorded at a site, located in the temperate climate zone, where solar radiation and therefore also other meteorological variables and ecosystem fluxes vary substantially over the course of a day and over the course of a year. Although not explicitly separated, the date-time object thus encodes information along multiple axes of variation in the data. For example, over the course of one day (2*24 rows in our data), the shortwave incoming radiation SW_IN_F varies over a typical diurnal cycle: plot( half_hourly_fluxes[1:(2*24),]$TIMESTAMP_START, half_hourly_fluxes[1:(2*24),]$SW_IN_F, type = &quot;l&quot; ) Note: plot() is the very basic of plotting data. In Chapter 4, you will get introduced to additional methods for visualising data. The argument type = \"l\" indicates that we want a line plot, rather than points. Over the course of an entire year, shortwave incoming radiation varies with the seasons, peaking in summer: plot( half_hourly_fluxes[1:(365*2*24),]$TIMESTAMP_START, half_hourly_fluxes[1:(365*2*24),]$SW_IN_F, type = &quot;l&quot; ) All data frames have two dimensions, rows and columns. Our data frame is organised along half-hourly time steps in rows. As described above, these time steps belong to different days, months, and years, although these “axes of variation” are not reflected by the structure of the data frame object itself and we do not have columns that indicate the day, month or year of each half-hourly time step. This would be redundant information since the date-time objects of columns TIMESTAMP_* contain this information. However, for certain applications, it may be useful to separate information regarding these axes of variation more explicitly. For example by: half_hourly_fluxes |&gt; mutate(year = year(TIMESTAMP_START), month = month(TIMESTAMP_START), doy = yday(TIMESTAMP_START) # day of year ) |&gt; select(TIMESTAMP_START, TIMESTAMP_END, year, month, doy) # for displaying ## # A tibble: 52,608 × 5 ## TIMESTAMP_START TIMESTAMP_END year month doy ## &lt;dttm&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2004-01-01 00:00:00 2004-01-01 00:30:00 2004 1 1 ## 2 2004-01-01 00:30:00 2004-01-01 01:00:00 2004 1 1 ## 3 2004-01-01 01:00:00 2004-01-01 01:30:00 2004 1 1 ## 4 2004-01-01 01:30:00 2004-01-01 02:00:00 2004 1 1 ## 5 2004-01-01 02:00:00 2004-01-01 02:30:00 2004 1 1 ## 6 2004-01-01 02:30:00 2004-01-01 03:00:00 2004 1 1 ## 7 2004-01-01 03:00:00 2004-01-01 03:30:00 2004 1 1 ## 8 2004-01-01 03:30:00 2004-01-01 04:00:00 2004 1 1 ## 9 2004-01-01 04:00:00 2004-01-01 04:30:00 2004 1 1 ## 10 2004-01-01 04:30:00 2004-01-01 05:00:00 2004 1 1 ## # ℹ 52,598 more rows Note that we used mutate() here to create a new variable (column) in the data frame, as opposed to above where we overwrote an existing variable with the same function. 3.2.8 Tidy data Data comes in many forms and shapes. For example, Excel provides a playground for even the wildest layouts of information in some remotely tabular form and merged cells as we will see in the Exercises. A data frame imposes a relatively strict formatting in named columns of equal length. But even data frames can come in various shapes - even if the information they contain is the same. co2_concentration ## # A tibble: 36 × 3 ## year month co2_concentration ## &lt;int&gt; &lt;ord&gt; &lt;dbl&gt; ## 1 1959 Jan 315. ## 2 1959 Feb 316. ## 3 1959 Mar 316. ## 4 1959 Apr 318. ## 5 1959 May 318. ## 6 1959 Jun 318 ## 7 1959 Jul 316. ## 8 1959 Aug 315. ## 9 1959 Sep 314. ## 10 1959 Oct 313. ## # ℹ 26 more rows co2_concentration_monthly ## # A tibble: 3 × 13 ## year Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1959 315. 316. 316. 318. 318. 318 316. 315. 314. 313. 315. 315. ## 2 1960 316. 317. 317. 319. 320. 319. 318. 316. 314 314. 315. 316. ## 3 1961 317. 318. 318. 319. 320. 320. 318. 317. 315. 315. 316. 317. co2_concentration_yearly ## # A tibble: 12 × 4 ## month `1959` `1960` `1961` ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jan 315. 316. 317. ## 2 Feb 316. 317. 318. ## 3 Mar 316. 317. 318. ## 4 Apr 318. 319. 319. ## 5 May 318. 320. 320. ## 6 Jun 318 319. 320. ## 7 Jul 316. 318. 318. ## 8 Aug 315. 316. 317. ## 9 Sep 314. 314 315. ## 10 Oct 313. 314. 315. ## 11 Nov 315. 315. 316. ## 12 Dec 315. 316. 317. There are advantages for interoperability and ease of use when data frames come with consistent layouts, adhering to certain design principles. We have learned that in tabular data, each row contains the same number of columns and each column contains the same type of values (for example numeric, or characters). And that each row can be regarded as a separate instance of the same type, for example a record of simultaneously taken measurements, along with some attributes. Following these principles strictly leads to tidy data. In essence, quoting Wickham and Grolemund (2017), data are tidy if: Each variable has its own column. Each observation has its own row. Each value has its own cell. Figure 3.1: Rules for tidy data. Figure from Wickham and Grolemund (2017) The {tidyr} package provides powerful functions to make data tidy. In the examples above, co2_concentration_monthly and and co2_concentration_yearly are not tidy. In co2_concentration_monthly, the same variable (CO2 concentration) appears in multiple columns. Organising columns by months leads to a “wide” table format. We can convert it to a “long” format by: co2_concentration_monthly |&gt; pivot_longer(cols = 2:13, names_to = &quot;month&quot;, values_to = &quot;co2&quot;) ## # A tibble: 36 × 3 ## year month co2 ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1959 Jan 315. ## 2 1959 Feb 316. ## 3 1959 Mar 316. ## 4 1959 Apr 318. ## 5 1959 May 318. ## 6 1959 Jun 318 ## 7 1959 Jul 316. ## 8 1959 Aug 315. ## 9 1959 Sep 314. ## 10 1959 Oct 313. ## # ℹ 26 more rows This corresponds to the format of co2_concentration and is tidy. A long format of data frames is required to visualise data using the plotting functions of the {ggplot2} package which will be introduced in Chapter 4. Either way, for certain applications, it may be advantageous to work with a wide format. We can convert from a long to a wide format by: co2_concentration |&gt; pivot_wider(names_from = year, values_from = co2_concentration) ## # A tibble: 12 × 4 ## month `1959` `1960` `1961` ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jan 315. 316. 317. ## 2 Feb 316. 317. 318. ## 3 Mar 316. 317. 318. ## 4 Apr 318. 319. 319. ## 5 May 318. 320. 320. ## 6 Jun 318 319. 320. ## 7 Jul 316. 318. 318. ## 8 Aug 315. 316. 317. ## 9 Sep 314. 314 315. ## 10 Oct 313. 314. 315. ## 11 Nov 315. 315. 316. ## 12 Dec 315. 316. 317. When seeking, for example, the average CO2 concentration for each month, you may be tempted to work with a wide data frame and treat it as a matrix to calculate a mean by rows. You can do so, but then, you leave the tidyverse. This will complicate your life. You’ll learn how to perform tidy data aggregation below. The concept of tidy data can even be taken further by understanding a “value” as any object type, e.g., a list or a data frame. This leads to a list or data frame “nested” within a data frame. You will learn more about this below. 3.2.9 Aggregating data Aggregating data refers to collapsing a larger set of values into a smaller set of values that are derived from the larger set. For example, we can aggregate over all \\(N\\) rows in a data frame (\\(N\\times M\\)), calculating the sum for each of the \\(M\\) columns. This returns a data frame (\\(1 \\times M\\)) with the same number of columns as the initial data frame, but only one row. Often, aggregations are done not across all rows but for rows within \\(G\\) groups of rows. This yields a data frame (\\(G \\times M\\)) with the number of rows corresponding to the number of groups. Let’s say we want to calculate the mean of half-hourly shortwave radiation within each day. We thus have \\(N\\) half-hourly time steps in \\(G\\) days. That is, to aggregate our half-hourly data to daily data by taking a mean. There are two pieces of information needed for an aggregation step: The factor (or “axis of variation”), here days, that groups a vector of values for collapsing it into a single value, and the function used for collapsing values, here, the mean() function. This function should take a vector as an argument and return a single value as an output. These two steps are implemented by the {dplyr} functions group_by() and summarise(). The entire aggregation workflow is implemented by the following code: daily_fluxes &lt;- half_hourly_fluxes |&gt; mutate(date = as_date(TIMESTAMP_START)) |&gt; # converts the ymd_hm-formatted date-time object to a date-only object (ymd) group_by(date) |&gt; summarise(SW_IN_F = mean(SW_IN_F)) The seasonal course can now be more clearly be visualized with the data aggregated to daily values. plot(daily_fluxes[1:365,]$date, daily_fluxes[1:365,]$SW_IN_F, type = &quot;l&quot;) We can also apply multiple aggregation functions to different variables simultaneously. In the example below, we aggregate half-hourly data to daily data by… taking the daily mean GPP counting the number of half-hourly data points by day counting the number of measured (not gap-filled) data points taking the mean shortwave radiation Finally, we calculate the fraction of measured underlying half-hourly data from which the aggregation is calculated and we save the daily data frame as a CSV file for later use. daily_fluxes &lt;- half_hourly_fluxes |&gt; mutate(date = as_date(TIMESTAMP_START)) |&gt; # converts time object to a date object group_by(date) |&gt; summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF, na.rm = TRUE), n_datapoints = n(), # counts the number of observations per day n_measured = sum(NEE_VUT_REF_QC == 0), # counts the number of actually measured data (excluding gap-filled and poor quality data) SW_IN_F = mean(SW_IN_F, na.rm = TRUE), # we will use this later .groups = &#39;drop&#39; # to un-group the resulting data frame ) |&gt; mutate(f_measured = n_measured / n_datapoints) # calculate the fraction of measured values over total observations write_csv(daily_fluxes, file = &quot;data/daily_fluxes.csv&quot;) daily_fluxes ## # A tibble: 1,096 × 6 ## date GPP_NT_VUT_REF n_datapoints n_measured SW_IN_F f_measured ## &lt;date&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2004-01-01 -0.0138 48 0 38.1 0 ## 2 2004-01-02 0.768 48 0 23.9 0 ## 3 2004-01-03 0.673 48 0 54.1 0 ## 4 2004-01-04 -0.322 48 0 41.7 0 ## 5 2004-01-05 0.841 48 0 17.4 0 ## 6 2004-01-06 1.22 48 0 40.5 0 ## 7 2004-01-07 0.215 48 0 31.6 0 ## 8 2004-01-08 1.11 48 0 58.4 0 ## 9 2004-01-09 1.44 48 0 11.9 0 ## 10 2004-01-10 0.364 48 0 27.6 0 ## # ℹ 1,086 more rows Note that above, we specified the argument .groups = 'drop' to “un-group” the resulting data frame. The same can also be achieved by a separate function call ungroup() after the summarise() step. More info on how to group values using summarise functions here, or a summary on the inputs the function group_by() and summarise() take. Aggregating is related to nesting performed by the {tidyr} function nest(): half_hourly_fluxes |&gt; mutate(date = as_date(TIMESTAMP_START)) |&gt; group_by(date) |&gt; nest() ## # A tibble: 1,096 × 2 ## # Groups: date [1,096] ## date data ## &lt;date&gt; &lt;list&gt; ## 1 2004-01-01 &lt;tibble [48 × 20]&gt; ## 2 2004-01-02 &lt;tibble [48 × 20]&gt; ## 3 2004-01-03 &lt;tibble [48 × 20]&gt; ## 4 2004-01-04 &lt;tibble [48 × 20]&gt; ## 5 2004-01-05 &lt;tibble [48 × 20]&gt; ## 6 2004-01-06 &lt;tibble [48 × 20]&gt; ## 7 2004-01-07 &lt;tibble [48 × 20]&gt; ## 8 2004-01-08 &lt;tibble [48 × 20]&gt; ## 9 2004-01-09 &lt;tibble [48 × 20]&gt; ## 10 2004-01-10 &lt;tibble [48 × 20]&gt; ## # ℹ 1,086 more rows Here, the data frame has one row per date and therefore the same number of rows as the data frame daily_fluxes, but the data itself is not reduced by a summarising function. Instead, the data are kept at the half-hourly level, but it’s nested inside the new column data, which now contains a list of half-hourly data frames for each day. This is just a brief perspective of what nesting is about. More is explained in Section 3.3. More comprehensive tutorials on nesting and functional programming are provided in Altman, Behrman and Wickham (2021) or in Wickham and Grolemund (2017), Chapter 21. 3.2.10 Data cleaning Data cleaning is often a time-consuming task and decisions taken during data cleaning may be critical for analyses and modelling. In the following, we distinguish between cleaning formats, the identification (and removal) of “bad” data, and the gap-filling of missing or removed data. An excellent resource for further reading is the Quartz Guide to Bad Data which provides an overview of how to deal with different types of bad data. 3.2.10.1 Cleaning formats As a general principle, we want to have machine readable data. Key for achieving machine-readability is that a cell should only contain one value of one type. Hence, for example, character strings should be kept in separate columns (as separate variables) from numeric data. Character strings can impose particular challenges for achieving machine-readability. Typically, they encode categorical or ordinal information, but are prone to spelling inconsistencies or errors that undermine the ordering or categorization. Here are typical examples for challenges working with character strings and lessons for avoiding problems: Often, character strings encode the units of a measurement, and entries may be c(\"kg m-2\", \"kg/m2\", \"Kg / m2\", \"1000 g m-2\") . They are all equivalent, but “the machine” treats them as non-identical. To clean such data, one may compile a lookup-table to identify equivalent (but not identical) strings. Much better is to specify a consistent treatment of units before data collection. Even if the data are clean and contain a consistently spelled categorical variable in the form of a character string, R doesn’t necessarily treat it as categorical. For certain downstream steps of the workflow, it may be necessary to transform such a variable to one of type factor. For example, as entries of an unordered categorical variable, we have unique(df$gender) = c(\"female\", \"male\", \"non-binary\"). To treat them as categorical and not just mere character strings, we would have to do: df &lt;- df |&gt; dplyr::mutate(gender = as.factor(gender)) Character strings may encode ordinal information. For example, entries specify quality control information and are one of c(\"good quality\", \"fair quality, \"poor quality\"). A challenge could be that the spelling is inconsistent (c(\"Good quality\", \"good quality\", …)). Using integers (positive natural numbers) instead of character strings avoids such challenges and enforces an order. The quality control variable NEE_VUT_REF_QC in our example dataset half_hourly_fluxes follows this approach: unique(half_hourly_fluxes$NEE_VUT_REF_QC) ## [1] 3 2 1 0 An entry like &gt;10 m is not a friend of a data scientist. Here, we have three pieces of information: &gt; as in “greater than”, 10, and m indicating the units. A machine-readable format would be obtained by creating separate columns for each piece of information. The &gt; should be avoided already at the stage of recording the data. Here, we may have to find a solution for encoding it in a machine readable manner (see Exercises). String manipulations are usually required for cleaning data. The Section @ref(#strings) below demonstrates some simple examples. Note that a majority of machine learning algorithms and other statistical model types require all data to be numeric. Methods exist to convert categorical data into numeric data, as we will learn later. We re-visit data cleaning in the form of data pre-processing as part of the modelling workflow in Chapter 9. 3.2.10.2 Bad data Data may be “bad” for different reasons, including sensor error, human error, a data point representing a different population, or unsuitable measurement conditions. In this sense, data are “bad” if they don’t represent what they are assumed by the user to represent. Its presence in analyses and modelling may undermine the model skill or even lead to spurious results. A goal of data cleaning typically is to remove bad data. But how to detect them? And how safe is it to remove them? A diversity of processes may generate bad data and it is often not possible to formulate rules and criteria for their identification a priori. Therefore, an understanding of the data and the data generation processes is important for the identification and treatment of bad data. Often, such an understanding is gained by repeated exploratory data analysis cycles, involving the visualization, transformation, and analysis of the data. Ideally, information about the quality of the data are provided as part of the dataset. Also other meta-information (e.g., sensor type, human recording the data, environmental conditions during the data collection) may be valuable for data cleaning purposes. In our example dataset, the column with suffices _QC provide such information (see ‘Example data’ section above) and an example for their use in data cleaning is given further below. Bad data may come in the form of outliers, which are commonly defined based on their value with respect to the distribution of all values of the same variable in a dataset. Hence, their identification most commonly relies on quantifying their distance from the center of the variable’s empirical distribution. The default boxplot() plotting function in R (which we will learn about more in Chapter 4) shows the median (bold line in the center), the upper and lower quartiles (corresponding to the 25% and the 75% quantiles, often referred to as \\(Q_1\\) and \\(Q_3\\) , given by the upper and lower edge of the box plot) and the range of \\(( Q_1 - 1.5 (Q_3 - Q_1), Q_3 + 1.5 (Q_3 - Q_1))\\). Any point outside this range is plotted by a circle and labeled an “outlier”. However, this definition is very restrictive and may lead to a false labeling of outliers, in particular if they are drawn from a distribution with a fat tail or from asymmetrical distributions. Outliers may also be identified via multivariate distributions. We will re-visit such methods later, in Chapter 8. For certain applications, outliers or anomalies may be the target of the investigation, not the noise in the data. This has spurred the field of anomaly detection which relies on machine learning algorithms for determining whether a value is anomalous, given a set of covariates. Sensor error or algorithm error may generate spurious values, identified, for example when a continuous variable attains the numerically identical value with a spuriously high frequency. half_hourly_fluxes$GPP_NT_VUT_REF |&gt; table() |&gt; sort(decreasing = TRUE) |&gt; head() ## ## 5.18422 3.54996 1.3107 -5.57199 0.984756 2.49444 ## 32 22 19 18 17 17 The probability of a certain numeric value of a continuous variable to appear twice in a dataset is practically zero. Here, several values appear multiple times - the value 5.18422 even 32 times! This must be bad data. Other processes may lead to spurious trends or drift in the data, for example caused by sensor degradation. Spurious step changes or change points in time series or in (multivariate) regressions may be related to the replacement or deplacement of the measuring device. Different methods and R libraries help identifying such cases (see for example this tutorial). Solutions have to be found for the remediation of such spurious patterns in the data on a case-by-case basis. 3.2.10.3 Handling missing data The question about when data are “bad” and whether to remove it is often critical. Such decisions are important to keep track of and should be reported as transparently as possible in publications. In reality, where the data generation process may start in the field with actual human beings writing notes in a lab book, and where the human collecting the data is often not the same as the human analyzing the data or writing the paper, it’s often more difficult to keep track of such decisions. As a general principle, it is advisable to design data records such that decisions made during the data collection process remain transparent throughout all stages of the workflow and that sufficient information be collected to enable later revisions of particularly critical decisions. In practice, this means that the removal of data and entire rows should be avoided and implemented only at the very last step if necessary (e.g., when passing the data into a model fitting function). Instead, information about whether data are bad should be kept in a separate, categorical, variable (a quality control variable, like *_QC variables in our example data half_hourly_fluxes). Data may be missing for several reasons. Some yield random patterns of missing data, others not. In the latter case, we can speak of informative missingness (Kuhn and Johnson, 2019) and its information can be used for modelling. For categorical data, we may replace such data with \"none\" (instead of NA). Some machine learning algorithms (mainly tree-based methods, e.g., Random Forest) can handle missing values. However, when comparing the performance of alternative ML algorithms, they should be tested with the same data and removing missing data should be done beforehand. Most machine learning algorithms require missing values to be removed. That is, if any of the cells in one row has a missing value, the entire cell gets removed. This generally leads to a loss of information contained in the remaining variables. Methods exist to impute missing values in order to avoid this information loss. However, the gain of data imputation has to be traded off against effects of associating the available variables with the imputed (knowingly wrong) values, and effects of data leakage have to be considered. Data imputation as part of the modelling process will be dealt with in Chapter 9. In our example dataset, some values of SWC_F_MDS_* are given as -9999. half_hourly_fluxes |&gt; select(TIMESTAMP_START, starts_with(&quot;SWC_F_MDS_&quot;)) |&gt; head() ## # A tibble: 6 × 9 ## TIMESTAMP_START SWC_F_MDS_1 SWC_F_MDS_2 SWC_F_MDS_3 SWC_F_MDS_4 ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2004-01-01 00:00:00 -9999 -9999 -9999 -9999 ## 2 2004-01-01 00:30:00 -9999 -9999 -9999 -9999 ## 3 2004-01-01 01:00:00 -9999 -9999 -9999 -9999 ## 4 2004-01-01 01:30:00 -9999 -9999 -9999 -9999 ## 5 2004-01-01 02:00:00 -9999 -9999 -9999 -9999 ## 6 2004-01-01 02:30:00 -9999 -9999 -9999 -9999 ## # ℹ 4 more variables: SWC_F_MDS_1_QC &lt;dbl&gt;, SWC_F_MDS_2_QC &lt;dbl&gt;, ## # SWC_F_MDS_3_QC &lt;dbl&gt;, SWC_F_MDS_4_QC &lt;dbl&gt; When reading the documentation of this specific dataset, we learn that -9999 is the code for missing data. The {dplyr} functions help us to clarify these missing values by mutating across all numeric variables and overwrite entries with NA if they hold a -9999. half_hourly_fluxes &lt;- half_hourly_fluxes |&gt; mutate(across(where(is.numeric), ~na_if(., -9999))) half_hourly_fluxes |&gt; select(TIMESTAMP_START, starts_with(&quot;SWC_F_MDS_&quot;)) |&gt; head() ## # A tibble: 6 × 9 ## TIMESTAMP_START SWC_F_MDS_1 SWC_F_MDS_2 SWC_F_MDS_3 SWC_F_MDS_4 ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2004-01-01 00:00:00 NA NA NA NA ## 2 2004-01-01 00:30:00 NA NA NA NA ## 3 2004-01-01 01:00:00 NA NA NA NA ## 4 2004-01-01 01:30:00 NA NA NA NA ## 5 2004-01-01 02:00:00 NA NA NA NA ## 6 2004-01-01 02:30:00 NA NA NA NA ## # ℹ 4 more variables: SWC_F_MDS_1_QC &lt;dbl&gt;, SWC_F_MDS_2_QC &lt;dbl&gt;, ## # SWC_F_MDS_3_QC &lt;dbl&gt;, SWC_F_MDS_4_QC &lt;dbl&gt; This lets us visualise the data and its gaps with vis_miss() from the {visdat} package. Visualising missing data can be informative for making decisions about dropping rows with missing data versus removing predictors from the analysis (which would imply too much data removal). visdat::vis_miss( half_hourly_fluxes |&gt; slice(1:10000), cluster = FALSE, warn_large_data = FALSE ) For many applications, we want to filter the data so that the values of particular variables satisfy certain conditions. The {dplyr} function used for such tasks is filter(). As argument, it takes the expressions that specify the criterion for filtering using logical operators (&gt;, &gt;=, &lt;, ==, !-, ..., see Chapter 1). Multiple filtering criteria can be combined with logical (boolean) operators: &amp;: logical AND |: logical OR ! logical NOT For example, if we wanted only those rows in our data where NEE is based on measured or good quality gap-filled NEE data, we write: half_hourly_fluxes |&gt; filter(NEE_VUT_REF_QC == 0 | NEE_VUT_REF_QC == 1) For evaluating multiple OR operations simultaneously, we can write alternatively and equivalently: half_hourly_fluxes |&gt; filter(NEE_VUT_REF_QC %in% c(0,1)) Note that filter() removes entire rows. In some cases this is undesired and it is preferred to replace bad quality values with NA. It is important to note that specifying a value as missing is information itself. Dropping an entire row leads to the loss of this information. For cases where we do not want to drop entire rows when applying filter(), we can just replace certain values with NA. In our case, where we want to retain only data where NEE is based on actual measurements or good quality gap-filling, we can do this by: half_hourly_fluxes |&gt; mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC %in% c(0,1), GPP_NT_VUT_REF, NA)) If we decide to drop a row containing NA in any of the variables later during the workflow, we can do this, for example using the useful {tidyr} function drop_na(). half_hourly_fluxes |&gt; drop_na() An excellent source for a more comprehensive introduction to missing data handling is given in Kuhn and Johnson (2019). 3.2.11 Writing data to CSV After having applied some data reduction and cleaning steps above, let’s save the data frame in the form of a CSV file for use in later chapters. write_csv(half_hourly_fluxes, file = &quot;data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006_CLEAN.csv&quot;) Note: Making a file publicly available as a .rds or .RData file (explained in Chapter 1) violates the open science principles (introduced in Chapter 6). These two formats make it very easy to save R objects related to your analysis project, but are not adequate to save data. Therefore, whenever possible, save your data in a format that is readable across platforms without requiring proprietary software. Hence use write_csv() from {readr} whenever possible. We will encounter other non-proprietary formats that let you save and share more complex data structures in Chapter 5. 3.2.12 Combining relational data Often, data are spread across multiple files and tables and need to be combined for the planned analysis. In the simplest case, data frames have an identical number of columns, arranged in the same order, and we can “stack” them along rows: co2_conc_subset_1 ## # A tibble: 6 × 4 ## month `1959` `1960` `1961` ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jan 315. 316. 317. ## 2 Feb 316. 317. 318. ## 3 Mar 316. 317. 318. ## 4 Apr 318. 319. 319. ## 5 May 318. 320. 320. ## 6 Jun 318 319. 320. co2_conc_subset_2 ## # A tibble: 6 × 4 ## month `1959` `1960` `1961` ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jul 316. 318. 318. ## 2 Aug 315. 316. 317. ## 3 Sep 314. 314 315. ## 4 Oct 313. 314. 315. ## 5 Nov 315. 315. 316. ## 6 Dec 315. 316. 317. bind_rows(co2_conc_subset_1, co2_conc_subset_2) ## # A tibble: 12 × 4 ## month `1959` `1960` `1961` ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jan 315. 316. 317. ## 2 Feb 316. 317. 318. ## 3 Mar 316. 317. 318. ## 4 Apr 318. 319. 319. ## 5 May 318. 320. 320. ## 6 Jun 318 319. 320. ## 7 Jul 316. 318. 318. ## 8 Aug 315. 316. 317. ## 9 Sep 314. 314 315. ## 10 Oct 313. 314. 315. ## 11 Nov 315. 315. 316. ## 12 Dec 315. 316. 317. In other cases, data frames may have an identical set of rows (and arranged in the same order) and we can “stack” them along columns. co2_conc_subset_3 ## # A tibble: 12 × 3 ## month `1959` `1960` ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jan 315. 316. ## 2 Feb 316. 317. ## 3 Mar 316. 317. ## 4 Apr 318. 319. ## 5 May 318. 320. ## 6 Jun 318 319. ## 7 Jul 316. 318. ## 8 Aug 315. 316. ## 9 Sep 314. 314 ## 10 Oct 313. 314. ## 11 Nov 315. 315. ## 12 Dec 315. 316. co2_conc_subset_4 ## # A tibble: 12 × 2 ## month `1961` ## &lt;ord&gt; &lt;dbl&gt; ## 1 Jan 317. ## 2 Feb 318. ## 3 Mar 318. ## 4 Apr 319. ## 5 May 320. ## 6 Jun 320. ## 7 Jul 318. ## 8 Aug 317. ## 9 Sep 315. ## 10 Oct 315. ## 11 Nov 316. ## 12 Dec 317. bind_cols(co2_conc_subset_3, co2_conc_subset_4) ## New names: ## • `month` -&gt; `month...1` ## • `month` -&gt; `month...4` ## # A tibble: 12 × 5 ## month...1 `1959` `1960` month...4 `1961` ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; &lt;dbl&gt; ## 1 Jan 315. 316. Jan 317. ## 2 Feb 316. 317. Feb 318. ## 3 Mar 316. 317. Mar 318. ## 4 Apr 318. 319. Apr 319. ## 5 May 318. 320. May 320. ## 6 Jun 318 319. Jun 320. ## 7 Jul 316. 318. Jul 318. ## 8 Aug 315. 316. Aug 317. ## 9 Sep 314. 314 Sep 315. ## 10 Oct 313. 314. Oct 315. ## 11 Nov 315. 315. Nov 316. ## 12 Dec 315. 316. Dec 317. But beware! In particular the stacking along columns (bind_cols()) is very error-prone and should be avoided. Since a tidy data frame regards each row as an instance of associated measurements, the rows of the two data frames and their order must match exactly. Otherwise, an error is raised or (even worse) rows get associated when they shouldn’t be. In such cases, where information about a common set of observations is distributed across multiple data objects, we are dealing with relational data. The key for their combination (or “merging”) is the join variable - the column that is present in both data frames and which contains values along which the merging of the two data frames is performed. In our example from above, this is month, and we can use the {dplyr} function left_join(). co2_conc_subset_3 |&gt; slice(sample(1:n(), replace = FALSE)) |&gt; # re-shuffling rows left_join(co2_conc_subset_4, by = &quot;month&quot;) |&gt; arrange(month) # sort in ascending order ## # A tibble: 12 × 4 ## month `1959` `1960` `1961` ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jan 315. 316. 317. ## 2 Feb 316. 317. 318. ## 3 Mar 316. 317. 318. ## 4 Apr 318. 319. 319. ## 5 May 318. 320. 320. ## 6 Jun 318 319. 320. ## 7 Jul 316. 318. 318. ## 8 Aug 315. 316. 317. ## 9 Sep 314. 314 315. ## 10 Oct 313. 314. 315. ## 11 Nov 315. 315. 316. ## 12 Dec 315. 316. 317. Note that here, we first re-shuffled (permuted) the rows of df6 for demonstration purposes, and arranged the output data frame again by month - an ordinal variable. left_join() is not compromised by the order of the rows, but instead relies on the join variable, specified by the argument by = \"month\", for associating (merging, joining) the two data frames. In some cases, multiple columns may act as the joining variables in their combination (for example by = c(\"year\", \"month\")). Other variants of *_join() are available as described here. 3.3 Extra material 3.3.1 Functional programming I Above, we read a CSV table into R and applied several data transformation steps. In practice, we often have to apply the same data transformation steps repeatedly over a set of similar objects. This extra material section outlines an example workflow for demonstrating how to efficiently work with lists of similar objects - in particular, lists of data frames. Our aim is to read a set of files into R data frames and apply transformation steps to each data frame separately. Here, we will work with daily data, not half-hourly data. The daily data contains largely identical variables with consistent naming and units as in the half-hourly data (description above). Let’s start by creating a list of paths that point to the files with daily data. They are all located in the directory \"./data\" and share a certain string of characters in their file names \"_FLUXNET2015_FULLSET_DD_\". vec_files &lt;- list.files(&quot;./data&quot;, pattern = &quot;_FLUXNET2015_FULLSET_DD_&quot;, full.names = TRUE) print(vec_files) ## [1] &quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot; ## [2] &quot;./data/FLX_CH-Lae_FLUXNET2015_FULLSET_DD_2004-2014_1-4.csv&quot; ## [3] &quot;./data/FLX_FI-Hyy_FLUXNET2015_FULLSET_DD_1996-2014_1-3.csv&quot; ## [4] &quot;./data/FLX_FR-Pue_FLUXNET2015_FULLSET_DD_2000-2014_2-3.csv&quot; To reproduce this code chunk, you can download the files FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv from here and read it from the local path where the file is stored on your machine. vec_files is now a vector of three files paths as character strings. To read in the three files and combine the three data frames (list_df below) into a list of data frames, we could use a for loop: list_df &lt;- list() for (ifil in vec_files){ list_df[[ifil]] &lt;- read_csv(ifil) } Repeatedly applying a function (here read_csv()) over a list similar objects is facilitated by the map*() family of functions from the {purrr} package. An (almost) equivalent statement is: list_df &lt;- purrr::map(as.list(vec_files), ~read_csv(.)) Here, purrr::map() applies the function read_csv() to elements of a list. Hence, we first have to convert the vector vec_files to a list. A list is always the first argument within the purrr::map() function. Note two new symbols (~ and .). The ~ always goes before the function that is repeatedly applied (or “mapped”) to elements of the list. The . indicates where the elements of the list would go if spelled out (e.g., here, read_csv(.) would be read_csv(\"./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv\") for the first iteration). The output of purrr::map() is again a list. There are many variants of the function purrr::map() that each have a specific use. A complete reference for all {purrr} functions is available here. A useful and more extensive tutorial on {purrr} is available here. The above purrr::map() call does not return a named list as our for loop created. But we can give each element of the returned list of data frames different names by: names(list_df) &lt;- vec_files # this makes it a named list Next, we will apply a similar data cleaning procedure to this data set as we did above for half-hourly data. To do so, we “package” the individual cleaning steps into a function … # function definition clean_data_dd &lt;- function(df){ df &lt;- df |&gt; # select only the variables we are interested in dplyr::select( TIMESTAMP, ends_with(&quot;_F&quot;), GPP_NT_VUT_REF, NEE_VUT_REF_QC, starts_with(&quot;SWC_F_MDS_&quot;), -contains(&quot;JSB&quot;)) |&gt; # convert to a nice date object dplyr::mutate(TIMESTAMP = lubridate::ymd(TIMESTAMP)) |&gt; # set all -9999 to NA dplyr::mutate(across(where(is.numeric), ~na_if(., -9999))) return(df) } … and apply this function to each data frame within our list of data frames: list_df &lt;- purrr::map(list_df, ~clean_data_dd(.)) Having different data frames as elements of a list may be impractical. Since we read in similarly formatted files and selected always the same variables in each data frame, all elements of the list of data frames list_df share the same columns. This suggests that we can collapse our list of data frames and “stack” data frames along rows. As described above, this can be done using bind_rows() and we can automatically create a new column \"siteid\" in the stacked data frame that takes the name of the corresponding list element. daily_fluxes_allsites &lt;- bind_rows(list_df, .id = &quot;siteid&quot;) daily_fluxes_allsites ## # A tibble: 23,011 × 21 ## siteid TIMESTAMP TA_F SW_IN_F LW_IN_F VPD_F PA_F P_F WS_F ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ./data/FLX_CH-Dav_… 1997-01-01 -4.57 77.4 223. 0.565 82.6 0.4 0.559 ## 2 ./data/FLX_CH-Dav_… 1997-01-02 -3.34 45.6 235. 0.978 82.9 0 1.11 ## 3 ./data/FLX_CH-Dav_… 1997-01-03 0.278 74.1 239. 2.24 82.4 0 2.03 ## 4 ./data/FLX_CH-Dav_… 1997-01-04 -1.88 58.1 250. 1.38 81.7 1.8 1.92 ## 5 ./data/FLX_CH-Dav_… 1997-01-05 -4.96 80.8 248. 1.16 82.3 0 0.407 ## 6 ./data/FLX_CH-Dav_… 1997-01-06 -4.48 59.6 237. 0.838 82.7 0 0.466 ## 7 ./data/FLX_CH-Dav_… 1997-01-07 -3.15 45.5 234. 1.33 82.9 0 1.03 ## 8 ./data/FLX_CH-Dav_… 1997-01-08 -2.45 76.7 222. 1.87 82.7 0 1.95 ## 9 ./data/FLX_CH-Dav_… 1997-01-09 -2.43 47.6 251. 1.44 82.2 0 0.785 ## 10 ./data/FLX_CH-Dav_… 1997-01-10 -3.09 39.6 242. 0.776 82.8 0 1.25 ## # ℹ 23,001 more rows ## # ℹ 12 more variables: GPP_NT_VUT_REF &lt;dbl&gt;, NEE_VUT_REF_QC &lt;dbl&gt;, ## # SWC_F_MDS_1 &lt;dbl&gt;, SWC_F_MDS_2 &lt;dbl&gt;, SWC_F_MDS_3 &lt;dbl&gt;, ## # SWC_F_MDS_1_QC &lt;dbl&gt;, SWC_F_MDS_2_QC &lt;dbl&gt;, SWC_F_MDS_3_QC &lt;dbl&gt;, ## # SWC_F_MDS_4 &lt;dbl&gt;, SWC_F_MDS_4_QC &lt;dbl&gt;, SWC_F_MDS_5 &lt;dbl&gt;, ## # SWC_F_MDS_5_QC &lt;dbl&gt; A visualisation of missing data indicates that soil water content data (SWC_F_MDS_*) are often missing. # create a subset of the data daily_fluxex_subset &lt;- daily_fluxes_allsites |&gt; slice(1:10000) # visualize missing data visdat::vis_miss( daily_fluxex_subset, cluster = FALSE, warn_large_data = FALSE ) 3.3.2 Strings The column siteid currently contains strings specifying the full paths of the files that were read in earlier. The next task is to extract the site name from these strings. The file names follow a clear pattern (this also highlights why naming files wisely can often make life a lot simpler). daily_fluxes_allsites$siteid |&gt; head() ## [1] &quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot; ## [2] &quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot; ## [3] &quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot; ## [4] &quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot; ## [5] &quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot; ## [6] &quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot; The paths each start with the subdirectory where they are located (\"./data/\"), then \"FLX_\", followed by the site name (the first three entries of the table containing data from all sites are for the site \"CH-Dav\"), and then some more specifications, including the years that respective files’ data cover. The {stringr} package (part of tidyverse) offers a set of functions for working with strings. Wikham and Grolemund (2017) provide a more comprehensive introduction to working with strings. Here, we would like to extract the six characters, starting at position 12. The function str_sub() does that job. vec_sites &lt;- str_sub(vec_files, start = 12, end = 17) head(vec_sites) ## [1] &quot;CH-Dav&quot; &quot;CH-Lae&quot; &quot;FI-Hyy&quot; &quot;FR-Pue&quot; We can use this function to mutate all values of column \"siteid\", overwriting it with just these six characters. daily_fluxes_allsites &lt;- daily_fluxes_allsites |&gt; mutate(siteid = str_sub(siteid, start = 12, end = 17)) daily_fluxes_allsites ## # A tibble: 23,011 × 21 ## siteid TIMESTAMP TA_F SW_IN_F LW_IN_F VPD_F PA_F P_F WS_F ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 CH-Dav 1997-01-01 -4.57 77.4 223. 0.565 82.6 0.4 0.559 ## 2 CH-Dav 1997-01-02 -3.34 45.6 235. 0.978 82.9 0 1.11 ## 3 CH-Dav 1997-01-03 0.278 74.1 239. 2.24 82.4 0 2.03 ## 4 CH-Dav 1997-01-04 -1.88 58.1 250. 1.38 81.7 1.8 1.92 ## 5 CH-Dav 1997-01-05 -4.96 80.8 248. 1.16 82.3 0 0.407 ## 6 CH-Dav 1997-01-06 -4.48 59.6 237. 0.838 82.7 0 0.466 ## 7 CH-Dav 1997-01-07 -3.15 45.5 234. 1.33 82.9 0 1.03 ## 8 CH-Dav 1997-01-08 -2.45 76.7 222. 1.87 82.7 0 1.95 ## 9 CH-Dav 1997-01-09 -2.43 47.6 251. 1.44 82.2 0 0.785 ## 10 CH-Dav 1997-01-10 -3.09 39.6 242. 0.776 82.8 0 1.25 ## # ℹ 23,001 more rows ## # ℹ 12 more variables: GPP_NT_VUT_REF &lt;dbl&gt;, NEE_VUT_REF_QC &lt;dbl&gt;, ## # SWC_F_MDS_1 &lt;dbl&gt;, SWC_F_MDS_2 &lt;dbl&gt;, SWC_F_MDS_3 &lt;dbl&gt;, ## # SWC_F_MDS_1_QC &lt;dbl&gt;, SWC_F_MDS_2_QC &lt;dbl&gt;, SWC_F_MDS_3_QC &lt;dbl&gt;, ## # SWC_F_MDS_4 &lt;dbl&gt;, SWC_F_MDS_4_QC &lt;dbl&gt;, SWC_F_MDS_5 &lt;dbl&gt;, ## # SWC_F_MDS_5_QC &lt;dbl&gt; 3.3.3 Functional programming II Functions can be applied to a list of objects of any type. Therefore, purrr::map() is a powerful approach to “iterating” over multiple instances of the same object type and can be used for all sorts of tasks. In the following, list elements are data frames of daily data and the function lm() fits a linear regression model of GPP versus shortwave radiation to each sites’ data. We’ll learn more about fitting statistical models in R in Chapter 8. list_linmod &lt;- purrr::map(list_df, ~lm(GPP_NT_VUT_REF ~ SW_IN_F, data = .)) Note how the . indicates where the elements of list_df go when evaluating the lm() function. This returns a list of linear model objects (the type of objects returned by the lm() function call). We can spin the functional programming concept further and apply (or map) the summary() function to the lm-model objects to get a list of useful statistics and metrics, and then further extract the element \"r.squared\" from that list as: list_linmod |&gt; purrr::map(summary) |&gt; # apply the summary() function to each list element map_dbl(&quot;r.squared&quot;) # extract R-squared from the list generated by summary() ## ./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv ## 0.4201802 ## ./data/FLX_CH-Lae_FLUXNET2015_FULLSET_DD_2004-2014_1-4.csv ## 0.5074248 ## ./data/FLX_FI-Hyy_FLUXNET2015_FULLSET_DD_1996-2014_1-3.csv ## 0.6415685 ## ./data/FLX_FR-Pue_FLUXNET2015_FULLSET_DD_2000-2014_2-3.csv ## 0.3772839 map_dbl() is a variant of the purrr::map() function that returns not a list, but a vector of numeric values of class “double” (hence, the name _dbl). Note further, that providing a character (\"r.squared\") as an argument instead of an (unquoted) function name, purrr::map() extracts the correspondingly named list element, instead of applying a function to a list element. When writing code for an analysis, it’s useful, if not essential, to understand the objects we’re working with, understand its type and shape, and make sense of the results of simple print &lt;object&gt; statements. Data frames are particularly handy as they provide an organisation of data that is particularly intuitive (variables along columns, observations along rows, values in cells). Here, we’re dealing with a list of linear model objects. Can such a list fit into the paradigm of tidy data frames? Yes, they can. Think of the linear model objects as ‘values’. Values don’t necessarily have to be scalars, but they can be of any type (class). tibble::tibble( siteid = vec_sites, linmod = list_linmod ) ## # A tibble: 4 × 2 ## siteid linmod ## &lt;chr&gt; &lt;named list&gt; ## 1 CH-Dav &lt;lm&gt; ## 2 CH-Lae &lt;lm&gt; ## 3 FI-Hyy &lt;lm&gt; ## 4 FR-Pue &lt;lm&gt; The fact that cells can contain any type of object offers a powerful concept. Instead of a linear model object as in the example above, each cell may even contain another data frame. In such a case, we say that the data frame is no longer flat, but nested. The following creates a nested data frame, where the column data is defined by the list of data frames read from files above (list_df). tibble::tibble( siteid = vec_sites, data = list_df ) ## # A tibble: 4 × 2 ## siteid data ## &lt;chr&gt; &lt;named list&gt; ## 1 CH-Dav &lt;tibble [6,574 × 16]&gt; ## 2 CH-Lae &lt;tibble [4,018 × 18]&gt; ## 3 FI-Hyy &lt;tibble [6,940 × 20]&gt; ## 4 FR-Pue &lt;tibble [5,479 × 10]&gt; We can achieve the same result by directly nesting the flat data frame holding all sites’ data (daily_fluxes_allsites). This is done by combining the group_by(), which we have encountered above when aggregating using summarise(), with the function nest() from the {tidyr} package. daily_fluxes_allsites |&gt; group_by(siteid) |&gt; nest() ## # A tibble: 4 × 2 ## # Groups: siteid [4] ## siteid data ## &lt;chr&gt; &lt;list&gt; ## 1 CH-Dav &lt;tibble [6,574 × 20]&gt; ## 2 CH-Lae &lt;tibble [4,018 × 20]&gt; ## 3 FI-Hyy &lt;tibble [6,940 × 20]&gt; ## 4 FR-Pue &lt;tibble [5,479 × 20]&gt; The function nest() names the nested data column automatically \"data\". This structure is very useful. For example, for applying functions over sites’ data frames separately (and not over the entire data frame). By combining purrr::map() and mutate(), we can fit linear models on each site’s data frame individually in one go. daily_fluxes_allsites |&gt; group_by(siteid) |&gt; nest() |&gt; dplyr::mutate(linmod = purrr::map(data, ~lm(GPP_NT_VUT_REF ~ SW_IN_F, data = .))) This approach is extremely powerful and lets you stick to working with tidy data frames and use the rows-dimension flexibly. Here, rows are sites and no longer time steps, while the nested data frames in column \"data\" have time steps along their rows. The power of nesting is also to facilitate complex aggregation steps over a specified dimension (or axis of variation, here given by siteid), where the aggregating function is not limited to taking a vector as input and returning a scalar, as is the case for applications of summarise() (see above). Combining the steps described above into a single workflow, we have: daily_fluxes_allsites_nested &lt;- daily_fluxes_allsites |&gt; group_by(siteid) |&gt; nest() |&gt; dplyr::mutate(linmod = purrr::map(data, ~lm(GPP_NT_VUT_REF ~ SW_IN_F, data = .))) |&gt; dplyr::mutate(summ = purrr::map(linmod, ~summary(.))) |&gt; dplyr::mutate(rsq = map_dbl(summ, &quot;r.squared&quot;)) |&gt; arrange(desc(rsq)) # to arrange output, with highest r-squared on top daily_fluxes_allsites_nested ## # A tibble: 4 × 5 ## # Groups: siteid [4] ## siteid data linmod summ rsq ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;dbl&gt; ## 1 FI-Hyy &lt;tibble [6,940 × 20]&gt; &lt;lm&gt; &lt;smmry.lm&gt; 0.642 ## 2 CH-Lae &lt;tibble [4,018 × 20]&gt; &lt;lm&gt; &lt;smmry.lm&gt; 0.507 ## 3 CH-Dav &lt;tibble [6,574 × 20]&gt; &lt;lm&gt; &lt;smmry.lm&gt; 0.420 ## 4 FR-Pue &lt;tibble [5,479 × 20]&gt; &lt;lm&gt; &lt;smmry.lm&gt; 0.377 This code is a demonstration of the power of tidy and nested data frames and for the clarity of the {tidyverse} syntax. Nesting is useful also for avoiding value duplication when joining relational data objects. Above, we nested time series data objects (where time steps and sites are both organised along rows) by sites and got a data frame where only sites are organised along rows, while time steps are nested inside the column \"data\". This now fits the structure of a relational data object (siteinfo_fluxnet2015) containing site-specific meta information (also with only sites along rows). base::load(&quot;data/siteinfo_fluxnet2015.rda&quot;) # loads siteinfo_fluxnet2015 Joining the nested data frame with site meta information results in a substantially smaller and much handier data frame compared to an alternative, where the site meta information is joined into the un-nested (daily) data frame, and therefore duplicated for each day within sites. daily_fluxes_allsites_nested_joined &lt;- siteinfo_fluxnet2015 |&gt; rename(siteid = sitename) |&gt; right_join( select(daily_fluxes_allsites_nested, -linmod, -summ, -rsq), by = &quot;siteid&quot; ) daily_fluxes_allsites_joined &lt;- siteinfo_fluxnet2015 |&gt; rename(siteid = sitename) |&gt; right_join( daily_fluxes_allsites, by = &quot;siteid&quot; ) print(paste(&quot;Flat and joined:&quot;, format(object.size(daily_fluxes_allsites_joined), units = &quot;auto&quot;, standard = &quot;SI&quot;))) ## [1] &quot;Flat and joined: 5.8 MB&quot; print(paste(&quot;Nested and joined:&quot;, format(object.size(daily_fluxes_allsites_nested_joined), units = &quot;auto&quot;, standard = &quot;SI&quot;))) ## [1] &quot;Nested and joined: 3.7 MB&quot; # save for later use write_rds( daily_fluxes_allsites_nested_joined, file = &quot;data/daily_fluxes_allsites_nested_joined.rds&quot; ) 3.4 Exercises Hint: For all exercises remember the resources we provided on finding help in section 2.2.3. Star wars {dplyr} comes with a toy dataset dplyr::starwars (just type it into the console to see its content). Have a look at the dataset with View(). Play around with the dataset to get familiar with the {tidyverse} coding style. Use (possibly among others) the functions dplyr::filter(), dplyr::arrange(), dplyr::pull(), dplyr::select(), dplyr::desc() and dplyr::slice() to answer the following questions: How many pale characters come from the planets Ryloth and Naboo? Who is the oldest among the tallest thirty characters? What is the name of the smallest character and their starship in “Return of the Jedi” Hint: Use unnest() to expand columns that contain lists inside cells. The expansion of such columns creates additional rows in the data frame if the cell contained a list with more than one element. Aggregating You have learned about aggregating in the {tidyverse}. Let’s put it in practice. Reuse the code in the tutorial to read, reduce, and aggregate the half_hourly_fluxes dataset to the daily scale, calculating the following metrics across half-hourly VPD_F values within each day: mean, 25% quantile, and 75% quantile. Retain only the daily data for which the daily mean VPD is among the upper or the lower 10% quantiles. Calculate the mean of the 25% and the mean of the 75% quantiles of half-hourly VPD within the upper and lower 10% quantiles of mean daily VPD. Patterns in data quality The uncleaned dataset FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv holds half-hourly data that is sometimes of poor quality. Investigate whether NEE data quality is randomly spread across hours in a day by calculating the proportion of (i) actually measured data, (ii) good quality gap-filled data, (iii) medium quality data, and (iv) poor quality data within each hour-of-day (24 hours per day). Hint: summarise(total = n()) aggregates by counting the number of values. Hint: summarise(count_0 = sum(x == 0)) aggregates by counting the number of values for which the evaluation is TRUE. Interpret your findings: Are the proportions evenly spread across hours in a day? Perform an aggregation of the half-hourly GPP data (variable GPP_NT_VUT_REF) to daily means of the unmodified data read from file FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv, and from cleaned data where only measured (not gap-filled) data is kept. This yields two data frames with daily GPP data. Calculate the overall mean GPP for the two data frames (across all days in the data frame). Are the overall mean GPP values equal? If not, why? 3.5 Report Exercise Analyzing changes in soil organic matter during elevated CO\\(_2\\) experiments Open Science requires that data underlying published research articles is made available upon publication of the article. A separate aspect is the format of the shared data. Is it provided in an open-access data format? How easy is it to use the data for your own analyses? In this exercise, you will encounter data that was made freely available, but not in an open access format. Although it is “nice-looking” data, you will encounter that it is not tidy. In this exercise, you will investigate the data published by Groeningen et al. (2014), where they looked at how soil carbon stocks may change due to the anthropogenic increase in \\(CO_2\\). They gathered data on changes in the soil organic matter content from experiments where ecosystems were exposed to elevated CO\\(_2\\) concentrations and your task is to have a high-level look at this dataset. So, perform the following steps: Download the data file (.xlsx) from the Supplementary Material of the following paper: Groenigen, Kees Jan van, Xuan Qi, Craig W. Osenberg, Yiqi Luo, and Bruce A. Hungate. “Faster Decomposition Under Increased Atmospheric CO2 Limits Soil Carbon Storage.” Science 344, no. 6183 (May 2, 2014): 508–9. https://doi.org/10.1126/science.1249534. Manually clean the data in the tab “Database S1” and save it as a CSV file that can be read into R. “Database S1” contains data of soil organic carbon measurements in experiments, where ecosystems are exposed to ambient (low) and elevated (high) CO\\(_2\\) concentrations. The mean soil organic carbon of multiple samples (“n”) is recorded within each experiment for different sample dates. Information is provided for the time in years since the start of the experiment (“Time (years)”). In RStudio, create RMarkdown file. Then, write your code into the R chunks of the file to aggregate the data per experiment and calculate the log-response ratio within each experiment, as specified below. A log-response ratio can be used to quantify the effect that a treatment (e.g., elevated CO\\(_2\\)) can have on your target variable \\(x\\) (e.g., soil organic matter content). The log-response ratio can be calculated as: \\(\\text{RR} = \\ln \\left( \\frac{x_\\text{elevated}}{x_\\text{ambient}} \\right)\\) Aggregate data across all experiments for different years since the start of the experiment, distinguishing an early phase (&lt;3 years since start), a mid-phase (3-6 years since start), and a late phase (&gt;6 years since start). Calculate the log-response ratio for each phase. Calculate the log-response ratio for each parallel observation of SOC under ambient and elevated CO\\(_2\\), and then aggregate log response ratios by taking their mean (and not the other way round). Present your results as tables using the knitr::kable() function. Tip: Depending on your Excel settings, your exported csv is separated using , or ;. The read_csv() function only works with ,. So, if your file is separated with ;, either change the export settings or use read_csv2().Where would it have been useful if more information had been available. Answer the following questions: What are the data that you are looking at? What do you expect your analysis to show, what is your hypothesis? How should soil organic matter content change under elevated CO\\(_2\\)? Interpret your results after aggregating the data: What do your final numbers mean? Do they support your initial hypothesis? Why so, why not? Tip: Skim the paper and its references to better understand the data and processes you are looking at!`` Deliverables for the report A key learning of this course is that you know how to create reproducible workflows and we pay a lot of attention on this during the assessment. You will learn more about the specific aspects to do so in Chapters 6 and 7. So, if the following instructions for submitting this report exercise may sound cryptic, hang tight! It will make more sense as you work through this course. You have to submit all report exercises via your GitHub repository that you created in 1.2.8.4. For now, save this report exercise as a RMarkdown file with the name re_tidy.Rmd and place that file in a sub-directory called vignettes (a directory is simply a folder), which is located in your project directory (in full notation that means save your RMarkdown as ./vignettes/re_tidy.Rmd). Additionally, produce a HTML version of your solution by knitting your RMarkdown file (see the Knit command in the panel below the files tabs in RStudio). Save this HTML file in the same vignettes directory (./vignettes/re_tidy.html). For your cleaned dataset, pick a sensible name and put it into a sub-direcory called ./data. As with all other report exercises, make sure that your work is reproducible, for example, by letting another student download your repository from GitHub and knit your RMarkdown files on their computer. Important: Structure your RMarkdown so that you first load necessary libraries, load your data and then write down your solution. Remember to access files with relative paths and not with hard-coded paths. So, access your file via here::here('data/tidy_data.csv), where here starts at the position of your .Rproj file. Never write hard-coded paths that looks like this: \"~/Desktop/Studium/.../tidy_data.csv\" or \"C:/Users/.../tidy_data.csv\" (see Chapter 1.2.7). Tip: Your results do not have to match the results from the paper. It is important that you make your code reproducible, aggregate the data as instructed, and interpret your results. Tip: If you are new to using RMarkdown, check out this guide. "],["datavis.html", "Chapter 4 Data visualisation 4.1 Learning objectives 4.2 Tutorial 4.3 Exercises 4.4 Report Exercises", " Chapter 4 Data visualisation Chapter lead author: Benjamin Stocker 4.1 Learning objectives In this chapter you will learn how to visualize data for exploration or publication. You will learn among others the: appropriate choice of visualisations for different data types and different aspects of the data grammar of graphics, i.e., using the {ggplot2} library the proper use of colours in visualization 4.2 Tutorial Visualizations often take the center stage of publications and are often the main vehicles for transporting information in scientific publications and (ever more often) in the media. Visualizations communicate data and its patterns in visual form. Visualizing data is also an integral part of the exploratory data analysis cycle. Visually understanding the data guides its transformation and the identification of suitable models and analysis methods. The quality of a data visualization can be measured by its effectiveness of conveying information about the data and thus of answering a question with the data and telling a story. Different aspects determine this effectiveness, including the appropriateness of visualization elements, the intuitiveness of how information can be decoded from the visualization by the reader, the visual clarity and legibility (taking into account the vision and potential vision deficiencies of the reader), the visual appeal, etc. This tutorial introduces data visualization under the premise that not all aspects of data visualization are a matter of taste. There are appropriate and less appropriate ways of encoding data in visual form. This tutorial is inspired by the comprehensive and online available textbook Fundamentals of Data Visualization by Claus O. Wilke. Another excellent resource is the Chapter Data Visualisation in R for Data Science by Hadley Wickham 4.2.1 The grammar of graphics In Chapter 3, we learned about axes of variation in the data. For example, time is an axis of variation in our example data half_hourly_fluxes, or site identity and the date are axes of variation in our example data daily_fluxes. We have also learned that we can aggregate over axes of variation, and that we can often separate an axis of variation into a hierarchy of subordinate axes of variation (e.g., years, months, days, and a half-hourly time axis). In this chapter, we will be working mainly with the same half-hourly time series data of ecosystem-atmosphere fluxes and parallel measurements of meteorological variables - as in Chapters 1 and 3. For time series data, the entry point of the exploratory data analysis cycle may be a visualization of some variable of interest (here GPP_NT_VUT_REF) against time: half_hourly_fluxes &lt;- readr::read_csv(&quot;data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006_CLEAN.csv&quot;) ## Rows: 52608 Columns: 20 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (18): TA_F, SW_IN_F, LW_IN_F, VPD_F, PA_F, P_F, WS_F, GPP_NT_VUT_REF, N... ## dttm (2): TIMESTAMP_START, TIMESTAMP_END ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. plot(half_hourly_fluxes$TIMESTAMP_START, half_hourly_fluxes$GPP_NT_VUT_REF, type = &quot;l&quot;) To reproduce this code chunk, you can download the file FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006_CLEAN from here and read it from the local path where the file is stored on your machine. All data files used in this tutorials are stored here. You may notice the spurious-looking values on the left, in the first third of year 2004. This demonstrates the role of visualisation in understanding the data and its quality. We’ll revisit this point later in this Chapter. From (Wilke): “All data visualizations map data values into quantifiable features of the resulting graphic. We refer to these features as aesthetics.” Applied to our example, the aesthetics are the x-axis and the y-axis of a cartesian coordinate system. TIMESTAMP_START is mapped onto the x-axis, GPP_NT_VUT_REF is mapped onto the y-axis, and their respective values specify the position of points in the cartesian coordinate system that are then connected with lines - making up the geometrical object that represents the data. Often, the aesthetic that is used to plot the target variable against corresponds to a known axis of variation in the data. The notion of mapping data onto aesthetics and using objects whose geometry is defined by the aesthetics gives rise to the grammar of graphics and to the {ggplot2} R package for data visualisation (which we will use throughout the remainder of this course). The equivalent {ggplot2} code that follows the philosophy of the grammar of graphics is: ggplot(data = half_hourly_fluxes, aes(x = TIMESTAMP_START, y = GPP_NT_VUT_REF)) + geom_line() The argument provided by the aes() statement specifies the aesthetics (x, and y) and which variables in data are mapped onto them. Once this is specified, we can use any suitable geometrical object that is defined by these aesthetics. Here, we used a line plot specified by geom_line(). Note also that the different objects to creating a ggplot graph are connected with a +, which takes a similar role as the pipe (|&gt;). The data visualisation above is a dense plot and we cannot distinguish patterns because variations in GPP happen at time scales that are too narrow for displaying three years of half-hourly data in one plot. GPP varies throughout a day just as much as it varies throughout a season. To see this, we can focus on a narrower time span (selecting rows by index using dplyr::slice() in the code below). Visual clarity is also facilitated by an appropriate labeling (title and axes labels using labs()) and by a reduction of displayed elements to a minimum (therefore, the changing of the formatting theme by theme_classic()): # prepare plot data plot_data &lt;- half_hourly_fluxes |&gt; dplyr::slice(24000:25000) # plot figure plotme &lt;- ggplot( data = plot_data, aes(x = TIMESTAMP_START, y = GPP_NT_VUT_REF)) + geom_line() + labs(title = &quot;Gross primary productivity&quot;, subtitle = &quot;Site: CH-Lae&quot;, x = &quot;Time&quot;, y = expression(paste(&quot;GPP (&quot;, mu,&quot;mol CO&quot;[2], &quot; m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() plotme Figure 4.1: An example time series created with {ggplot2}. Find a complete reference to {ggplot2} here. The grammar of graphics has found its way also into Python and you can use ggplot using the plotnine Python package (see here). 4.2.2 Every data has its representation In the above example, we mapped two continuous variables (TIMESTAMP_START and GPP_NT_VUT_REF) onto the aesthetics x, and y to visualize time series data. A line plot is an appropriate choice for such data as points are ordered along the time axis and can be connected by a line. Different “geometries” are suitable for visualizing different aspects of the data, and different variable types are suited to mapping onto different aesthetics. Common, available aesthetics are shown in Fig. 4.2 and can be allocated to variable types: Continuous variables: position, size, color (a color gradient), line width, etc. Categorical variables: shape, color (a discrete set of colors), line type, etc. Figure 4.2: Common aesthetics to display different variable types. Figure from Wilke. Not only the different aesthetics, but also the type of geometry (the layers of the visualization added to a plot by + geom_*()) goes with certain types of variables and aspects of the data (but not with others). The sub-sections below provide a brief categorization of data visualization types. A more comprehensive overview is given by GGPlot2 Essentials for Great Data Visualization in R by Alboukadel Kassambara. 4.2.2.1 One value per category Probably the simplest case of data visualization is where a single value is shown across a categorical variable. This calls for a bar plot (geom_bar()). In the example below, we plot the mean GPP for each month. The “custom plot” shown below is a demonstration for what you can do by combining different elements with {ggplot2}. Try to understand the command for creating the object plot_2. Both examples are based on the data frame daily_fluxes which we created in Chapter 3. # read in demo daily data # as saved in the previous chapter daily_fluxes &lt;- read_csv(&quot;data/daily_fluxes.csv&quot;) ## Rows: 1096 Columns: 6 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (5): GPP_NT_VUT_REF, n_datapoints, n_measured, SW_IN_F, f_measured ## date (1): date ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Aggregate to monthly mdf &lt;- daily_fluxes |&gt; dplyr::mutate(month = month(date, label = TRUE)) |&gt; dplyr::group_by(month) |&gt; dplyr::summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF)) # Bar plot plot_1 &lt;- ggplot( data = mdf, aes(x = month, y = GPP_NT_VUT_REF)) + geom_bar(stat = &quot;identity&quot;) + theme_classic() + labs(title = &quot;Bar plot&quot;, x = &quot;Month&quot;, y =expression(paste(&quot;GPP (&quot;, mu,&quot;mol CO&quot;[2], &quot; m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) # Custom plot plot_2 &lt;- ggplot( data = mdf, aes(x = month, y = GPP_NT_VUT_REF)) + geom_segment(aes(x = month, xend = month, y = 0, yend = GPP_NT_VUT_REF), size = 3, color = &quot;grey40&quot;) + geom_point(aes(x = month, y = GPP_NT_VUT_REF), size = 8, color = &quot;tomato&quot;) + geom_text(aes(x = month, y = GPP_NT_VUT_REF, label = format(GPP_NT_VUT_REF, digits = 2)), size = 3, color = &quot;white&quot;) + theme_classic() + labs(title = &quot;Custom plot&quot;, x = &quot;Month&quot;, y = expression(paste(&quot;GPP (&quot;, mu,&quot;mol CO&quot;[2], &quot; m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + scale_y_continuous(limits = c(0, 8.75), expand = c(0, 0)) + coord_flip() # combine plots cowplot::plot_grid(plot_1, plot_2) Above, we created two objects, plot_1 and plot_2, that contain the instructions for creating the plots. To combine multiple sub-plots within panels of a single plot, we used cowplot::plot_grid() from the {cowplot} library. Note also the stat = \"identity\" specification within the geom_bar() function call. This is required when the bar height is specified by a single value within each category (month in the example above). To visualize not a value per se but the count of values within categories, use stat = \"count\" to get the equivalent result as when aggregating by taking the number of observations within categories explicitly using the function dplyr::summarise(). This equivalency is demonstrated below. # subset plot data and count occurences of bad data (NEE_VUT_REF_QC == 0) df_count &lt;- half_hourly_fluxes |&gt; dplyr::filter(NEE_VUT_REF_QC == 0) |&gt; dplyr::group_by(NIGHT) |&gt; dplyr::summarise(count = n()) # separate aggregation plot_1 &lt;-ggplot( data = df_count, aes(x = NIGHT, y = count)) + geom_bar(stat = &quot;identity&quot;) + labs(subtitle = &quot;Count via &#39;summarise&#39; and &#39;stat = identiy&#39;&quot;) + theme_classic() # prepare data (not summarizing counts) half_hourly_fluxes_bad &lt;- half_hourly_fluxes |&gt; dplyr::filter(NEE_VUT_REF_QC == 0) # implicit aggregation by &#39;stat&#39; plot_2 &lt;- ggplot( data = half_hourly_fluxes_bad, aes(x = NIGHT)) + geom_bar(stat = &quot;count&quot;) + labs(subtitle = &quot;Count directly via &#39;stat = count&#39;&quot;) + theme_classic() # combine plots cowplot::plot_grid(plot_1, plot_2) 4.2.2.2 Distribution of one variable Examining the distribution of a variable is often the first step of exploratory data analysis. A histogram displays the distribution of numerical data by mapping the frequency (or count) of values within discrete bins (equally spaced ranges along the full range values of a given variable) onto the “height” of a bar, and the range of values within bins onto the position of the bar. In other words, it shows the count of how many points of a certain variable (below GPP_NT_VUT_REF) fall into a discrete set of bins. When normalizing (scaling) the “bars” of the histogram to unity, we get a density histogram. To specify the y-axis position of the upper end of the histogram bar as the density, use y = ..density.. in the aes() call. To show counts, use y = ..count... ggplot( data = half_hourly_fluxes, aes(x = GPP_NT_VUT_REF, y = ..density..) ) + geom_histogram(fill = &quot;grey70&quot;, color = &quot;black&quot;) + geom_density(color = &quot;red&quot;) + # we can overlay multiple plot layers! labs(title = &quot;Histogram and density&quot;, x = expression(paste(&quot;GPP (&quot;, mu,&quot;mol CO&quot;[2], &quot; m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Note that the red line plotted by geom_density() on top of the density histogram visualises the density distribution in continuous (not discrete, or binned) form. Note also that both “geoms” share the same aesthetics with aes() specified in the ggplot() function call. 4.2.2.3 Distributions within categories To visualize distributions of a single continuous variable within categories, perhaps the most common visualization type is the box plot. As described in Chapter 3, it shows the median (bold line in the center), the upper and lower quartiles, corresponding to the 25% and the 75% quantiles, often referred to as \\(Q_1\\) and \\(Q_3\\) , and given by the upper and lower edge of the box plot. The lines extending from the box edges visualize the range of \\(( Q_1 - 1.5 (Q_3 - Q_1)\\) to \\(Q_3 + 1.5 (Q_3 - Q_1)\\). Any point outside this range is plotted by a point. The box plot is rather reductionist in showing the data (the vector of all values is reduced to the median, \\(Q_1\\) , \\(Q_3\\), and outlying points) and may yield a distorted picture of the data distribution and does not reflect information about the data volume. For this reason, several journals are now requiring individual data points or at least the number of data points to be shown in addition to each box. Below, points are added by geom_jitter() , where points are “jittered”, that is, randomly spread out along the x-axis. Violin plots are a hybrid of a density plot and a box plot. The shape of their edge is given by the density distribution of the points they represent. # prepare plot data set.seed(1985) # for random number reproducibility in sample_n() and jitter half_hourly_fluxes_subset &lt;- half_hourly_fluxes |&gt; sample_n(300) |&gt; mutate(Night = ifelse(NIGHT == 1, TRUE, FALSE)) # Boxplot plot_1 &lt;- ggplot( data = half_hourly_fluxes_subset, aes(x = Night, y = VPD_F)) + geom_boxplot(fill = &quot;grey70&quot;) + labs(title = &quot;Boxplot&quot;) + labs(y = &quot;VPD (hPa)&quot;) + theme_classic() # Box plot + jittered points plot_2 &lt;- ggplot( data = half_hourly_fluxes_subset, aes(x = Night, y = VPD_F)) + geom_boxplot(fill = &quot;grey70&quot;, outlier.shape = NA) + geom_jitter(width = 0.2, alpha = 0.3) + labs(title = &quot;Boxplot + jitter points&quot;) + labs(y = &quot;VPD (hPa)&quot;) + theme_classic() # Violin plot plot_3 &lt;- ggplot( data = half_hourly_fluxes_subset, aes(x = Night, y = VPD_F)) + geom_violin(fill = &quot;grey70&quot;) + labs(title = &quot;Violin plot&quot;) + labs(y = &quot;VPD (hPa)&quot;) + theme_classic() # combine plots cowplot::plot_grid(plot_1, plot_2, plot_3, ncol = 3) 4.2.2.4 Regression of two continuous variables Scatter plots visualize how two variables co-vary. The position of each point in a scatter plot is given by the simultaneously recorded value of two variables, provided in two columns along the same row in a data frame, and mapped onto two dimensions in a cartesian coordinate system. We can also say that two variables are regressed against each other. In the figure below, we start with a simple scatter plot (a), regressing GPP against shortwave radiation. A visualization is supposed to tell a story with data. The positive and largely linear relationship between shortwave radiation and GPP is expected from theory and our process understanding of the dominant controls on photosynthesis - it’s mainly solar (shortwave) radiation (However, strictly speaking, this is supposed to emerge only at longer time scales, not in half-hourly data). The linear regression line, added by geom_smooth(method = \"lm\") in (a), indicates that relationship. You will learn more about linear regression in Chapter 8. Are there additional variables that modify the relationship between solar radiation and GPP? To visually investigate this, we can map additional variables in our data set onto additional aesthetics. For example, at night, photosynthesis ceases as shown in (b). Here, the variable NIGHT was mapped onto the aesthetic color of same geometry (geom_point()). By default, ggplot() used a continuous color scale, as indicated by the color key on the right. It did so although NIGHT is a categorical (a binary) variable because in the data frame, NIGHT is stored as a numeric value (as can be checked by class(half_hourly_fluxes$NIGHT)). To avoid this, and automatically trigger the use of a color scheme that is suitable for categorical variables, we specify aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = as.factor(NIGHT)) in (c). In (d), temperature, a continuous variable, is mapped onto a continuous color scheme. # prepare plot data half_hourly_fluxes_subset &lt;- half_hourly_fluxes |&gt; sample_n(1000) # a plot_1 &lt;- ggplot( data = half_hourly_fluxes_subset, aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + geom_point(size = 0.75) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (&quot;, mu,&quot;mol CO&quot;[2], &quot; m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() # b plot_2 &lt;- ggplot( data = half_hourly_fluxes_subset, aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = NIGHT)) + geom_point(size = 0.75) + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (&quot;, mu,&quot;mol CO&quot;[2], &quot; m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() # c plot_3 &lt;- ggplot( data = half_hourly_fluxes_subset, aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = as.factor(NIGHT))) + geom_point(size = 0.75) + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (&quot;, mu,&quot;mol CO&quot;[2], &quot; m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() # d plot_4 &lt;- ggplot( data = half_hourly_fluxes_subset, aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = TA_F)) + geom_point(size = 0.75) + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (&quot;, mu,&quot;mol CO&quot;[2], &quot; m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() + scale_color_viridis_c() # combine plots cowplot::plot_grid(plot_1, plot_2, plot_3, plot_4, ncol = 2, labels = &quot;auto&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Figure 4.3: Regression plots. 4.2.2.5 Use of colors The above example demonstrates that color schemes have to be chosen depending on the nature of the data. Mapping a continuous variable onto the aesthetics color requires a continuous color scheme to be applied, categorical data requires discrete color schemes. More distinctions should be considered: Continuous variables should be distinguished further if they span a range that includes zero or not. If so, diverging color schemes should be used, where zero appears neutral (e.g., white). If zero is not contained within the range of values in the data, diverging color schemes should be avoided. Continuous or ordinal variables may be cyclic in nature. For example, hours in a day are cyclic, although there are twelve discrete numbers. The time 00:00 is nearer to 23:59 than it is from, for example, 01:00. The cyclical, or periodical nature of the data should be reflected in the choice of a color scheme where the edges of the range are more similar to one another than they are to the center of the range (see example below) Multisequential color schemes reflect that there is a natural distinction between two parts of the range of continuous values (see example below). Choices of colors and their combination is far from trivial. Colors in color schemes (or “scales”) should be: Distinguishable for people with color vision deficiency Distinguishable when printed in black and white Evenly spaced in the color space Intuitively encoding the information in the data (for example, blue-red for cold-hot) Visually appealing In (d), we mapped temperature, a continuous variable, onto the color aesthetic of the points and chose the continuous {viridis} color scale by specifying + scale_color_viridis_c(). The viridis scales have become popular for their respect of the points listed above. For further reading, several excellent resources exist that theorize and guide the use of color in data visualization. Excellent sources are: Fabio Crameri’s Scientific colour maps, Crameri (2018) and its R package {scico} (on CRAN). Paul Tol’s Notes, available for example in the {khroma} R package (on CRAN). 4.2.2.6 Regression within categories In the sub-plot (d) above, we may observe a pattern: GPP recorded at low temperatures (dark colored points) tend to be located in the lower range of the cloud of points. We may formulate a hypothesis from this observation, guiding further data analysis and modelling. This illustrates how data visualization is an integral part of any (geo-) data science workflow. Since the relationship between incoming solar radiation and ecosystem photosynthesis is strongly affected by how much of this light is actually absorbed by leaves, and because the amount of green foliage varies strongly throughout a year (the site CH-Lae from which the data is recorded is located in a mixed forest), the slope of the regression between solar radiation and GPP should change between months. Hence, let’s consider months as the categories to be used for separating the data and analyzing the bi-variate relationships separately within. Below, two alternatives are presented. Either the data is separated into a grid of sub-plots, or the data is separated by colors within the same plot panel. Separation by color # prepare data daily_fluxes &lt;- daily_fluxes |&gt; dplyr::mutate(month = month(date, label = TRUE)) ggplot( data = daily_fluxes, aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = month)) + geom_point(alpha = 0.5) + geom_smooth(formula = y ~ x + 0, method = &quot;lm&quot;, se = FALSE) + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (&quot;, mu,&quot;mol CO&quot;[2], &quot; m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)) ) + theme_classic() + scico::scale_color_scico_d(palette = &quot;romaO&quot;) Note three aspects here. First, the color-mapping is specified within aes() in the ggplot() function call and then adopted for all subsequent additions of geoms. Hence, also the geom_smooth() thus takes the color information, and not by a “hard-coded” specification of color = inside the geom_smooth() call as done in Fig. 4.3. Second, we specified a formula for the linear regression “smooting curve” to force the lines through the origin (y ~ x + 0). This is motivated by our a priori understanding of the process generating the data: when solar radiation is zero, photosynthesis (and hence GPP) should be zero. Third, we chose a color palette that reflects the cyclic (or periodic) nature of the categories (months). January is closer to December than it is to April. Therefore, their respective colors should also be closer in the color space. An appropriate palette for this is \"romaO\" from the {scico} package. Separation into sub-plots Yet another “mapping” is available with facet_wrap(). It separates the visualisation into different sub-plots, each showing only the part of the data that falls into the respective category, separated by facet_wrap(). Note, this mapping is not dealt with the same way as other aesthetics - not with specifying it with aes()), but with adding the facet_wrap() with a + to the ggplot() object. The variable by which facet_wrap() separates the plot has to be specified as an argument with a preceeding ~. Here, this is ~month. ggplot( data = daily_fluxes, # reusing the previously subset data (see above) aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + geom_point(alpha = 0.4) + geom_smooth(formula = y ~ x + 0, method = &quot;lm&quot;, color = &quot;red&quot;, se = FALSE) + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (&quot;, mu,&quot;mol CO&quot;[2], &quot; m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)) ) + facet_wrap(~month) You may object here that a linear regression is not a good model for our data. Instead, the relationship looks saturating, as indicated for example by the data in August. But we’ll get to modelling in later chapters. Nevertheless, the two visualizations above confirm our suspicion that the light-GPP relationship varies between months - a demonstration for why data visualization is an integral part of the scientific process. 4.2.2.7 Time series A time series plot can be regarded as a special case of a regression of two variables. In this case, one variable is regressed against time. A defining aspect of time is that there is a natural order in time steps. Therefore, it makes sense to visualize temporal data using lines that connect the points using geom_line(). The example below shows the time series of daily GPP in three years. ggplot( data = daily_fluxes, aes(x = date, y = GPP_NT_VUT_REF)) + geom_line() + labs(title = &quot;Line plot&quot;, x = &quot;Time&quot;, y = expression(paste(&quot;GPP (&quot;, mu,&quot;mol CO&quot;[2], &quot; m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() In the line plot above, we see a spurious-looking part of the time series in the first third of year 2004. Is this bad data that should be removed? Also, in winter of 2005/2005, some daily GPP values appear as high as a typical summer level of GPP. Is this bad data? Remember, that in Chapter 3, we aggregated the half-hourly half_hourly_fluxes data frame to a daily data frame daily_fluxes from which data is visualized above. The aggregation kept a record of the fraction of actually measured (not gap-filled) half-hourly data points per day (f_measured). This yields a “data quality axis”. Is there a pattern between f_measured and the presumably bad data? Discerning such patterns is often only possible with a suitable visualization. What is suitable here? A solution is to “map” f_measured to the color axis. When adding such an additional mapping to visualisation dimensions (“aesthetics”), we have to specify it using aes(). This only affects the points and the color of points, while the lines and points and their position in x-y space is shared. Hence, we write aes(x = date, y = GPP_NT_VUT_REF) in the ggplot() function call (indicating that all subsequent additions of geom_ layers share this x-y mapping); while aes(color = f_measured) is specified only in the geom_point() layer. ggplot( data = daily_fluxes, aes(x = date, y = GPP_NT_VUT_REF)) + geom_line() + geom_point(aes(color = f_measured), size = 0.9) + labs(x = &quot;Time&quot;, y = expression(paste(&quot;GPP (&quot;, mu,&quot;mol CO&quot;[2], &quot; m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + scale_color_viridis_c(direction = -1) + # inverse color scale is more intuitive here theme_classic() We observe the presumably bad data appear in yellow, and are therefore indeed characterised with a particularly low fraction of actually measured data from which their values are derived. This is an insight we would never have reached by just looking at the naked values in our data frames. Data visualizations are essential for guiding analyses and data processing throughout all steps. Having learned this, we now have a justification for applying further data filtering criteria. 4.2.2.8 Periodic data The seasons are an important axis of variation in our data. Hence our data are periodic - with a periodicity of 365 days in the daily_fluxes dataset and with both 12 hours and 365 days in the half_hourly_fluxes dataset. A polar coordinate system, instead of a cartesian system, lends itself to displaying periodic data. A polar coordinate system reflects the fact that, for example, January 1st is closer to December 31st, although they are located on the extreme end of a linear spectrum of days in a year. In a polar coordinate system, the x-axis spans the angle (360\\(^\\circ\\), like a clock), while the y-axis spans the radius (distance from the center). This is specified by changing the coordinate system of the ggplot object by + coord_polar(). Below, we first aggregate the data to get a mean seasonal cycle from daily_fluxes (a, b), and to get a mean diurnal (daily) cycle from June data in half_hourly_fluxes (c, d). To get the mean seasonal cycle, we first determine the day-of-year (counting from 1 for January first to 365 for December 31st) using the {lubridate} function yday(). # prepare plot data daily_mean_fluxes &lt;- daily_fluxes |&gt; mutate(doy = yday(date)) |&gt; group_by(doy) |&gt; summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF)) # seasonal cycle, cartesian plot_1 &lt;- ggplot( data = daily_mean_fluxes, aes(doy, GPP_NT_VUT_REF)) + geom_line() + labs(y = expression(paste(&quot;GPP (&quot;, mu,&quot;mol CO&quot;[2], &quot; m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)), x = &quot;Day of year&quot;) # seasonal cycle, polar plot_2 &lt;- ggplot( data = daily_mean_fluxes, aes(doy, GPP_NT_VUT_REF)) + geom_line() + coord_polar() + labs(y = expression(paste(&quot;GPP (&quot;, mu,&quot;mol CO&quot;[2], &quot; m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)), x = &quot;Day of year&quot;) # prepare plot data (diurnal step) daily_mean_hourly_fluxes &lt;- half_hourly_fluxes |&gt; mutate(month = month(TIMESTAMP_START)) |&gt; filter(month == 6) |&gt; # taking only June data mutate(hour = hour(TIMESTAMP_START)) |&gt; dplyr::group_by(hour) |&gt; dplyr::summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF)) # diurnal cycle, cartesian plot_3 &lt;- ggplot( data = daily_mean_hourly_fluxes, aes(hour, GPP_NT_VUT_REF)) + geom_line() + labs(y = expression(paste(&quot;GPP (&quot;, mu,&quot;mol CO&quot;[2], &quot; m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)), x = &quot;Hour of day&quot;) # diurnal cycle, polar plot_4 &lt;- ggplot( data = daily_mean_hourly_fluxes, aes(hour, GPP_NT_VUT_REF)) + geom_line() + coord_polar() + labs(y = expression(paste(&quot;GPP (&quot;, mu,&quot;mol CO&quot;[2], &quot; m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)), x = &quot;Hour of day&quot;) # combine plots cowplot::plot_grid(plot_1, plot_2, plot_3, plot_4, ncol = 2, labels = &quot;auto&quot;) 4.2.2.9 Density along two continuous variables Scatter plots can appear “overcrowded” when points are plotted on top of each other and potentially important information is lost in the visualization. ggplot( data = half_hourly_fluxes, aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + geom_point() + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (&quot;, mu,&quot;mol CO&quot;[2], &quot; m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() To avoid obscuring important details about the data, we may want to visualise the density of points. We want to plot how many points fall within bins of a certain range of values in GPP and shortwave radiation, or, in other words, within grid cells in the GPP-radiation space. We can visualize the data, for example, with a raster plot that measures the density using stat_density_2d() or with a binning into hexagonal cells using the simple geom_hex() layer. # density raster ggplot( data = daily_fluxes, aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + stat_density_2d( geom = &quot;raster&quot;, # the geometric object to display the data aes(fill = after_stat(density)), # using `density`, a variable calculated by the stat contour = FALSE ) + scale_fill_viridis_c() + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (&quot;, mu,&quot;mol CO&quot;[2], &quot; m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)) ) + theme_classic() + scale_x_continuous(expand = c(0, 0)) + # avoid gap between plotting area and axis scale_y_continuous(expand = c(0, 0)) # density hexagonal bins ggplot( data = daily_fluxes, aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + geom_hex() + scale_fill_viridis_c() + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (&quot;, mu,&quot;mol CO&quot;[2], &quot; m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)) ) + theme_classic() An alternative solution to “overplotting” points is described in this blog post. 4.2.2.10 Raster data In the figure above, the density of points in a grid of equally spaced bins along two axes, one for each variable, was shown. Often, data is organised along a grid of equally spaced bins by nature - think a matrix or raster data. Examples of such data are climate model outputs (which often span more than two dimensions), remote sensing images (again, just one “layer” of an image or one “band”), or images in general. In these cases, two (often spatial) axes span the space of a cartesian coordinate system and the value within each pixel is mapped onto the color aesthetic. The base-R function image() can be used to visualize such spatial data as images. image(volcano) {ggplot2} forces us to the data frame paradigm and therefore doesn’t lend itself naturally to raster data. We can, however, convert raster data into a data frame in a separate step. # example from https://github.com/thomasp85/scico df_volcano &lt;- tibble( x = rep(seq(ncol(volcano)), each = nrow(volcano)), y = rep(seq(nrow(volcano)), ncol(volcano)), height = c(volcano) - 140 # subtract 140 for example below ) ggplot( data = df_volcano, aes(x = x, y = y, fill = height)) + geom_raster() + scico::scale_fill_scico(palette = &#39;bukavu&#39;, midpoint = 0) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) In this example, we used the multisequential \"bukavu\" color scale from the {scico} package to reflect values above 0 (e.g., elevation above the sea level) and below 0 (ocean depth). 4.2.2.11 Geospatial data Raster data often reflects values in geographic space. Also points and polygons may be located in geographic space. This opens the door to geospatial data visualisation (a.k.a. creating maps) which does not form part of this course. Interested readers can find more content on geospatial data visualisation here: Timo Grossenbacher’s blog on geographic data journalism (link) Koen Hufken’s blog containing various worked example. 4.2.3 Writing to file Plots can be saved in figure files in any common format. To write the figure that was rendered last into a file, call the ggsave(filename) function. The format of the image is automatically determined based on filename. E.g., a PDF is created if the filename is. For example, plotme ggsave(&quot;./figures/plot_gpp.pdf&quot;) … saves the plot generated by printing plotme (created in the code chunk preceeding Fig. 4.1) into a PDF located in the appropriate (see also 1.2.7.1 what’s “appropriate”) sub-directory. Plots are ideally saved as vector graphics - the types of graphics that are defined by geometric shapes (polygons, lines, points), not rasters. Vector graphics never look “pixelated”. When saving a ggplot object as PDF, it’s saved as a vector graphic. Other vector graphics formats are .eps, or .svg. Beware however, when plotting very large numbers points, lines, or polygons, file sizes can get excessively large and render very slowly. In such cases, consider representing the density of overlaying points which creates a vector graphics, e.g., of rectangles (raster), hexagons, or other polygons (see Sec. 4.2.2.9). Alternatively, plots may be saved as a raster image (e.g., PNG with files ending on .png, giving the option to manually set the resolution with the argument dpi) or a compressed image format (e.g, JPEG). When rendering an RMarkdown file, figures are placed in the output and, unless explicitly triggered with a ggsave() call inside an executed code chunk, no separate figure files are created. 4.3 Exercises Spurious data In Section 3.2.10.2, we discovered that certain values of GPP_NT_VUT_REF in the half-hourly data half_hourly_fluxes (to be read from file data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv) are repeated with a spuriously high frequency. Determine all values of GPP_NT_VUT_REF that appear more than once in half_hourly_fluxes and label them as being “spurious”. Visualise the time series of the first year of half-hourly GPP, mapping the information whether the data is spurious or not to the color aesthetic. Then aggregate half-hourly to daily data, taking the mean of GPP_NT_VUT_REF and recording the proportion of underlying half-hourly data points that are “spurious”. Visualise the time series of daily GPP_NT_VUT_REF with the color scale indicating the proportion of spurious half-hourly data that was used for determining the respective date’s mean GPP. Hint: The base-R function duplicated(df, ...) takes a data frame df as input and returns a logical vector of length nrow(df) specifying whether a row has an exact duplicate in the same data frame. Identifying Outliers A key part of data cleaning is to detect and understand outliers. Visualisations can help. Your task here is to find outliers in GPP_NT_VUT_REF. First, using the half-hourly fluxes data, determine “outliers” as those values of GPP_NT_VUT_REF that fall outside \\(( Q_1 - 1.5 (Q_3 - Q_1)\\) to \\(Q_3 + 1.5 (Q_3 - Q_1)\\). Plot GPP_NT_VUT_REF versus shortwave radiation and highlight outliers in red. Hint: Use boxplot.stats() to return a list containing a vector of the data points which lie beyond the extremes of the whiskers of the boxplot. Hint: Use scale_color_manual() to mannually define the color scale. Now, we want to “control” for the influence of shortwave radiation on GPP and define outliers with respect to the distribution of residuals of the linear regression between the two variables. Relax the definition of what is considered an outlier by setting adjusting their definition to falling outside \\(( Q_1 - 5 (Q_3 - Q_1)\\) to \\(Q_3 + 5 (Q_3 - Q_1)\\). Again, plot GPP_NT_VUT_REF versus shortwave radiation and highlight outliers in red. Hint: Fit the linear regression model as lm(GPP_NT_VUT_REF ~ SW_IN_F, data = half_hourly_fluxes) and obtain the residuals from the object returned by the lm() function (see ‘Value’ in its help page). Hint: The output of boxplot.stats(x) is a list, containing an element out. out is a named vector of the oulier values with names referring to the row numbers of x. Use as.integer(names(boxplot.stats(x)$out)) to get row numbers. Visualising diurnal and seasonal cycles of GPP As explored in the previous Chapter’s exercises, GPP varies over diurnal and seasonal cycles. Create a publication-ready figure that visualises the mean diurnal cycle of GPP for each day-of-year (mean across multiple years). Make sure that the figure is properly labelled, and legible for readers with a color vision deficiency. Hint: To get the diurnal and seasonal cycles, summarise the half-hourly data by the hour of the day and the day of the year simultaneously using multiple grouping variables within group_by() and calculate mean values for GPP for each group. Hint: Chose an appropriate visualisation that maps the hour-of-day to the x-axis and the day-of-year to the y-axis. Trend in carbon dioxide concentrations This exercise explores the longest available atmospheric CO\\(_2\\) record, obtained at the Mauna Loa observatory in Hawaii. Atmospheric CO\\(_2\\) in the northern hemisphere is characterised by seasonal swings, caused by the seasonal course of CO\\(_2\\) uptake and release by the terrestrial biosphere. We’ve explored the seasonality of the CO\\(_2\\) uptake measured at one site (in Switzerland) extensively in this an previous chapters. Your task here is to calculate and visualise the long-term trend of CO\\(_2\\). Follow these steps: Download the monthly mean CO2\\(_2\\) data as a CSV file from here and read it into R. Make a simple graph for visualizing the monthly CO\\(_2\\) time series. Write a function that computes a 12-month running mean of the CO\\(_2\\) time series. The running mean for month \\(m\\) should consider values of \\(m-5\\) to \\(m+6\\). Define arguments for the function that let the user specify the width of the running mean “box” (i.e., setting the \\(5\\) and \\(6\\) to any other integer of choice) Make a publication-ready figure that shows the monthly and the 12-month running mean time series of the CO\\(_2\\) record. Hint: You don’t need to clean the .txt file by hand, find a suitable function in R. Hint: Arguments to your function may be a vector of the original (monthly) data and a parameter defining the number of elements over which the mean is to be taken. Hint: To automatically render the time axis with ggplot, you can create a time object by combining the year and month columns: lubridate::ymd(paste(as.character(year), \"-\", as.character(month), \"-15\")) 4.4 Report Exercises Telling a story from data In the previous exercises and tutorials, you have learned how to wrangle data, fit simple linear regression models and identify outliers, create figures for temporal patterns, and develop and test hypotheses. Use these skills to analyze the airquality dataset (directly available in R, just type datasets::airquality into the console). The target variable of this dataset is the ozone concentration and your task is to tell a story about it. Look at the other variables in the dataset and become creative! Think of what patterns and relationships could be interesting to talk about. Your report must include the following elements: A description of the airquality dataset (where is it from, what are the variables’ units, etc.). A specific question that you want to answer through analyzing the data. At least three statistical metrics from your dataset that aid you in answering your question (e.g., mean values, ranges, etc.). At least three publishable figures or tables that show important relationships that aid you in answering your question (e.g., outliers, temporal patterns, scatterplots, etc.). Make sure to interpret and discuss your results and hypotheses. Why were you right / why were you wrong? Important: The text alone should not exceed one A4 page (max. 400 words). Hint: Metrics, figures, tables, etc. without any written-out explanation what they show do not count. Hint: To get more background information on the data, use the help functionalities in RStudio. Deliverables for the report Following the same requirements as mentioned in 3.5, present your solutions in a file called re_airquality.Rmd, save it in your vignettes folder alongside the HTML version, and make sure that your code is reproducible (make sure your .rmd is knittable, that all data is available, that paths to that data work, etc.). "],["datavariety.html", "Chapter 5 Data variety 5.1 Learning objectives 5.2 Tutorial 5.3 Exercises", " Chapter 5 Data variety Chapter lead author: Koen Hufkens 5.1 Learning objectives As a scientist you will encounter a variety of data (formats). In this section, you will learn some of the most common formats, their structure, and the advantages and disadvantages of using a particular data format. Only singular files are considered in this section, and databases are not covered although some files (formats) might have a mixed use. More and more data moves toward a cloud server-based model where data is queried from an online database using an Application Programming Interface (API). Although the explicit use of databases is not covered, you will learn basic API usage to query data which is not represented as a file. In this chapter you will learn: how to recognize data and file formats understand data and file format limitations manipulation wise content wise how to read and/or write data in a particular file format how to query an API and store its data locally 5.2 Tutorial 5.2.1 Files and file formats 5.2.1.1 File extensions In order to manipulate data and make some distinctions on what a data file might contain, files carry a particular file format extension. These file extensions denote the intended content and use of a particular file. For example, a file ending in .txt suggests that it contains text. A file extension allows you, or a computer, to anticipate the content of a file without opening the file. File extensions are therefore an important tool in assessing what data you are dealing with, and what tools you will need to manipulate (read and write) the data. NOTE: File extensions can be changed. In some cases, the file extension does not represent the content of the data contained within the file. TIP: If a file doesn’t read, it is always wise to try to open the file in a text editor and check the first few lines to verify if the data has a structure that corresponds to the file extension. # On a linux/macos system you can use the terminal command (using # the language bash) to show the first couple lines of a file head your_file # alternatively you can show the last few lines # of a file using tail your_file 5.2.1.2 Human-readable data One of the most important distinctions in data formats falls along the line of it being human-readable or not. Human-readable data is, as the term specifies, made up of normal text characters. Human-readable text has the advantage that it is easily read and edited using conventional text editors. This convenience comes at the cost of the files not being compressed in any way, and file sizes can become unnecessarily large. However, for many applications where file sizes are limited (&lt;50MB), human-readable formats are the preferred option. Most human-readable data falls in two broad categories, tabular data and structured data. Tabular data Often, human-readable formats provide data in tabular form using a consistent delimiter. This delimiter is a character separating columns of a table. column_one, column_two, column_three 1, 2, 3 1, 2, 3 1, 2, 3 Common delimiters in this context are the comma (,), as shown in the above example. A file with this particular format often carries the comma-separated values file extension (*.csv). Other delimiters are the tabulation (tab) character. Files with tab delimited values have the *.tsv format. TIP: File extensions aren’t always a true indication of the delimiter used. For example, .txt files often contain comma or tab separated data. If reading a file using a particular delimiter fails it is best to check the first few lines of a file. Structured data Tabular data is row and column-oriented and therefore doesn’t allow complex structured content, e.g. tables within tables. This issue is sidestepped by the JSON format. The JSON format uses attribute-value pairs to store data, and is therefore more flexible in terms of accommodating varying data structures. Below, you see an example of details describing a person, with entries being fo varying length and data types. { &quot;firstName&quot;: &quot;John&quot;, &quot;lastName&quot;: &quot;Smith&quot;, &quot;isAlive&quot;: true, &quot;age&quot;: 27, &quot;address&quot;: { &quot;streetAddress&quot;: &quot;21 2nd Street&quot;, &quot;city&quot;: &quot;New York&quot;, &quot;state&quot;: &quot;NY&quot;, &quot;postalCode&quot;: &quot;10021-3100&quot; }, &quot;phoneNumbers&quot;: [ { &quot;type&quot;: &quot;home&quot;, &quot;number&quot;: &quot;212 555-1234&quot; }, { &quot;type&quot;: &quot;office&quot;, &quot;number&quot;: &quot;646 555-4567&quot; } ], &quot;children&quot;: [ &quot;Catherine&quot;, &quot;Thomas&quot;, &quot;Trevor&quot; ], &quot;spouse&quot;: null } NOTE: Despite being human-readable, a JSON file is considerably harder to read than a comma separated file. Editing such a file is therefore more prone to errors if not automated. Other human-readable structured data formats include the eXtensible Markup Language (XML), which is commonly used in web infrastructure. XML is used for storing, transmitting, and reconstructing arbitrary data but uses (text) markup instead of attribute-value pairs. &lt;note&gt; &lt;to&gt;Tove&lt;/to&gt; &lt;from&gt;Jani&lt;/from&gt; &lt;heading&gt;Reminder&lt;/heading&gt; &lt;body&gt;Don&#39;t forget me this weekend!&lt;/body&gt; &lt;/note&gt; 5.2.1.3 Writing and reading human-readable files in R There are a number of ways to read human-readable formats into an R work environment. Here the basic approaches are listed, in particular reading CSV and JSON data. Large volumes of data are available as CSV files or similar. Understanding how to read in such data into a programming environment is key. In this context the read.table() function is a general purpose tool to read in text data. Depending on the format, additional meta-data or comments, and certain parameters need to be specified. Its counterpart is a function to write human-readable data to file, called - you guessed it - write.table(). Again, parameters are required for maximum control over how things are written to file, by default though data are separated by a single empty space ” “, not a comma. Note: In Chapter 3, we used the {readr} (tidyverse) function read_csv(). It serves the same purpose as read.csv(), but is faster and reads data into a tidyverse-data frame (a tibble) which has some useful additional characteristics, on top of a common R data frame. For particularly large data, you may consider even better solutions for fast reading, see here. Below, you find and example in which a file is written to a temporary location, and read in again using the above mentioned functions. # create a data frame with demo data df &lt;- data.frame( col_1 = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), col_2 = c(&quot;d&quot;, &quot;e&quot;, &quot;f&quot;), col_3 = c(1,2,3) ) # write table as CSV to disk write.table( x = df, file = file.path(tempdir(), &quot;your_file.csv&quot;), sep = &quot;,&quot;, row.names = FALSE ) # Read a CSV file df &lt;- read.table( file.path(tempdir(), &quot;your_file.csv&quot;), header = TRUE, sep = &quot;,&quot; ) # help files of both functions can be accessed by # typing ?write.table or ?read.table in the R console In this example, a data frame is generated with three columns. This file is then written to a temporary file in the temporary file directory tempdir(). Here, tempdir() returns the location of the temporary R directory, which you can use to store intermediate files. We use the file.path() function to combine the path (tempdir()) with the file name (your_file.csv). Using file.path() is good practice as directory structures are denoted differently between operating systems, e.g., using a backslash (\\) on Windows vs. a slash (/) on Unix-based systems (Linux/macOS). The file.path() function ensures that the correct directory separator is used. Note that in this command, we have to manually set the separator (sep = \",\") and whether a header is present (header = TRUE). Depending on the content of a file, you will have to alter these parameters. Additional parameters of the read.table() function allow you to specify comment characters, skip empty lines, etc. Similar to this simple CSV file, we can generate and read JSON files. For this, we do need an additional library, as default R install does not provide this capability. However, the rest of the example follows the above workflow. # we&#39;ll re-use the data frame as generated for the CSV # example, so walk through the above example if you # skipped ahead # load the library library(&quot;jsonlite&quot;) # write the file to a temporary location jsonlite::write_json( x = df, path = file.path(tempdir(), &quot;your_json_file.json&quot;) ) # read the freshly generated json file df_json &lt;- jsonlite::read_json( file.path(tempdir(), &quot;your_json_file.json&quot;), simplifyVector = TRUE ) # check if the two data sets # are identical (they should be) identical(df, df_json) ## [1] TRUE Note that the reading and writing JSON data is easier, as the structure of the data (e.g., field separators) are more strictly defined. While reading the data, we use the simplifyVector argument to return a data frame rather than a nested list. This works as our data has a tabular structure, but this might not always be the case. Finally, we compare the original data with the data read in using identical(). TIP: In calling the external library we use the :: notation. Although by loading the library with library() makes all jsonlite functions available, the explicit referencing of the origin of the function makes debugging often easier. 5.2.1.4 Binary data All digital data which is not represented as text characters can be considered binary data. Binary data can vary in its content from an executable, which runs a program, to the digital representation of an image (jpeg images). However, in all cases, the data is represented as bytes (made of eight bits) and not text characters. One of the advantages of binary data is that it is an efficient representation of data, saving space. This comes at the cost of requiring a dedicated software, other than a text editor, to manipulate the data. For example, digital images in a binary format require image manipulation software. More so than human-readable data, the file format (extension) determines how to treat the data. Knowing common data formats and their use cases is therefore key. 5.2.1.5 Common file formats Environmental sciences have particular file formats which dominate the field. Some of these file formats relate to the content of the data, some of these formats are legacy formats due to the history of the field itself. Here we will list some of the most common formats you will encounter. File format (extension) Format description Use case R Library *.csv comma separated tabular data General purpose flat files with row and column oriented data base R *.txt tabular data with various delimiters General purpose flat files with row and column oriented data base R *.json structured human-readable data General purpose data format. Often used in web application. Has geospatial extensions (geojson). jsonlite *.nc NetCDF data array data Array-oriented data (matrices with &gt; 2 dimensions). Commonly used to store climate data or model outputs. Alternative to HDF data. ncdf4, terra, raster *.hdf HDF array data Array-oriented data (matrices with &gt; 2 dimensions). Commonly used to store Earth observation data. hdf *.tiff, *.geotiff Geotiff multi-dimensional raster data (see below) Layered (3D) raster (image) data. Commonly used to represent spatial (raster) data. terra, raster *.shp Shapefile of vector data (see below) Common vector based geospatial data. Used to describe data which can be captured by location/shape and attribute values. sp, sf 5.2.2 Meta-data Meta-data is data that is associated with the main data file and is key to understanding the file content and the context of the data. In some cases, you will find this data only as a general description referencing the file(s) itself. In other cases, meta-data is included in the file itself. For example, many tabular CSV data files contain a header specifying the content of each column, and at times a couple of lines of data specifying the content of the file itself - or context within which the data should be considered. # This is meta-data associated with the tabular CSV file # for which the data is listed below. # # In addition to some meta-data, the first row of the data # contains the column header data column_one, column_two, column_three 1, 2, 3 1, 2, 3 1, 2, 3 In the case of binary files it will not be possible to read the meta-data directly as plain text. In this case, specific commands can be used to read the meta-data included in a file. The example below shows how you would list the meta-data of a GeoTiff file using the bash. # list geospatial data for a geotiff file gdalinfo your_geotiff.tiff TIP: Always keep track of your meta-data by including it, if possible, in the file itself. If this is not possible, meta data is often provided in a file called README. Meta-data is key in making science reproducible and guaranteeing consistency between projects. Key meta-data to retain are: the source of your data (URL, manuscript, DOI) the date when the data was downloaded manipulations on the data before using the data in a final workflow Meta-data of data read into R can be accessed by printing the object itself, i.e., calling the object in the console. If it is a simple table, the first lines of the table will be shown. If it is a more complex object, the meta data will be output as a formatted statement. You can also use the str() or summary() functions to summarize data and meta-data. 5.2.3 Spatial data representation Environmental data often has an explicit spatial and temporal component. For example, climate data is often represented as 2D maps which vary over time. This spatial data requires an additional level of understanding of commonly used data formats and structures. In general, we can distinguish two important data models when dealing with spatial data, the raster and vector data model. Both data have their typical file formats (see above) and particular use cases. The definition of these formats, optimization of storage and math/logic on such data are the topic of Geographic Information System (GIS) science and beyond the scope of this course. We refer to other elective GIS courses for a greater understanding of these details. However, a basic understanding of both raster and vector data is provided here. 5.2.3.1 Raster data model The basic raster model represents geographic (2D) continuous data as a two-dimensional array, where each position has a geographic (x, y) coordinate, a cell size (or resolution) and a given extent. Using this definition, any image adheres to the raster model. However, in most geographic applications, coordinates are referenced and correspond to a geographic position, e.g., a particular latitude and longitude. Often, the model is expanded with a time dimension, stacking various two-dimensional arrays into a three-dimensional array. The raster data model is common for all data sources which use either imaging sensors, such as satellites or unmanned aerial vehicles (UAVs), or outputs of models that operate on a cartesian grid, including most climate and numerical weather prediction models. Additional meta data stores both the geographic reference system, the definition and format of the time information, and well as other data which might be helpful to end users (e.g., variable units). Within the environmental sciences, NetCDF and GeoTiff are common raster data file formats. 5.2.3.2 Vector data model The vector data model, in contrast to the raster data model, describes (unbound) features using a geometry (location, shape) using coordinates and linked feature attributes. Geometries can be points, lines, polygons, or even volumes. Vector data does not have a defined resolution, making them scale-independent. This makes the vector data model ideal for discrete features such as roads or building outlines. Conversely, vector data is poorly suited for continuous data. Conversions between the vector and raster model are possible, but limitations apply. For example, when converting vector data to raster data a resolution needs to be specified, as you lose scale independence of the vector format. Conversions from raster to vector are similarly limited by the original resolution of the raster data. In this course we will focus on raster data only, the most common format within the context of data science. 5.2.4 Online data sources The sections above assume that you have inherited some data from someone, or have data files on disk (in a particular format). Yet, most of the time, gathering data is the first step in any analysis. Depending on where data is hosted you can simply download data through your web browser or use the internal download.file() R function to grab data. Today, many of the data described in previous sections are warehoused in large cloud facilities. These data (and their underlying data formats) are stored in large databases and displayed through various applications. For example, Google Maps displays remote sensing (satellite) raster image data in addition to street level vector based labels. These services allow you to access the underlying (original) data using an API, hence programmatically using code. Mastering the use of these services has become key in gathering research data. 5.2.4.1 Direct downloads Before diving into a description of APIs, we remind you that some file reading functions in R are web-aware, and can not only read local files but also remote ones (i.e., URLs). Getting ahead of ourselves a bit (see tutorials below), the example code shows you how to read the content of a URL directly into your R environment. Although using this functionality isn’t equivalent to using an API, the concept is the same. I.e., you load a remote data source. # define a URL with data of interest # in this case annual mean CO2 levels at Mauna Loa url &lt;- &quot;https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_annmean_mlo.csv&quot; # read in the data directly from URL df &lt;- read.table( url, header = TRUE, sep = &quot;,&quot; ) 5.2.4.2 APIs Web-based Application Programming Interfaces (APIs) offer a way to specify the scope of the returned data, and ultimately, the processing which goes on behind the scene in response to a (data) query. APIs are a way to, in a limited way, control a remote server to execute a certain (data) action. In most (RESTful) APIs, such query takes the form of an HTTP URL via an URL-encoded scheme using an API endpoint (or base URL). To reduce some of the complexity of APIs, it is common that a wrapper is written around an API in the language of choice (e.g., R, Python). These dedicated API libraries make it easier to access data and limit coding overhead. Dedicated API libraries As an example of a dedicated library, we use the {MODISTools} R package which queries remote sensing data generated by the MODIS remote sensing (satellite) mission from the Oak Ridge National Laboratories data archive. # load the library library(&quot;MODISTools&quot;) # list all available products products &lt;- MODISTools::mt_products() # print the first few lines # of available products print(head(products)) ## product ## 1 Daymet ## 2 ECO4ESIPTJPL ## 3 ECO4WUE ## 4 GEDI03 ## 5 GEDI04_B ## 6 MCD12Q1 ## description ## 1 Daily Surface Weather Data (Daymet) on a 1-km Grid for North America, Version 4 R1 ## 2 ECOSTRESS Evaporative Stress Index PT-JPL (ESI) Daily L4 Global 70 m ## 3 ECOSTRESS Water Use Efficiency (WUE) Daily L4 Global 70 m ## 4 GEDI Gridded Land Surface Metrics (LSM) L3 1km EASE-Grid, Version 2 ## 5 GEDI Gridded Aboveground Biomass Density (AGBD) L4B 1km EASE-Grid, Version 2.1 ## 6 MODIS/Terra+Aqua Land Cover Type (LC) Yearly L3 Global 500 m SIN Grid ## frequency resolution_meters ## 1 1 day 1000 ## 2 Varies 70 ## 3 Varies 70 ## 4 One time 1000 ## 5 One time 1000 ## 6 1 year 500 # download a demo dataset # specifying a location, a product, # a band (subset of the product) # and a date range and a geographic # area (1 km above/below and left/right). # Data is returned internally and the # progress bar of the download is not shown. subset &lt;- MODISTools::mt_subset( product = &quot;MOD11A2&quot;, lat = 40, lon = -110, band = &quot;LST_Day_1km&quot;, start = &quot;2004-01-01&quot;, end = &quot;2004-02-01&quot;, km_lr = 1, km_ab = 1, internal = TRUE, progress = FALSE ) # print the dowloaded data print(head(subset)) ## xllcorner yllcorner cellsize nrows ncols band units ## 1.1 -9370963.05 4445948.79 926.625433055834 3 3 LST_Day_1km Kelvin ## 2.1 -9370963.05 4445948.79 926.625433055834 3 3 LST_Day_1km Kelvin ## 3.1 -9370963.05 4445948.79 926.625433055834 3 3 LST_Day_1km Kelvin ## 4.1 -9370963.05 4445948.79 926.625433055834 3 3 LST_Day_1km Kelvin ## 1.2 -9370963.05 4445948.79 926.625433055834 3 3 LST_Day_1km Kelvin ## 2.2 -9370963.05 4445948.79 926.625433055834 3 3 LST_Day_1km Kelvin ## scale latitude longitude site product start end complete ## 1.1 0.02 40 -110 sitename MOD11A2 2004-01-01 2004-02-01 TRUE ## 2.1 0.02 40 -110 sitename MOD11A2 2004-01-01 2004-02-01 TRUE ## 3.1 0.02 40 -110 sitename MOD11A2 2004-01-01 2004-02-01 TRUE ## 4.1 0.02 40 -110 sitename MOD11A2 2004-01-01 2004-02-01 TRUE ## 1.2 0.02 40 -110 sitename MOD11A2 2004-01-01 2004-02-01 TRUE ## 2.2 0.02 40 -110 sitename MOD11A2 2004-01-01 2004-02-01 TRUE ## modis_date calendar_date tile proc_date pixel value ## 1.1 A2004001 2004-01-01 h09v05 2020168005635 1 13148 ## 2.1 A2004009 2004-01-09 h09v05 2020168010833 1 13160 ## 3.1 A2004017 2004-01-17 h09v05 2020168012220 1 13398 ## 4.1 A2004025 2004-01-25 h09v05 2020168013617 1 13412 ## 1.2 A2004001 2004-01-01 h09v05 2020168005635 2 13153 ## 2.2 A2004009 2004-01-09 h09v05 2020168010833 2 13140 A detailed description of all functions of the {MODISTools} R package is beyond the scope of this course. However, the listed command show you what a dedicated API package does. It is a shortcut to functional elements of an API. For example mt_products() allows you to quickly list all products without any knowledge of an API URL. Although more complex, as requiring parameters, the mt_subset() routine allows you to query remote sensing data for a single location (specified with a latitude lat and longitude lon), and a given date range (e.g., start, end parameters), a physical extent (in km left-right and above-below). GET Depending on your data source, you will either need to rely on a dedicated R package to query the API or study the API documentation. The general scheme for using an API follows the use of the GET() command of the {httr} R library. You define a query using API parameters, as a named list, and then use a GET() statement to download the data from the endpoint (url). # formulate a named list query to pass to httr query &lt;- list( &quot;argument&quot; = &quot;2&quot;, &quot;another_argument&quot; = &quot;3&quot; ) # The URL of the API (varies per product / param) url &lt;- &quot;https://your.service.endpoint.com&quot; # download data using the # API endpoint and query data # status variable will include if # the download was successful or not # the write_disk() function captures # data if available and writes it to # disk status &lt;- httr::GET( url = url, query = query, httr::write_disk( path = &quot;/where/to/store/data/filename.ext&quot;, overwrite = TRUE ) ) Below, we provide an example of using the GET command to download data from the Regridded Harmonized World Soil Database (v1.2) as hosted on the Oak Ridge National Laboratory computer infrastructure. In this case we download a subset of a global map of topsoil sand content (T_SAND). # set API URL endpoint # for the total sand content url &lt;- &quot;https://thredds.daac.ornl.gov/thredds/ncss/ornldaac/1247/T_SAND.nc4&quot; # formulate query to pass to httr query &lt;- list( &quot;var&quot; = &quot;T_SAND&quot;, &quot;south&quot; = 32, &quot;west&quot; = -81, &quot;east&quot; = -80, &quot;north&quot; = 34, &quot;disableProjSubset&quot; = &quot;on&quot;, &quot;horizStride&quot; = 1, &quot;accept&quot; = &quot;netcdf4&quot; ) # download data using the # API endpoint and query data status &lt;- httr::GET( url = url, query = query, httr::write_disk( path = file.path(tempdir(), &quot;T_SAND.nc&quot;), overwrite = TRUE ) ) # to visualize the data # we need to load the {terra} # library library(&quot;terra&quot;) sand &lt;- terra::rast(file.path(tempdir(), &quot;T_SAND.nc&quot;)) terra::plot(sand) Authentication Depending on the API, authentication using a user name and a key or password is required. Then, the template should be slightly altered to accommodate for these requirements. Note that instead of the GET() command we use POST() as we need to post some authentication data before we can get the data in return. # an authenticated API query status &lt;- httr::POST( url = url, httr::authenticate(user, key), httr::add_headers(&quot;Accept&quot; = &quot;application/json&quot;, &quot;Content-Type&quot; = &quot;application/json&quot;), body = query, encode = &quot;json&quot; ) 5.3 Exercises Files and file formats Reading and writing human-readable files While not leaving your R session, download and open the files at the following locations: https://raw.githubusercontent.com/geco-bern/agds/main/data/demo_1.csv https://raw.githubusercontent.com/geco-bern/agds/main/data/demo_2.csv https://raw.githubusercontent.com/geco-bern/agds/main/data/demo_3.csv Once loaded into your R environment, combine and save all data as a temporary CSV file. Read in the new temporary CSV file, and save it as a JSON file in your current working directory. Reading and writing binary files Download and open the following file: https://raw.githubusercontent.com/geco-bern/agds/main/data/demo_data.nc What file format are we dealing with? What library would you use to read this kind of data? What does this file contain? Write this file to disk in a different geospatial format you desire (use the R documentation of the library used to read the file and the chapter information). Download and open the following file: https://raw.githubusercontent.com/geco-bern/agds/main/data/demo_data.tif. Does this data seem familiar, and how can you tell? What are your conclusions? API use GET Download the HWSD total sand content data for the extent of Switzerland following the tutorial example. Visualize/plot the data as a simple map. Download the HWSD topsoil silt content for the extent of Switzerland. 5.3.0.1 Dedicated library Use the {hwsdr} library (a dedicated package for the API) to download the same data. How does this compare to the previous code written? List how many data products there are on the ORNL MODIS data repository. Download the MODIS land cover map for the canton of Bern. "],["openscience.html", "Chapter 6 Open science practices 6.1 Learning objectives 6.2 Tutorial 6.3 Exercises", " Chapter 6 Open science practices Chapter lead author: Koen Hufkens 6.1 Learning objectives In this chapter, you will learn the reasons for practicing open science and some of the basic methodological techniques that we can use to facilitate an open science workflow. In this chapter you will learn how to: structure a project manage a project workflow capture a session or machine state use dynamic reporting ensure data and code retention 6.2 Tutorial The scientific method relies on repeated testing of a hypothesis. When dealing with data and formal analysis, one can reduce this problem to the question: could an independent scientist attain the same results given the described methodology, data and code? Although this seems trivial, this issue has vexed the scientific community. These days, many scientific publications are based on complex analyses with often large data sets. More so, methods in publications are often insufficiently detailed to really capture the scope of an analysis. Even from a purely technical point of view, the reproducibility crisis or the inability to reproduce experimental results, is a complex problem. This is further compounded by social aspects and incentives. In recent decades, scientific research has seen a steady increase in speed due to the digitization of many fields and the commodification of science. Although digitization has opened up new research possibilities, its potential for facilitating, accelerating, and advancing science are often not fully made use of. Historically, research and its output in the form of data and code has been confined to academic journals, data and code has often not been made available, and data were not shared or behind pay-walls. This limits the impact of science also in the public domain. (In many ways, this is still the case today, in year 2023 as we write). Digitization has made research output better visible and accessible, but practical obstacles and weak standards often prevent it from uptake, re-use, and further development by the wider community. Open and reproducible science is a movement to make scientific research (output) widely accessible to the larger public, increase research transparency and enabling robust and verifiable science. Open science aims to be as open as possible about the whole scientific process, and as closed as desirable (e.g. privacy or security reasons). It is important to acknowledge that there is a spectrum of reproducible data and code workflows which depends on the state or source of the data and the output of the code (or analysis). Within the context of this course, we focus primarily on the practical aspects for reproducible science, i.e., ensuring that given the same data and code, the results will be similar. Figure 6.1: The reproducibility matrix by The Turing Way The basics of open science coding and data practices rely on a number of simple concepts. The sections below describe a selection of the most important ones. Sticking to these principles and tools will increase the reproducibility of your work greatly. 6.2.1 Project structure Reproducible science relies on a number of key components. Data and code management and the tracking of required meta-data is the first step in an open science workflow. In Chapter 1, you learned how to setup an R project. An R project gathers all components of your analysis in a single directory. Although current computers make it easy to “find” your files and are largely file location-agnostic, this is not the case in many research environments. Projects grow quickly, and often, the number of files will flood a single directory. Therefore, files need a precise and structured location. This structure allows you to determine both the function and order of a workflow without reading any code. It is good practice to have a consistent project structure within and between projects. This allows you to find most project components regardless of when you return to a particular project. Structuring a project in one folder also makes projects portable. All parts reside in one location making it easy to create a git project from this location (see Chapter 7), or just copy the project to a new drive. An example data structure for raw data processing is given below and we provide an R project template to work from and adjust through our lab GitHub profile. A full description on using the template is provided in Chapter 7. data-raw/ ├─ raw_data_product/ ├─ 00_download_raw_data.R ├─ 01_process_raw_data.R 6.2.2 Managing workflows Although some code is agnostic to the order of execution, many projects are effectively workflows, where the output of one routine is required for the successful execution of the next routine. In order to make sure that your future self, or a collaborator, understands the order in which things should be executed, it is best to number scripts accordingly. This is the most basic approach to managing workflows. In the example below, all statistics code is stored in the statistics folder in an overall analysis folder (which also includes code for figures). All statistical analyses are numbered to ensure that the output of a first analysis is available to the subsequent one. analysis/ ├─ statistics/ │ ├─ 00_randomforest_model.R │ ├─ 01_randomforest_tuning.R ├─ figures/ │ ├─ global_model_results_map.R │ ├─ complex_process_visualization.R The code-chunk above is a visualisation of a folder (aka. directory) structure on your computer. The lines and indents denote folder levels. In this example, you have a folder analysis which holds two more folders statistics and figures, and in both sub-folders, you have different *.R files (* is a so-called “wild-card” which is a placeholder for any text). Note that different people may use different symbols to visualise folder structures but generally, folder levels are shown with indents, and files are identifiable by their suffixes. 6.2.2.1 Automating and visualizing workflows with targets To sidestep some of the manual management in R you can use a dedicated pipeline tool like the {targets} package in R. The package learns how your pipeline fits together, skips tasks that are already up-to-date, and runs only the necessary computation. {targets} can also visualize the progress of your workflow. Figure 6.2: A targets visualized workflow by rOpenSci Due to the added complexity of the {targets} package, we won’t include extensive examples of such a workflow but refer to the excellent documentation of the package for simple examples here. 6.2.3 Capturing your session state Often, code depends on various components, packages or libraries. These libraries and all software come in specific versions, which might or might not alter the behaviour of the code and the output it produces. If you want to ensure full reproducibility, especially across several years, you will need to capture the state of the system and libraries with which you ran the original analysis. In R the {renv} package serves this purpose and will provide an index of all the packages used in your project as well as their version. For a particular project, it will create a local library of packages with a static version. These static packages will not be updated over time, and therefore ensure consistent results. This makes your analysis isolated, portable, and reproducible. The analogue in Python would be the virtual environments, or venv program. When setting up your project you can run: # Initiate a {renv} environment renv::init() To initiate your static R environment. Whenever you want to save the state of your project (and its packages) you can call: # Save the current state of the environment / project renv::snapshot() To save any changes made to your environment. All data will be saved in a project description file called a lock file (i.e. renv.lock). It is advised to update the state of your project regularly, and in particular before closing a project. When you move your project to a new system, or share a project on github with collaborators, you can revert to the original state of the analysis by calling: # On a new system, or when inheriting a project # from a collaborator you can use a lock file # to restore the session/project state using renv::restore() NOTE: As mentioned in the {renv} documentation: “For development and collaboration, the .Rprofile, renv.lock and renv/activate.R files should be committed to your version control system. But the renv/library directory should normally be ignored. Note that renv::init() will attempt to write the requisite ignore statements to the project .gitignore.” We refer to 6.1 for details on github and its use. 6.2.4 Capturing a system state Although R projects and the use of {targets} make your workflow consistent, the package versions used between various systems (e.g., your home computer or the cluster at the university might vary). To address issues with changes in the versions of package, you can use the {renv} package which manages package version (environments) for you. When tasks are even more complex and include components outside of R, you can use Docker to provide containerization of an operating system and the included ancillary application. The {rocker} package provides access to some of these features within the context of reproducible R environments. Using these tools, you can therefore emulate the state of a machine, independently of the machine on which the docker file is run. These days, machine learning applications are often deployed as docker sessions to limit the complexity of installing required software components. The application of docker-based installs is outside the scope of the current course, but feel free to explore these resources as they are widespread in data science. 6.2.5 Readable reporting using Rmarkdown Within Rstudio, you can use Rmarkdown dynamic documents to combine both text and code. Rmarkdown is ideal for reporting, i.e., writing your final document and presenting your analysis results. A Rmarkdown document consists of a header that specifies document properties (whether it should be rendered as an html page, a docx file or a pdf), and the actual content. You have encountered RMarkdown already in Chapter 1.2.6. 6.2.6 Project structure In R projects, all files can be referenced relative to the top-most path of the project. When opening your_project.Rproj in RStudio, you can load data that is located in a sub-directory of the project directory ./data/ by read.table(\"./data/some_data.csv\"). The use of relative paths and consistent directory structures across projects, enables that projects can easily be ported across computers and code adopted across projects. project/ ├─ your_project.Rproj ├─ vignettes/ │ ├─ your_dynamic_document.Rmd ├─ data/ │ ├─ some_data.csv Rmarkdown files commonly reside in a sub-directory ./vignettes/ and are rendered relative to the file path where it is located. This means that to access data which resides in data/ using code in ./vignettes/your_dynamic_document.Rmd, we would have to write: data &lt;- read.table(&#39;../data/some_data.csv&#39;) Note the ../ to go one level up. To allow for more flexibility in file locations within your project folder, you may use the {here} package package. I gathers the absolute path of the R project and allows for a specification of paths inside scripts and functions that always start from the top project directory, irrespective of where the script that implements the reading-code is located. Therefore, we can just write: data &lt;- read.table(here::here(&#39;data/some_data.csv&#39;)) But why not use absolute paths to begin with? Portability! When I would run your \\*.Rmd file with an absolute path on my computer, it would not render as the file some_data.csv would then be located at: /my_computer/project/data/some_data.csv 6.2.6.1 Limitations of notebooks The file referencing issue and the common use of Rmarkdown, and notebooks in general, as a one size fits all solution, containing all aspects from data cleaning to reporting, implies some limitations. RMarkdown documents mix two cognitive tasks, writing text content (i.e. reporting) and writing code. Switching between these two modes comes with undue overhead. If you code, you should not be writing prose, and vise versa. If your R markdown file contains more code than it does text, it should be considered an R script or function (with comments or documentation). Conversely, if your RMarkdown file contains more text than code, it probably is easier to collaborate on a true word processing file (or cloud-based solution). Notebooks, such as RMarkdown, are most suitable for communicating implementations, demonstrating functions, and reporting reproducible results. They can also be used like lab notes. They are less suited for code development. 6.2.7 Data retention Coding practices and documenting all moving parts in a coding workflow is only one practical aspect of open science. An additional component is long-term data and code retention and versioning. In Chapter 7, you will learn more about git for code management and collaboration. Several online make use of git for providing web-based collaboration functionalities and remote storage of your repositories. Examples are GitHub, GitLab, Codeberg, or Bitbucket. However, their remote storage service should only be considered an aid for collaboration, and not a place to store code into perpetuity. Furthermore, these services mostly have a limit to how much data can be stored in a repository (mostly ~2 GB). For small projects, data can be included in the repository itself. For larger projects and for making larger datasets accessible, this won’t be possible. To ensure long-term storage of code and data, outside of commercial for profit services (e.g., Dropbox, Google Drive etc), it is best to rely on public permanent repositories, such as Zenodo. Zenodo is an effort by the European commission, but accessible to all, to facilitate archiving of science projects of all nature (code and data) up to 50 GB. In addition, Zenodo provides a citable digital object identifier or DOI. This allows data and code, even if not formally published in a journal, to be cited. Other noteworthy open science storage options include Dryad and the Center for Open Science. The broad-purpose permanent data repositories mentioned above are not edited and are therefore not ideal for data discovery. In contrast, edited data repositories often have a specific thematic scope and different repositories are established in different research communities. Below you find a list of widely used data repositories, generalist and others, that provide manual or automated download access to their data. Note that this list contains some example and is far from extensive. Data type Website Description Download Copernicus Climate Data Store https://cds.climate.copernicus.eu Freely available climate data (reanalysis as well as future projections) API Oak Ridge National Laboratories Digital Active Archive Center (ORNL DAAC) https://daac.ornl.gov/ Environmental data of varying sources, either remote sensing, field work and or re-analysis. multiple APIs or manual downloads Land Processes Digital Active Archive Center (LP DAAC) https://lpdaac.usgs.gov/ Remote sensing (analysis ready) data products. Login walled automated downloads Environmental Data Initiative https://edirepository.org/ Generalist data repository for study data, with a strong focus on biology. Manual download Dryad https://datadryad.org Generalist data repository for study data, with a strong focus on biology. Manual downloads Zenodo https://zenodo.org/ Generalist data repository for study data. Manual downloads Eurostat https://ec.europa.eu/eurostat Generalist data repository for EU wide (demographic) data. Manual downloads Swiss Open Government data https://opendata.swiss/en/ Generalist data repository from the Swiss Federal statistics office. API or manual downloads 6.3 Exercises External data You inherit a project folder which contains the following files. ~/project/ ├─ survey.xlsx ├─ xls conversion.csv ├─ xls conversion (copy 1).csv ├─ Model-test_1.R ├─ Model-test-final.R ├─ Plots.R ├─ Figure 1.png ├─ test.png ├─ Rplot01.png ├─ Report.Rmd ├─ Report.html ├─ my_functions.R What are your steps to make this project more reproducible? Write down how and why you would organize your project. A new project What are the basic steps to create a reproducible workflow from a file management perspective? Create your own R project using these principles and provide details the on steps involved and why they matter. The project should be a reproducible workflow: Download and plot a MODIS land cover map for Belgium using skills you learned in Chapter 5. Write a function to count the occurrences of land cover classes in the map as a formal function using skills you learned in Chapter 3. Create a plot of the land cover map, see Chapter 4. Write a dynamic report describing your answers to the above questions regarding how to structure a reproducible workflow. Tracking the state of your project Track the packages you use in the project you created using {renv}. Install any additional library and update the state of your project. Create a simple {targets} project using the above workflow Make changes to the API download routine. Rerun the targets project. "],["codemgmt.html", "Chapter 7 Code management 7.1 Learning objectives 7.2 Tutorial 7.3 Exercises 7.4 Report Exercises", " Chapter 7 Code management Chapter lead author: Koen Hufkens 7.1 Learning objectives In this chapter you will learn how to manage your code with common version control tools, i.e., git. You will learn how to: Create a git project (new or from a template) Track changes in your code project Collaborate with others Ensure reproducibility of your project by openly sharing your work and progress. 7.2 Tutorial Code management is key for managing any data science project, especially when collaborating. Proper code management limits mistakes, such as code loss, and increases efficiency by structuring projects. In this chapter, we will discuss the management of code in both the location sense, where things are kept, and tracking temporal changes over time using a version control system. Current version control of code is dominated by the software tool git. However, version control has a long history and can be found not only in code development practices. For example, whenever you use track changes in a text document, you apply a form of version control. That is, you track changes in your text over time and selectively accept changes. In this respect, git, as a tool for version control of code, does not differ much from track changes of a text document. In contrast to track changes in a text document, with git the user has manual control over staging and committing edits on a file. Figure 7.1: The git workflow- by Paola Corrales and Elio Campitelli Git allows for the collaboration of multiple contributors on the same code and manages the integration of contributions into the repository. Built upon git are cloud-based platforms such as GitHub, GitLab, Codeberg, or Bitbucket which make these collaborative decisions and operations even easier. Figure 7.2: The github remote workflow- by Paola Corrales and Elio Campitelli In this chapter, you will learn how to use git and GitHub to manage your project and collaborate on code. NOTE: Coding style, and documentation practices of the code itself have been covered previously in Chapter 2. Although the tutorial below focuses on GitHub, the jargon and operations are transferable to other platforms such as GitLab and Codeberg. 7.2.1 Git and local version control Git allows for the tracking of changes in code (or any file) within a git project. A git project is defined by the topmost directory in which a git project is created. For example, the following project is not tracked for changes using git. project/ ├─ YOUR_PROJECT.Rproj You can start tracking a project by initiating a local git repository using the following code in R. We’ll use the {usethis} package to make some of the setup a project easier. usethis::use_git() This will create a git repository in your project. It will also create a .gitignore file which specifies which files NOT to track (even if asked to). In addition it will make an first commit. 7.2.1.1 git add Before we can track anything, we need to tell git which files to track. In git-speak, we stage the files. We therefore have to add them to an index of tracked files. You can either do this on the command line using: git add your_file.csv Or using the RStudio Git panel. In this panel, you will see all un-tracked files or directories highlighted with a yellow question mark. Figure 7.3: Unstaged files in a git enabled R project. You select the file tick boxes to the left to stage all files for inclusion into the git repository. Once staged, the next step will be to finally commit these staged files to be included in git tracking. Figure 7.4: Staged files in a git enabled R project. 7.2.1.2 git commit To store any changes to the files which were staged we need to commit these changes. We therefore hit the commit button. A new window will pop up. Figure 7.5: Entering a commit message. Each commit needs a brief message describing what you have included in the staged files, or the commit message, as shown in the panel on the right. You need to provide this small message before pressing the commit button once more. This will let git track the changes to these files. A message will be shown if the commit is successful. Figure 7.6: A completed commit. With this, you will track all files locally. Any new changes to a file will need to be committed to the git repository once more. So, unlike cloud services such as Dropbox, your files are not automatically tracked. Instead, this is a manual step. As with normal documents, you are advised to save (commit) your changes to your project frequently. And remember, if you create a new file, you will need to add it before you can commit it. You can commit changes of staged files using the command line as well using the following command. git commit -m &quot;A message&quot;&quot; 7.2.2 Remote version control Keeping files and working with git locally limit the extent in which you can collaborate with others. This is where remote cloud-based git solutions, such as GitHub, GitLab and Codeberg, come in. They provide a cloud-based git repository which you can associate with your local project (see figure above). To create a remote project and successfully associate it with an R project, we first have to specify some details, such as the user name and email you used is singing up for GitHub. To not leave your R session, you can use the {usethis} package for this. # Configure your project library(usethis) usethis::use_git_config(user.name = &quot;Jane Doe&quot;, user.email = &quot;jane@example.org&quot;) For security reasons, the use of your GitHub password is not allowed in remote actions. You therefore need to generate a personal access token (PAT) which can be restricted in time and functionality. To proceed, first generate a GitHub PAT using these instructions. To create a new project on GitHub, hit the “+” sign top left on the GitHub main page (once logged in), and select the “new repository” from the dropdown menu. Figure 7.7: Create a new github repository A new interface will open up in which you should not use any template, but specify your own project name and brief description. Make sure your project is public, and all other settings are kept as is before you hit the “Create repository” button. Figure 7.8: Set the github project name Note the URL that is generated for your project. You will need it when creating a new RStudio project linked to GitHub. Figure 7.9: A repository link you need during the R project wizard Next, we’ll setup an R project which is associated with the repository. Use: File &gt; New Project &gt; Version Control &gt; Git. Figure 7.10: Linking the github project to a new git enabled R project In the “repository URL”, paste the URL of your new GitHub repository. In the example above, this would be https://github.com/khufkens/YOUR_PROJECT.git. Select a location where to store the project, select the “Open in new session” option and click “Create Project”. A window will pop up, asking for your GitHub username and a password. This password is not your GitHub login password but the PAT described above. After entering your credential, RStudio creates a \\*.Proj file as well as a .gitignore file (Section 7.2.1.1). You can add both files as you would otherwise (see Section 7.2.1.1), and these files are tracked locally. 7.2.2.1 git push Once a remote git service has been configured, you can push your local git repository to this remote repository, i.e. syncing both. You can use both the push buttons in the RStudio panel for this or the command linen using git push. At the end of a day or a session, it is always advised to push your changes to your remote repository to store any changes. Figure 7.11: Remote git workflow - by Paola Corrales and Elio Campitelli NOTE: Syncing between GitHub and your local repository is a manual task. If not performed the repository, it will not be synced. To retain all your changes, sync both repositories often! 7.2.2.2 git pull and merge conflicts git pull compares your local git repository with the remote one and integrates the more recent changes if there are any. Note that if you make changes on the same line in file on both sides, i.e., in the remote (e.g., by another person) and in the local repository at the same time, you will generate a merge conflict. A merge conflict states that the remote and local changes can’t be reconciled without supervised intervention on your part. Changes will be made to your local repository, but the files will include the below syntax for highlighting conflicting differences. &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; Edited line from remote ========== Edited same line locally &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; You will need to remove the &lt;&lt;&lt;, === and &gt;&gt;&gt; brackets and retain the changes you want to keep for resolving the conflict. Then, commit the changes again. 7.2.2.3 git clone You can create a local copy of your remote repository that’s hosted on GitHub using the git clone command. For example, on the command line, you can use: # create a local copy of the remote github repository git clone git@github.com:khufkens/YOUR_PROJECT.git You can then start working on this repository by using the modify -&gt; staged -&gt; commit -&gt; push workflow. 7.2.2.4 git fork and pull request You can also create a copy of any public GitHub project from into your own GitHub account by creating a fork. You can create a fork of a project by clicking the fork button top right on any public GitHub project page. The number of forks of a project is displayed next to the button. In case of the {rpmodel} package there are currently 24 forks of the project. Figure 7.12: Creating a fork of an existing project (rpmodel) You can give the forked project a new name and description if so desired. Figure 7.13: Github fork settings A fork allows you to experiment with the code stored in the original project without affecting the original repository. However, the relation to the original project is maintained. If you want to contribute changes to the original project you can do so with a pull request. NOTE: To make changes to a forked project, you will first have to clone it to your local system. See workflow above. In a forked project, go to the Pull requests tab and click the green New pull request button. You will then have to provide a description of the changes you made. This information will be forwarded to the original owner of the project, who can accept these changes and accept the pull request and “pull” in the changes. Figure 7.14: Creating a new github pull request 7.2.3 Location based code management - github templates Both code (and data) management require you to be conscientious about where you store your code (and data). Structuring your projects using the same template will allow you to understand where all pieces of an an analysis are stored. This has been mentioned in Chapter 6. In our R project template, we provide a project structure for both data and code which removes the mental overhead of structuring data projects. This project structure sorts code, data and reporting in a consistent way. You can use the template in combination with a GitHub-based version control approach to manage your projects. Simply create a new project from this template and clone the project to your local computer. Any changes to the project can be tracked by the workflows described above. To use the template, create a new repository on GitHub, as you otherwise would using the big green button. If you are in the project on GitHub, you can hit the green button top right (Use this template). Figure 7.15: Use a github project as a template Otherwise, you can select the repository from the template dropdown menu, select geco-bern/R-project-template. Figure 7.16: Using a new template based repository Proceed as usual by naming your repository. However, be careful to select the correct owner of the project if you have multiple identities. Rename the default .Proj file. Figure 7.17: Assigning a new template based repository You can now clone the project to your local computer and continue to populate it with code and data. 7.3 Exercises Location based code management Create a new R project using the git R project template shown above. Make some changes to the README.md Put a small data set in the appropriate directory. Make sure that both local and remote repositories (projects) are synced. 7.4 Report Exercises Collaborative Work on Github This is a team exercise, so team up with someone else in the classroom. You will learn about how to collaborate online using git and Github. Important: When creating your repositories, make sure that you set the repository to be public and not private. Person 1 - Create a new repository (can be the same as you created following the tutorial but should no be the same as the one where you hand in your report) Person 2 - Fork the GitHub project that Person 1 created in Step 1. Person 2 - Create a new file in this project Person 2 - Commit and push these changes to this project. Create a pull request to the original project of Person 1. Person 1 - Review the pull request from Person 2. Provide some comments, accept the pull request, letting it the new code by Person 2 be integrated into the project. Person 1 - Add a new file to your own project, and update the GitHub project. Person 2 - Sync your forked project to integrate the changes made by Person 1 into your own repository. Voluntary: Can you force a merge conflict, for example by editing the same file at once, and resolve? To complete the exercise, reverse rolls between Person 1 and Person 2. Deliverables for the report This pair-coding exercise is part of your final performance assessment. We will check each repositories’ commit history to see whether this pair-coding exercise was done correctly. So, follow the steps above precisely! When you submit your report by mail at the end of the course, you have to provide the links to your GitHub account, to your report repositories that holds all other report exercises, and to the two repositories that you created during this pair-coding work exercise (your repository that your friend forked and the repository that you forked from your friend). Alternatively you can also create a ./vignettes/re_paircoding.Rmd in your report repository, where you provide these links. "],["regressionclassification.html", "Chapter 8 Regression and classification 8.1 Learning objectives 8.2 Tutorial 8.3 Extra material 8.4 Exercises 8.5 Report Exercise", " Chapter 8 Regression and classification Chapter lead author: Pepa Aran 8.1 Learning objectives After completing this tutorial, you will be able to: Understand the basics of regression and classification models. Fit linear and logistic regression models in R. Choose and calculate relevant model performance metrics. Evaluate and compare regression models. Detect data outliers. Select best predictive variables. Contents of this Chapter are inspired and partly adopted by the excellent book by Boehmke and Greenwell. 8.2 Tutorial 8.2.1 Types of models Models try to explain relationships between variables through a mathematical formulation, particularly to predict a given target variable using other explanatory variables, also called predictors. Generally, we say that the target variable \\(Y\\) is a function (denoted \\(f\\)) of a set of explanatory variables \\(X_1, X_2, \\dots, X_p\\) and some model parameters \\(\\beta\\). Models can be represented as: \\[Y \\sim f(X_1, X_2, \\dots, X_p, \\beta)\\] This is a very general notation and depending on the structure of these components, we get to different modelling approaches. The first distinction comes from the type of target variable. Whenever \\(Y\\) is a continuous variable, we are facing a regression problem. If \\(Y\\) is categorical, we speak of classification. Regression Classification Target variable Continuous Categorical Common models Linear regression, polynomial regression, KNN, tree-based regression Logistic regression, KNN, SVM, tree classifiers Metrics RMSE, \\(R^2\\), adjusted \\(R^2\\), AIC, BIC Accuracy, precision, AUC, F1 8.2.2 Regression In this section, we will introduce the most basic regression model - linear regression. We will explain how to fit the model with R, how to include categorical predictors and polynomial terms. Finally, several performance metrics for regression models are presented. 8.2.2.1 Linear regression Theory Let’s start with the simplest model: linear regression. You probably have studied linear regression from a statistical perspective. Here, we will take a data-fitting approach. For example, we can try to explain the relationship between GPP and short wave radiation, like in Chapter 4. The figure below shows a cloud of data points, and a straight line predicting GPP based on observed shortwave radiation values. # read and format data from Ch 3 half_hourly_fluxes &lt;- readr::read_csv(&quot;./data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006_CLEAN.csv&quot;) set.seed(2023) plot_1 &lt;- half_hourly_fluxes |&gt; sample_n(2000) |&gt; # to reduce the dataset ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + geom_point(size = 0.75, alpha = 0.4) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, se = FALSE) + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() segment_points &lt;- data.frame(x0 = 332, y0 = 3.65, y_regr = 8.77) plot_1 + geom_segment(aes(x = x0, y = y0, xend = x0, yend = y_regr), data = segment_points, color = &quot;blue&quot;, lwd = 1.2, alpha = 0.8) To reproduce this code chunk, you can download the file FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006_CLEAN from here and read it from the local path where the file is stored on your machine. All data files used in this tutorials are stored here. We want to find the best straight line that approximates a cloud of data points. For this, we assume a linear relationship between a single explanatory variable \\(X\\) and our target \\(Y\\): \\[ Y_i \\sim \\beta_0 + \\beta_1 X_i, \\;\\;\\; i = 1, 2, ...n \\;, \\] where \\(Y_i\\) is the i-th observation of the target variable, and \\(X_i\\) is the i-th value of the (single) predictor variable. \\(n\\) is the number of observations we have and \\(\\beta_0\\) and \\(\\beta_1\\) are constant coefficients (model parameters). We call \\(\\beta_0\\) the intercept and \\(\\beta_1\\) the slope of the regression line. Generally, \\(\\hat{Y}\\) denotes the model prediction. Fitting a linear regression is finding the values for \\(\\beta_0\\) and \\(\\beta_1\\) such that, on average over all points, the distance between the line at \\(X_i\\), that is \\(\\beta_0 + \\beta_1 X_i\\) (blue segment in the plot above), and the observed value \\(Y_i\\), is as small as possible. Mathematically, this is minimizing the sum of the square errors, that is: \\[ \\min_{\\beta_0, \\beta_1} \\sum_i (Y_i - \\beta_0 - \\beta_1 X_i)^2 . \\] This linear model can be used to make predictions on new data, which are obtained by \\(\\hat{Y}_\\text{new} = \\beta_0 + \\beta_1 X_\\text{new}\\). When the new data comes from the same distribution as the data used to fit the regression line, this should be a good prediction. It’s not hard to imagine that the univariate linear regression can be generalized to a multivariate linear regression, where we assume that the target variable is a linear combination of \\(p\\) predictor variables: \\[Y \\sim \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\; ... \\; + \\beta_p X_p \\;.\\] Note that here, \\(X_1, \\dots, X_p\\) and \\(Y\\) are vectors of length corresponding to the number of observations in our data set (\\(n\\) - as above). Analogously, calibrating the \\(p+1\\) coefficients \\(\\beta_0, \\beta_1, \\beta_2, ..., \\beta_p\\) is to minimize the sum of square errors \\(\\min_{\\beta} \\sum_i (Y_i - \\hat{Y}_i)^2\\). While the regression is a line in two-dimensional space for the univariate case, it is a plane in three-dimensional space for bi-variate regression, and hyperplanes in higher dimensions. Implementation in R To fit a univariate linear regression model in R, we can use the lm() function. Already in Chapter 3, we created linear models by doing: # numerical variables only, remove NA df &lt;- half_hourly_fluxes |&gt; dplyr::select(-starts_with(&quot;TIMESTAMP&quot;)) |&gt; tidyr::drop_na() # fit univariate linear regression linmod1 &lt;- lm(GPP_NT_VUT_REF ~ SW_IN_F, data = df) Here, GPP_NT_VUT_REF is \\(Y\\), and SW_IN_F is \\(X\\). The formula notation GPP_NT_VUT_REF ~ SW_IN_F is common in R and can be used for a most functions in different packages. The (single) variable to the left of the ~ is the target variable (\\(Y\\)). The variable to its right is the predictor. Of course, we can include multiple predictors for a multivariate regression, for example as: # fit multivariate linear regression linmod2 &lt;- lm(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = df) The shorthand for including all predictors that are available in the data (here, all columns other than GPP_NT_VUT_REF in df), we can write: linmod3 &lt;- lm(GPP_NT_VUT_REF ~ ., data = df) linmod* is now a model object of class \"lm\". It is a list containing the following components: ls(linmod1) ## [1] &quot;assign&quot; &quot;call&quot; &quot;coefficients&quot; &quot;df.residual&quot; ## [5] &quot;effects&quot; &quot;fitted.values&quot; &quot;model&quot; &quot;qr&quot; ## [9] &quot;rank&quot; &quot;residuals&quot; &quot;terms&quot; &quot;xlevels&quot; Enter ?lm in the console for a complete documentation of these components and other details of the linear model implementation. R offers a set of generic functions that work with this type of object. The following returns a human-readable report of the fit. Here the residuals are the difference between the observed target values and the predicted values. summary(linmod1) ## ## Call: ## lm(formula = GPP_NT_VUT_REF ~ SW_IN_F, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -38.699 -2.092 -0.406 1.893 35.153 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.8732273 0.0285896 30.54 &lt;2e-16 *** ## SW_IN_F 0.0255041 0.0001129 225.82 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.007 on 41299 degrees of freedom ## Multiple R-squared: 0.5525, Adjusted R-squared: 0.5525 ## F-statistic: 5.099e+04 on 1 and 41299 DF, p-value: &lt; 2.2e-16 We can also extract coefficients \\(\\beta\\) with coef(linmod1) ## (Intercept) SW_IN_F ## 0.87322728 0.02550413 and the residual sum of squares (which we wanted to minimize) with sum(residuals(linmod1)^2) ## [1] 1035309 Although summary() provides a nice, human-readable output, you may find it unpractical to work with. A set of relevant statistical quantities are returned in a tidy format using tidy() from the broom package: broom::tidy(linmod1) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.873 0.0286 30.5 1.25e-202 ## 2 SW_IN_F 0.0255 0.000113 226. 0 Model advantages and concerns An advantage of linear regression is that the coefficients provide information that is straight-forward to interpret. We’ve seen above, that GPP_NT_VUT_REF increases by 0.0255 for a unit increase in SW_IN_F. Of course, the units of the coefficients depend on the units of GPP_NT_VUT_REF and SW_IN_F. This has the advantage that the data does not need to be normalised. That is, a linear regression model with the same predictive skills can be found, irrespective of whether GPP_NT_VUT_REF is given in (g C m\\(^{-2}\\)s\\(^{-1}\\)) or in (t C km\\(^{-2}\\) year\\(^{-1}\\)). Another advantage of linear regression is that it’s much less prone to overfit than other algorithms. You’ll learn more about the concept of overfitting in Chapter 9. Not being prone to overfitting can also be a disadvantage: linear regression models are often not flexible enough to be effectively fit to the data. They are also not able to capture non-linearities in the observed relationship and, as we’ll see later in this chapter, they often fit the data less well and generate poorer predictions than more complex models. A further limitation is that least squares regression requires \\(n&gt;p\\). In words, the number of observations must be greater than the number of predictors. If this is not given, one can resort to stepwise forward regression, where predictors are sequentially added based on which predictor adds the most additional information at each step. We will encounter stepwise regression in the exercises. When multiple predictors are linearly correlated, then linear regression cannot discern individual effects and individual predictors may appear statistically insignificant when they would be significant if covarying predictors were not included in the model. Such instability can get propagated to predictions. Again, stepwise regression can be used to remedy this problem. However, when one predictor covaries with multiple other predictors, this may not work. For many applications in Geography and Environmental Sciences, we deal with limited numbers of predictors. We can use our own knowledge to examine potentially problematic covariations and make an informed pre-selection rather than throwing all predictors we can possibly think of at our models. Such a pre-selection can be guided by the model performance on a validation data set (more on that in Chapter 10). An alternative strategy is to use dimension reduction methods. Principal Component regression reduces the data to capture only the complementary axes along which our data varies and therefore collapses covarying predictors into a single one that represents their common axis of variation. Partial Least Squares regression works similarly but modifies the principal components so that they are maximally correlated to the target variable. You can read more on their implementation in R here. 8.2.2.2 Regression on categorical variables In the regression within categories section of Chapter 4, we saw that when we separate the data into sub-plots, hidden patterns emerge. This information is very relevant for modeling, because it can be included in our regression model. It is crucial to spend enough time exploring the data before you start modeling, because it helps to understand the fit and output of the model, but also to create models that capture the relationships between variables better. So far, we have only used continuous variables as explanatory variables in a linear regression. It is also possible to use categorical variables. To do this in R, such variables cannot be of class numeric, otherwise the lm() function treats them as continuous variables. For example, although the variable NIGHT is categorical with values 0 and 1, the model linmod3 treats it as a number. We must make sure that categorical variables have class character or, even better, factor. # create month category df_cat &lt;- half_hourly_fluxes |&gt; mutate(MONTH = lubridate::month(TIMESTAMP_START)) |&gt; tidyr::drop_na() |&gt; dplyr::select(MONTH, GPP_NT_VUT_REF, SW_IN_F) # fix class of categorical variables df_cat &lt;- df_cat |&gt; mutate(MONTH = as.factor(MONTH)) Now we can fit the linear model again: linmod_cat &lt;- lm(GPP_NT_VUT_REF ~ MONTH + SW_IN_F, data = df_cat) summary(linmod_cat) ## ## Call: ## lm(formula = GPP_NT_VUT_REF ~ MONTH + SW_IN_F, data = df_cat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -36.212 -2.346 -0.223 2.200 34.416 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.6146109 0.0893693 18.067 &lt; 2e-16 *** ## MONTH2 -1.8105447 0.1294675 -13.985 &lt; 2e-16 *** ## MONTH3 -2.8800172 0.1264177 -22.782 &lt; 2e-16 *** ## MONTH4 -2.5667281 0.1278097 -20.082 &lt; 2e-16 *** ## MONTH5 -0.0288745 0.1273491 -0.227 0.820631 ## MONTH6 0.4614556 0.1298069 3.555 0.000378 *** ## MONTH7 0.1697514 0.1283830 1.322 0.186100 ## MONTH8 1.2942463 0.1231252 10.512 &lt; 2e-16 *** ## MONTH9 0.5140562 0.1165474 4.411 1.03e-05 *** ## MONTH10 -0.4807082 0.1152536 -4.171 3.04e-05 *** ## MONTH11 -1.3370277 0.1159059 -11.535 &lt; 2e-16 *** ## MONTH12 -1.2634451 0.1151530 -10.972 &lt; 2e-16 *** ## SW_IN_F 0.0246420 0.0001169 210.810 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.865 on 41288 degrees of freedom ## Multiple R-squared: 0.5776, Adjusted R-squared: 0.5775 ## F-statistic: 4704 on 12 and 41288 DF, p-value: &lt; 2.2e-16 In the fit summary, you can observe that, there are MONTH2 to MONTH12 parameters. MONTH is a factor which can take 12 different values: 1 to 12. lm() uses one of the factor level as the reference, in this case 1, and fits an intercept for the other categories. The result is a set of parallel regression lines, one for each different month. df_cat |&gt; mutate(MONTH_NAME = lubridate::month(as.integer(MONTH), label = TRUE)) |&gt; ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + geom_point(alpha = 0.2) + geom_smooth(formula = y ~ x + 0, method = &quot;lm&quot;, color = &quot;red&quot;, se = FALSE) + labs(x = &quot;SW&quot;, y = &quot;GPP&quot;) + facet_wrap(~MONTH_NAME) + theme_classic() In the grid image, we can observe that GPP does not increase with SW at the same rate every month. For example, the increase in GPP is less steep in February than in September. To model this, we should consider a variable slope parameter for each month or category. In R, this is implemented by including an interaction term MONTH:SW_IN_F in the regression formula, like this: linmod_inter &lt;- lm(GPP_NT_VUT_REF ~ MONTH + SW_IN_F + MONTH:SW_IN_F, data = df_cat) # equivalently: lm(GPP_NT_VUT_REF ~ MONTH * SW_IN_F, data = df_cat) summary(linmod_inter) ## ## Call: ## lm(formula = GPP_NT_VUT_REF ~ MONTH + SW_IN_F + MONTH:SW_IN_F, ## data = df_cat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -28.891 -2.113 -0.420 1.892 34.029 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.0449603 0.0944991 21.640 &lt; 2e-16 *** ## MONTH2 -1.5386938 0.1369424 -11.236 &lt; 2e-16 *** ## MONTH3 -1.5249304 0.1365863 -11.165 &lt; 2e-16 *** ## MONTH4 -1.0050639 0.1396023 -7.199 6.15e-13 *** ## MONTH5 -0.4502367 0.1412720 -3.187 0.00144 ** ## MONTH6 -1.2559057 0.1474257 -8.519 &lt; 2e-16 *** ## MONTH7 -0.8440097 0.1446838 -5.833 5.47e-09 *** ## MONTH8 -0.2188300 0.1346734 -1.625 0.10419 ## MONTH9 -1.3407190 0.1269387 -10.562 &lt; 2e-16 *** ## MONTH10 -0.9991456 0.1235627 -8.086 6.32e-16 *** ## MONTH11 -1.2124373 0.1230946 -9.850 &lt; 2e-16 *** ## MONTH12 -1.0724209 0.1210819 -8.857 &lt; 2e-16 *** ## SW_IN_F 0.0158600 0.0008758 18.110 &lt; 2e-16 *** ## MONTH2:SW_IN_F -0.0030373 0.0011518 -2.637 0.00837 ** ## MONTH3:SW_IN_F -0.0058229 0.0009713 -5.995 2.05e-09 *** ## MONTH4:SW_IN_F -0.0038333 0.0009469 -4.048 5.17e-05 *** ## MONTH5:SW_IN_F 0.0087370 0.0009305 9.389 &lt; 2e-16 *** ## MONTH6:SW_IN_F 0.0135219 0.0009172 14.743 &lt; 2e-16 *** ## MONTH7:SW_IN_F 0.0110791 0.0009182 12.066 &lt; 2e-16 *** ## MONTH8:SW_IN_F 0.0151014 0.0009317 16.209 &lt; 2e-16 *** ## MONTH9:SW_IN_F 0.0180496 0.0009297 19.415 &lt; 2e-16 *** ## MONTH10:SW_IN_F 0.0097277 0.0009761 9.966 &lt; 2e-16 *** ## MONTH11:SW_IN_F -0.0011415 0.0010932 -1.044 0.29640 ## MONTH12:SW_IN_F -0.0099745 0.0012972 -7.689 1.52e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.593 on 41277 degrees of freedom ## Multiple R-squared: 0.6237, Adjusted R-squared: 0.6234 ## F-statistic: 2974 on 23 and 41277 DF, p-value: &lt; 2.2e-16 8.2.2.3 Polynomial regression Furthermore, the relationships between variables may be non-linear. In the previous example, we see that the increase in GPP saturates as shortwave radiation grows, which suggests that the true relationship could be represented by a curve. There are many regression methods that fit this kind of relationship, like polynomial regression, LOESS (local polynomial regression fitting), etc. Let’s fit a simple quadratic regression model, just for the month of August. For this we use the poly() function which constructs orthogonal polynomials of a given degree. Here, a second-order polynomial (a parabola) is fitted: quadmod &lt;- lm(GPP_NT_VUT_REF ~ poly(SW_IN_F, 2), data = df_cat |&gt; filter(MONTH == 8)) summary(quadmod) ## ## Call: ## lm(formula = GPP_NT_VUT_REF ~ poly(SW_IN_F, 2), data = filter(df_cat, ## MONTH == 8)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -26.367 -2.055 -0.253 1.801 32.375 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.13084 0.07944 89.77 &lt;2e-16 *** ## poly(SW_IN_F, 2)1 447.25113 4.61907 96.83 &lt;2e-16 *** ## poly(SW_IN_F, 2)2 -151.08797 4.61907 -32.71 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.619 on 3378 degrees of freedom ## Multiple R-squared: 0.7556, Adjusted R-squared: 0.7555 ## F-statistic: 5223 on 2 and 3378 DF, p-value: &lt; 2.2e-16 In the following plot, you can see how the model fit for GPP in August improves as we consider higher degree polynomials: df_cat |&gt; filter(MONTH == 8) |&gt; ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + geom_point(alpha = 0.4) + geom_smooth(formula = y ~ x, method = &quot;lm&quot;, aes(color = &quot;lm&quot;), se = FALSE) + geom_smooth(formula = y ~ poly(x, 2), method = &quot;lm&quot;, aes(color = &quot;poly2&quot;), se = FALSE) + geom_smooth(formula = y ~ poly(x, 3), method = &quot;lm&quot;, aes(color = &quot;poly3&quot;), se = FALSE) + labs(x = &quot;SW&quot;, y = &quot;GPP&quot;, color = &quot;Regression&quot;) + theme_classic() 8.2.2.4 Metrics for regression evaluation Metrics measure the quality of fit between predicted and observed values. Metrics are essential to model fitting, model selection, and for describing and quantifying patterns in the data. Metrics are also key for guiding the training of machine learning models, as you will learn in Chapter 10. Different metrics measure different aspects of the model-data agreement. In other words, a single metric never captures all aspects and patterns of the model-data agreement. Therefore, a visual inspection of the model fit is always a good start of the model evaluation. To get an intuitive understanding of the different abilities of different metrics, compare the scatterplots in Fig. 8.1 and how different aspects of the model-data agreement are measured by different metrics. The observed target values \\(Y\\) are plotted against the predicted values \\(\\hat{Y}\\) from a regression model, and the dashed line represents the ideal fit: predictions matching the data perfectly. Definitions of the metrics displayed and other metrics are given below. Figure 8.1: Correlation plots between observed and fitted target values. Common metrics used for evaluating regression fits are: MSE The mean squared error is defined, as its name suggests, as: \\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (Y_i - \\hat{Y_i})^2 \\]It measures the magnitude of the errors, and is minimized to fit a linear regression or, as we will see in Chapter 9, during model training when used as a loss function. Note that since it scales with the square of the errors, the MSE is sensitive to large errors in single points, including outliers. RMSE The root mean squared error is, as its name suggests, the root of the MSE: \\[ \\text{RMSE} = \\sqrt{\\text{MSE}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (Y_i - \\hat{Y_i})^2} \\]Like the MSE, the RMSE also measures the magnitude of the errors and is minimized during model training. By taking the square root of mean square errors, the RMSE is in the same units as the data \\(Y\\) and is less sensitive to outliers than the MSE. MAE The mean absolute error is similarly defined: \\[ \\text{MAE} = \\frac{1}{n} \\sum_{i = 1}^{n} |Y_i - \\hat{Y_i}| \\] Measuring the discrepancies between predictions and observations using absolute errors, instead of squared errors, gives less importance to errors of large magnitude and more importance to small errors than the MSE would. Hence, this measures is more stable in the presence of outliers. \\(R^2\\) - coefficient of determination describes the proportion of variation in \\(Y\\) that is captured by modelled values \\(\\hat{Y}\\). It tells us how much better our fitted values \\(\\hat{Y}\\) are than just taking the average of the target \\(\\bar{Y}\\) as predictions. In this case, the goal is to maximize the metric, thus trying the explain as much variation as possible. In contrast to the MSE and RMSE, \\(R^2\\) measures consistency, or correlation, or goodness of fit. It is defined as: \\[ R^2 = 1 - \\frac{\\sum_i (\\hat{Y}_i - Y_i)^2}{\\sum_i (Y_i - \\bar{Y})^2}\\\\ \\] When the regression model is fitted by minimizing the MSE, the \\(R^2\\) takes values between 0 and 1. A perfect fit is quantified by \\(R^2 = 1\\). There is no generally valid threshold of \\(R^2\\) for a model to be considered “good”. It depends on the application and the nature of the data and the data-generating process. Note that the above equation can also be written as \\(R^2 = 1 - \\text{MSE}/var(Y)\\). \\(r\\) - Pearson’s correlation The linear association between two variables (here \\(Y\\) and \\(\\hat{Y}\\)) is measured by the Pearson’s correlation coefficient \\(r\\). \\[ r = \\frac{\\sum_i (Y_i - \\bar{Y}) (\\hat{Y_i} - \\bar{\\hat{Y}}) }{\\sqrt{ \\sum_i(Y_i-\\bar{Y})^2 \\; (\\hat{Y_i}-\\bar{\\hat{Y}})^2 } } \\] The correlation calculated between the target \\(Y\\) and a predictor \\(X\\) can tell us about the predictive power of a regression model (the higher the correlation, the more powerful). We can also compute the correlation between the target \\(Y\\) and the predicted values \\(\\hat{Y}\\) by a model (multivariate, or even not linear) to assess the adequacy of the model chosen. See Figure 8.1 as an example. It is noteworthy to mention that correlation is location and scale invariant, hence it will not detect model deviations like the ones in the middle row plots. The squared value of the Pearson’s r is often reported as “\\(R^2\\)” but is not equivalent to the definition of the coefficient of determination given above. However, the square of the Pearson’s r is closely related to the coefficient of determination \\(R^2\\). For a linear regression, fitted by minimizing the MSE, they are identical (see proof here). In subsequent chapters, we will use “\\(R^2\\)” to refer to the square of the Pearson’s r between the observed \\(Y\\) and predicted \\(\\hat{Y}\\) values. Note the implementations in R.The \\(R^2\\) reported by the generic summary() function corresponds to the base-R function cor()^2 , to yardstick::rsq(), and to the definition of the square of the Pearson’s \\(r\\) given above. The yardstick::rsq_trad() returns the coefficient of determination as traditionally defined and is not equal to the values above, unless computed on the predicted values \\(\\hat{Y}\\). # generate correlated random data set.seed(1982) df &lt;- tibble(x = rnorm(100)) |&gt; mutate(y = x + rnorm(100)) |&gt; mutate(y_fitted = lm(y ~ x)$fitted.values) # implementations using Pearson&#39;s correlation summary(lm(y ~ x, data = df))$r.squared ## [1] 0.6186521 cor(df$y, df$x)^2 # remember: location and scale invariant ## [1] 0.6186521 yardstick::rsq(df, y, x) |&gt; pull(.estimate) ## [1] 0.6186521 (sum((df$x - mean(df$x))*(df$y - mean(df$y))))^2/ (sum((df$y - mean(df$y))^2)*sum((df$x - mean(df$x))^2)) ## [1] 0.6186521 # implementations using coefficient of determination definition 1 - sum((df$x - df$y)^2) / sum((df$y - mean(df$y))^2) # should be \\hat{y}, not x ## [1] 0.5993324 yardstick::rsq_trad(df, y, x) |&gt; pull(.estimate) # incorrect ## [1] 0.5993324 yardstick::rsq_trad(df, y, y_fitted) |&gt; pull(.estimate) # correct ## [1] 0.6186521 An “\\(R^2\\)” is commonly reported when evaluating the agreement between observed and predicted values of a given model. When the correlation between two different variables in a sample is quantified, \\(r\\) is commonly used to reflect also whether the correlation is positive or negative (\\(r\\) can attain positive or negative values in the interval \\([-1, 1]\\)). The coefficient of determination can return negative values when comparing observed and predicted values for uninformative estimates (worse than just using the average of \\(Y\\)) and is thus not actually bound between 0 and 1. Therefore, be careful with the interpreration of “\\(R^2\\)” and think on which variables it was computed and with which method. Sometimes, the Person’s version is computed between \\(Y\\) and \\(x\\), and it leads to the same number due to its “location and scale invariant” property. Nevertheless, this is conceptually wrong, as we should look at the predictions, not the predictors: We are not predicting \\(Y\\) by just giving the values of \\(x\\) instead. Hence, especially when using {yardstick} functions, make sure you compute the values on \\(\\hat{Y}\\). When we have several predictors, it’s already clear that we should compare \\(Y\\) to \\(\\hat{Y}\\) instead of \\(Y\\) to each predictor separately. Bias The bias is simply the mean error: \\[ \\text{bias} = \\frac{1}{n} \\sum_i^n{(\\hat{Y}_i - Y_i)} \\] Slope The slope refers to the slope of the linear regression line between predicted and observed values. It is returned as the second element of the vector returned by coef(lm(..)): coef(lm(y ~ y_fitted, data = df))[2] ## y_fitted ## 1 8.2.2.5 Metrics for regression model comparison In general, the aim of regression modelling is to find a model that best explains the data - but not the random errors in the data. More complex models tend to overfit more than simpler models. The implication of overfitting is that the model fits the data used for model fitting well, but doesn’t fit well when evaluating the predictions of the same model to new data (data not used for model fitting). In such a case, the model’s generalisability is poor. We’ll learn more about overfitting and generalisability in the context of supervised machine learning in later chapters. Often, simpler models generalise better than more complex model. The challenge is to strike a balance between complexity and generalisability. But how to find the “sweet spot” of this trade-off? In this context it should be noted that the \\(R^2\\) always increases when predictors are added to a model. Therefore, the \\(R^2\\) is not a suitable metric for comparing models that differ with respect to their number of predictors - a factor controlling model complexity. Cross-validation can be regarded as the “gold-standard” for measuring model generalisability if the data is plentiful. It will be introduced in the context of supervised machine learning in Chapter 10. However, when the data size is small, cross validation estimates may not be robust. Without resorting to cross validation, the effect of spuriously improving the evaluation metric by adding uninformative predictors can also be mitigated by penalizing the number of predictors \\(p\\). Different metrics are available: Adjusted \\(R^2\\) The adjusted \\(R^2\\) discounts values of \\(R^2\\) by the number of predictors. It is defined as \\[ {R}^2_{adj} = 1 - (1-R^2) \\; \\frac{n-1}{n-p-1} \\;, \\] where \\(n\\) (as before) is the number of observations, \\(p\\) the number of parameters and \\(R^2\\) the usual coefficient of determination. Same as for \\(R^2\\), the goal is to maximize \\(R^2_{adj}\\). AIC The Akaike’s Information Criterion is defined in terms of log-likelihood (covered in Quantitative Methoden) but for linear regression it can be written as: \\[ \\text{AIC} = n \\log \\Big(\\frac{\\text{SSE}}{n}\\Big) + 2(p+2) \\] where \\(n\\) is the number of observations used for estimation, \\(p\\) is the number of explanatory variables in the model and SSE is the sum of squared errors (SSE\\(= \\sum_i (Y_i-\\hat{Y_i})^2\\)). Also in this case we have to minimize it and the model with the minimum value of the AIC is often the best model for generalisations to new data. Since it penalizes having many parameters, it will favor less complex models. AIC\\(_c\\) For small values of \\(n\\) the AIC tends to select too many predictors. A bias-corrected version of the AIC is defined as: \\[ \\text{AIC}_c = \\text{AIC} + \\frac{2(p + 2)(p + 3)}{n-p-3} \\] Also AIC\\(_c\\) is minimized for an optimal predictive model. BIC The Schwarz’s Bayesian Information Criterion is defined as \\[ \\text{BIC} = n \\log \\Big(\\frac{\\text{SSE}}{n}\\Big) + (p+2) \\log(n) \\] Also for BIC, the goal is to minimize it. This metric has the feature that if there is a true underlying model, the BIC will select that model given enough data. The BIC tends to select a model with fewer predictors than AIC. Implementation in R Let’s calculate the metrics introduced above for a few of the fitted regression models. Some of these metrics, like \\(R^2\\) and \\(R^2_{adj}\\) are given by the summary() function. Alternatively, the {yardstick} package provides implementations for a few of these metrics, which we compute below: compute_regr_metrics &lt;- function(mod){ p &lt;- length(mod$coefficients) n &lt;- length(mod$residuals) tibble( mse = mean(mod$residuals^2), R2 = summary(mod)$r.squared, R2_adj = summary(mod)$adj.r.squared, AIC = extractAIC(mod)[2], AIC_adj = extractAIC(mod)[2] + 2*(p+2)*(p+3)/(n-p-3), BIC = BIC(mod) # this implementation is based on log-likelihood ) } list_metrics &lt;- purrr::map( list(linmod1, linmod2, linmod_cat, quadmod), ~compute_regr_metrics(.)) names(list_metrics) &lt;- c(&quot;Linear model&quot;, &quot;Linear model 2&quot;, &quot;Linear + categories&quot;, &quot;Quadratic model&quot;) bind_rows(list_metrics, .id = &quot;type&quot;) ## # A tibble: 4 × 7 ## type mse R2 R2_adj AIC AIC_adj BIC ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Linear model 25.1 0.553 0.553 133058. 133058. 250293. ## 2 Linear model 2 24.8 0.558 0.558 132590. 132590. 249842. ## 3 Linear + categories 23.7 0.578 0.577 130700. 130700. 248030. ## 4 Quadratic model 21.3 0.756 0.755 10350. 10350. 19972. 8.2.3 Model selection Model selection refers to several techniques that help you compare models and select the one that best explains the data. Here, concepts will be explained using regression as an example, but are directly translated to classification problems. First, be systematic with model comparisons. Three key ideas in model selection are: Comparisons should be hierarchical: compare a model to another that “contains it”, i.e. compare y ~ x1 to y ~ x1 + x2, and not y ~ x1 to y ~ x2 + x3. Complexity must be increased slowly: add one variable at a time, not three variables all at once. This helps avoid collinearity in the predictors. Choose the most appropriate metric: if possible, a metric that accounts for model complexity and represents the goal of your analysis (e.g., recall for a classification where you don’t want to miss any positives). If you’re considering different model approaches for the same task, you should first fit the best possible model for each approach, and then compare those optimized models to each other. For example, fit the best linear regression with your available data, the best KNN non-parametric regression model and a random forest. Then compare those three final models and choose the one that answers your research question the best. One must be careful not to keep training or improving models until they fit the data perfectly, but maintain the models’ ability to generalize to newly available data. Chapter 9 introduces the concept of overfitting, which is central to data science. Think of model interpretation and generalization when comparing them, not only of performance. Simple models can be more valuable than very complex ones because they tell a better story about the data (e.g., by having few very good predictors rather than thousands of mediocre ones, from which we cannot learn the underlying relationships). 8.2.3.1 Variable selection Let’s think of variable selection in the context of linear regression. A brute force approach to variable selection would be: Fit a linear regression for each combination of all available predictors, calculate a metric (e.g., AIC) and choose the best one (lowest AIC). The problem is, if you have, say, eight predictors, you would fit 40320 different regression models. This can be very computationally expensive. Instead, take a hierarchical, or “greedy”, approach, starting with an empty model (just an intercept) and adding one variable at a time. This is called stepwise forward regression. The algorithm goes as follows: Set the number of predictors to be considered to \\(p=1\\). Fit all regression models with \\(p\\) predictors and compute their \\(R^2\\). Select the model with \\(p\\) predictors that achieves the highest \\(R^2\\) (best fitting model) and compute its AIC. Increment to \\(p+1\\). Fit all regression models with \\(p+1\\) predictors that include the predictor selected at the previous step and compute their \\(R^2\\). Select the best fitting model and compute its AIC. If the AIC of the model with \\(p+1\\) predictors is poorer than the AIC of the model with \\(p\\) predictors, retain the model with \\(p\\) predictors and quit. You have found the (presumably) optimal model. Otherwise, continue with with step 4. Instead of the AIC, you may also use the BIC for model comparison. Note that algorithm doesn’t consider all possible combinations of predictors and it is possible that the globally optimal model is thus not found. The function stats::step() implements the stepwise algorithm in R. This stepwise approach can also be done backwards, starting with a full model (all available variables) and removing one at a time. Or even with a back-and-forth approach, where you look at both including a new or removing an existing variable at each step (optimizing AIC). Furthermore, this algorithm can be applied to fitting a polynomial regression whereby the aim si to increase the degree of the polynomials - step by step. For a model with categorical variables, interaction terms should only be considered after having the involved variables as “intercept only”. Multicollinearity exists when there is a correlation between multiple predictors in a multivariate regression model. This is problematic because it makes the estimated coefficients corresponding to the correlated variables unstable. Since correlated variables contain similar information, it doesn’t matter whether we include one or the other in the model or even if we include both of them. The performance metrics will be similar. Hence, it becomes difficult to conclude which variables actually influence the target. The variance inflation factor (VIF) is a score from economics that measures the amount of multicollinearity in regression based on how the estimated variance of a coefficient is inflated due to its correlation with another predictor. It’s calculated as \\[\\text{VIF}_j = \\frac{1}{1 - R^2_j},\\] where \\(R^2_j\\) is the coefficient of determination for regressing the \\(j^{th}\\) predictor on the \\(p-1\\) remaining predictors. More specifically, \\[R^2_j = 1 - \\frac{\\sum_i (X_{j, i} - \\hat{X}_{j, i})^2}{\\sum_i (X_{j, i} - \\bar{X}_j)^2},\\] where \\(\\hat{X}_{j, i}\\) is the fitted value corresponding to the regression \\(X_j \\sim X_1 + ... X_{j-1} + X_{j+1} + ... + X_p\\) for the \\(i^{th}\\) observation. A VIF\\(_j\\) is computed for each of the \\(p\\) predictors in the multivariate regression model we are evaluating, and their values interpreted to detect multicollinearity. Meaning: if \\(\\text{VIF}_j = 1\\) variables are not correlated; if \\(1 &lt; \\text{VIF}_j &lt; 5\\) there is moderate collinearity between \\(X_j\\) and the rest of predictors; and if \\(\\text{VIF}_j \\geq 5\\) they are highly correlated. Because variable \\(X_{j}\\) can be almost fully explained by all the other predictors (high \\(R^2_j\\)), this variable is redundant in our final model. To remedy collinearity, you may choose to use only one or two of those correlated variables. Another option would be to use Principal Component Analysis (PCA), which you may read more about here. What PCA does is to map the space of predictors into another space of smaller dimension, leading to a smaller set of predictor variables \\(\\{Z_1, ... , Z_q\\}\\), each of them being a linear combination of all the initial available predictors, that is \\(Z_1 = \\alpha^1_0 + \\alpha^1_1 X_1 + ... + \\alpha^1_p X_p\\), etc. If you have collinearity, those highly correlated variables would be summarized into one single new variable, called principal component. When we work with high-dimensional data (that is, we have more variables than observations) there are better techniques to do variable selection than stepwise regression. Since the predictors space is so large, we could fit a line that passes through all the observations (a perfect fit), but does the model generalize? We don’t know. For example, Lasso and Ridge regression incorporate variable selection in the fitting process (you can check this post if you’re curious). 8.2.4 Outlier detection Detecting outliers is important, because they can affect the fit of a model and render the model fitting not robust. When the data is large, individual points have less influence on the model fitting. Therefore, only outliers that are very far from normal values will affect the model fit (see below). Outliers are particularly problematic in the context of small data, because every value has a big influence on the fitted model. Take a look at the two linear regressions below and how one single point can strongly influence the fit. the added point (red circle around black dot) lies clearly outside the “cloud” of remaining points and doesn’t seem to follow the same pattern in the data. set.seed(2023) half_hourly_fluxes_small &lt;- half_hourly_fluxes |&gt; sample_n(100) |&gt; # reduce dataset select(SW_IN_F, GPP_NT_VUT_REF) plot_3 &lt;- half_hourly_fluxes_small |&gt; ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + geom_point(size = 0.75) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, fullrange = TRUE) + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (&quot;, mu,&quot;mol CO&quot;[2], &quot; m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() + ylim(-20, 40) + xlim(0, 1100) plot_4 &lt;- half_hourly_fluxes_small |&gt; add_row(SW_IN_F = 1100, GPP_NT_VUT_REF = -20) |&gt; # add outlier ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) + geom_point(size = 0.75) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, fullrange = TRUE) + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (&quot;, mu,&quot;mol CO&quot;[2], &quot; m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() + geom_point(aes(x = 1100, y = -20), colour = &#39;red&#39;, shape = 1, size = 3) + ylim(-20, 40) + xlim(0, 1100) cowplot::plot_grid(plot_3, plot_4) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; Figure 8.2: The influence of an outlier on a regression fit. The first step to identifying outliers is usually to look at the data, one variable at a time. Plot a histogram to check the distribution of a variable. The shape of the distribution is very informative for what can be considered an outlier. In Chapters 3 and 4 it was introduced how to identify values that fall “outside” a distribution using histograms and boxplots. Checking in the histogram if the distribution has fat tails helps to discern whether the values that pop out of a boxplot should be considered outliers or not. # create an outlier for demonstration purposes half_hourly_fluxes_outlier &lt;- half_hourly_fluxes_small |&gt; add_row(SW_IN_F = 1100, GPP_NT_VUT_REF = -20) # Various ways to identify the outlier using graphs plot_5 &lt;- ggplot( data = half_hourly_fluxes_outlier, aes(x = GPP_NT_VUT_REF, y = after_stat(density))) + geom_histogram(fill = &quot;grey70&quot;, color = &quot;black&quot;) + geom_density(color = &#39;red&#39;)+ labs(title = &#39;Histogram, density and boxplot&#39;, x = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() plot_6 &lt;- ggplot( data = half_hourly_fluxes_outlier, aes(x = &quot;&quot;, y = GPP_NT_VUT_REF)) + geom_boxplot(fill = &quot;grey70&quot;, color = &quot;black&quot;) + coord_flip() + theme_classic() + theme(axis.text.y=element_blank(), axis.ticks.y=element_blank()) + labs(y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) plot_7 &lt;- ggplot( data = half_hourly_fluxes_outlier, aes(x = SW_IN_F, y = after_stat(density))) + geom_histogram(fill = &quot;grey70&quot;, color = &quot;black&quot;) + geom_density(color = &#39;red&#39;)+ labs(title = &#39;Histogram, density and boxplot&#39;, x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;))) + theme_classic() plot_8 &lt;- ggplot( data = half_hourly_fluxes_outlier, aes(x = &quot;&quot;, y = SW_IN_F)) + geom_boxplot(fill = &quot;grey70&quot;, color = &quot;black&quot;) + coord_flip() + theme_classic() + theme(axis.text.y=element_blank(), axis.ticks.y=element_blank()) + labs(y = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;))) cowplot::plot_grid(plot_5, plot_7, plot_6, plot_8, ncol = 2, rel_heights = c(2,1), align = &#39;v&#39;, axis = &#39;lr&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. A Q-Q Plot depicts the sample quantiles of a variable against the theoretical quantiles of a distribution that is assumed to uderly the data. In the histograms above, GPP looks somewhat Gaussian (normally distributed) but with fatter tails and slightly skewed to the right, while shortwave radiation is clearly skewed to the right, resembling an exponential distribution. The Q-Q plots below reveal that both variables are clearly not normally distributed. plot_9 &lt;- ggplot( data = half_hourly_fluxes_outlier, aes(sample = GPP_NT_VUT_REF)) + geom_qq() + geom_qq_line() + labs(y = expression(paste(&quot;GPP (gC m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)), x = &quot;Theoretical normal quantiles&quot;) + theme_classic() plot_10 &lt;- ggplot( data = half_hourly_fluxes_outlier, aes(sample = SW_IN_F)) + geom_qq() + geom_qq_line() + labs(y = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), x = &quot;Theoretical normal quantiles&quot;) + theme_classic() cowplot::plot_grid(plot_9, plot_10, ncol = 2) QQ-plots serve to investigate whether the data follows an assumed theoretical distribution. For linear and logistic regression, we would like predictor variables to look as normal, i.e. Gaussian, as possible. You’ve probably learned some of the reasons for this in the Quantitative Methods course (or a similar classical Statistics course), and we will not discuss this further here. Note however, that neither the histograms, nor the boxplots, or the QQ-plots revealed any evidently suspicious pattern of the added point (shortwave radiation = 1100 W m\\(^{-2}\\) s\\(^{-1}\\), and GPP = -20 \\(\\mu\\)mol CO\\(_2\\) m\\(^{-2}\\) s\\(^{-1}\\)) compared to the individual distributions of the two variables. Yet, in Fig. 8.2, the influence of the outlying point on the fit was clear. Because the suspicious observation is off the multivariate pattern in the remaining data, it becomes very influential. That is, it has a big leverage. R provides some useful plots from the fitted regression objects, in particular the “Residuals vs. Leverage” plot: # Fit regression with outlier linmod_outlier &lt;- lm(GPP_NT_VUT_REF ~ SW_IN_F, data = add_row(half_hourly_fluxes_small, SW_IN_F = 1100, GPP_NT_VUT_REF = -20)) plot(linmod_outlier, 5) This plot shows the leverage (see the mathematical definition here) of each observation against the corresponding residual from the fitted linear regression. Points with high leverage, i.e., points that are far from the center of the predictor distribution, and large residuals, i.e., points that are far from the fitted regression line, are very influential. The Cook’s distance (definition here) is an estimate of the influence of a data point in a linear regression and observations with Cook’s distance &gt; 1 are candidates for being outliers. See in the plot above how the point with index 101 (our added outlier) has a very large Cook’s distance. Boundary regions for Cook’s distance equal to 0.5 (suspicious) and 1 (certainly influential) are drawn with a dashed line. Finally, it’s very important that, before you remove a value because it may be an outlier, you understand where the data came from and if such an abnormal observation is possible. If it depicts an extraordinary but possible situation, this information can be very valuable and it’s wiser to keep it in the model. Interesting research questions arise when data doesn’t align with our preconceptions, so keep looking into it and potentially collect more data. 8.3 Extra material 8.3.1 Classification Classification models predict a categorical target variable. Note that predictors of a classification model may still be, and often are, continuous. We will introduce a classification problem with a binary target, since it’s straightforward to generalize to categorical variables with more than two classes. As an example, we use the CO2 dataset from the {datasets} package, which contains data from an experiment on the cold tolerance of the grass species Echinochloa crus-galli. Figure 8.3: Echinochloa crus-galli, image from swbiodiversity.org We will try to classify the origin of each plant (categorical variable Type with values Quebec or Mississippi) depending on the carbon dioxide uptake rate of the plant (continuous variable uptake measured in \\(\\mu\\)mol m\\(^{-2}\\)s\\(^{-1}\\)). More information on the dataset can be obtained by typing ?datasets::CO2 in the console. datasets::CO2 |&gt; ggplot(aes(x = uptake, y = Type, color = Type)) + geom_point(size = 3, alpha = 0.5) + theme_classic() + labs(x = expression(paste(&quot;Uptake (&quot;, mu,&quot;mol CO&quot;[2], &quot; m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme(legend.position = &quot;none&quot;) At first sight, it’s easy to see that the carbon uptake is lower for the Mississippi type. Note that other predictors can be included in the model, but we’ll focus on a single predictor. Using this example, we’ll cover logistic regression, its implementation in R and metrics for classification. 8.3.1.1 Logistic regression Theory A classification problem is a bit more difficult to write mathematically than a regression problem. Before, the mathematical representation of GPP_NT_VUT_REF ~ SW_IN_F was GPP_NT_VUT_REF\\(\\;=\\; \\beta_0 + \\beta_1\\)SW_IN_F. With the classification model Type ~ uptake, we cannot just write Type\\(\\;=\\; \\beta_0 + \\beta_1\\)uptake because Type is not a number. Hence, the categorical variable must be encoded, in this case 0 represents Quebec and 1 represents Mississippi. The next issue is that a linear model makes continuous predictions in the entire real numbers space \\((-\\inf, \\inf)\\), but we want the predictions to be either 0 or 1. We can transform these values to be in the interval \\([0,1]\\) with a link function. For a binary response, it’s common to use a logit link function: \\[\\text{logit}(z) = \\frac{\\exp(z)}{1+\\exp(z)}.\\] ggplot() + geom_function(fun = function(x) exp(x)/(1 + exp(x))) + xlim(-5, 5) + labs(y = &quot;logit(x)&quot;, x = &quot;x&quot;) + theme_classic() Combining a linear model (with any type of predictors, like for regression) and a logit link function, we arrive at the logistic regression model: \\[f(X, \\beta) = \\text{logit}(\\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p) = \\frac{\\exp(\\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p)}{1 + \\exp(\\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p)}. \\] This predicted value can be understood as the probability of belonging to class 1 (in our example, Mississippi). A classification rule is defined such that an observation \\(X_{new}\\) with a predicted probability of belonging to class 1 higher than a given threshold \\(\\tau\\) (i.e. \\(f(X_{new}, \\beta) &gt; \\tau\\)) will be classified as 1. If the predicted probability is smaller than the threshold \\(\\tau\\), it will be classified as 0. A logistic regression model results in a linear classification rule. This means that the \\(p\\)-dimensional space will be divided in two by a hyperplane, and the points falling in each side of the hyperplane will be classified as 1 or 0. In the example above with carbon uptake as predictor, the classification boundary would be a point dividing the real line. If we include a second predictor, we would obtain a line dividing the 2-dimensional plane in two. Furthermore, to fit a logistic regression model means to calculate the maximum likelihood estimator of \\(\\beta\\) with an iterative algorithm. We will learn more about iterative model fitting, i.e. parameter optimization, in the context of supervised machine learning (Chapter 10). Implementation in R First, let’s see how the target variable is encoded. R directly loads the dataframe with Type as a factor and Quebec as the reference level. R factors work such that each factor level (here Quebec and Mississippi) corresponds to an integer value (its position given by levels(), here 1 and 2 respectively). We can fit a logistic model in R with this encoding. str(datasets::CO2) ## Classes &#39;nfnGroupedData&#39;, &#39;nfGroupedData&#39;, &#39;groupedData&#39; and &#39;data.frame&#39;: 84 obs. of 5 variables: ## $ Plant : Ord.factor w/ 12 levels &quot;Qn1&quot;&lt;&quot;Qn2&quot;&lt;&quot;Qn3&quot;&lt;..: 1 1 1 1 1 1 1 2 2 2 ... ## $ Type : Factor w/ 2 levels &quot;Quebec&quot;,&quot;Mississippi&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Treatment: Factor w/ 2 levels &quot;nonchilled&quot;,&quot;chilled&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ conc : num 95 175 250 350 500 675 1000 95 175 250 ... ## $ uptake : num 16 30.4 34.8 37.2 35.3 39.2 39.7 13.6 27.3 37.1 ... ## - attr(*, &quot;formula&quot;)=Class &#39;formula&#39; language uptake ~ conc | Plant ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; ## - attr(*, &quot;outer&quot;)=Class &#39;formula&#39; language ~Treatment * Type ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; ## - attr(*, &quot;labels&quot;)=List of 2 ## ..$ x: chr &quot;Ambient carbon dioxide concentration&quot; ## ..$ y: chr &quot;CO2 uptake rate&quot; ## - attr(*, &quot;units&quot;)=List of 2 ## ..$ x: chr &quot;(uL/L)&quot; ## ..$ y: chr &quot;(umol/m^2 s)&quot; levels(datasets::CO2$Type) ## [1] &quot;Quebec&quot; &quot;Mississippi&quot; To fit a logistic regression in R we can use the glm() function, which fits a generalized linear model, indicating that our target variable is binary and the link function is a logit function. Let’s see the model output: logmod &lt;- glm(Type ~ uptake, family = binomial(link = logit), data = datasets::CO2) summary(logmod) ## ## Call: ## glm(formula = Type ~ uptake, family = binomial(link = logit), ## data = datasets::CO2) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.87192 0.87273 4.437 9.14e-06 *** ## uptake -0.14130 0.02992 -4.723 2.32e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 116.449 on 83 degrees of freedom ## Residual deviance: 83.673 on 82 degrees of freedom ## AIC: 87.673 ## ## Number of Fisher Scoring iterations: 4 This fitted model results in a linear classification boundary (discontinued line) that splits the predictor variables space in two. Where that line falls depends on the choice of threshold, in this case \\(\\tau=0.5\\) (see where the grey logistic regression line meets the dashed threshold line). You can see it plotted below: beta &lt;- coef(logmod) # reuse previous plot with classification line datasets::CO2 |&gt; ggplot(aes(x = uptake, y = as.numeric(Type)-1, color = Type)) + geom_point(size = 3, alpha = 0.5) + labs(x = expression(paste(&quot;Uptake (&quot;, mu,&quot;mol CO&quot;[2], &quot; m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;)), y = &quot;&quot;) + theme_classic() + # call glm model fit as part of the plotting stat_smooth(method = &quot;glm&quot;, color = &quot;grey&quot;, se = FALSE, method.args = list(family = binomial), size = 3) + # manually plot of logit function with fitted coefficients geom_function(fun = function(x) exp(beta[1] + beta[2] * x)/(1 + exp(beta[1] + beta[2] * x)), color = &quot;black&quot;, size = 0.5) + # visualise threshold geom_vline(xintercept = -beta[1] / beta[2], lty = 2, linetype = &quot;dotted&quot;) + xlim(0, 60) Most turquoise points fall to one side of the dotted line, representing the threshold \\(\\tau\\), and most reddish points are the other side. This is what we intended. The points that are on the wrong side of the line are mis-classified by the logistic regression model. Our aim is to minimize mis-classification. Note that, just like for linear regression, a logistic regression model allows to use categorical explanatory variables and polynomial transformations of the predictors to achieve better-fitting classification models. Model advantages and concerns One advantage of logistic regression is simplicity. It’s part of the generalized linear regression family of models and the concept of a link function used to build such a model can also be used for various types of response variables (not only binary, but also count data…). You can find more details in this Wikipedia article. Furthermore, logistic regression allows for an interesting interpretation of its model parameters: odds and log-odds. Odds represent how likely it is to find one class versus the other (e.g., if class 1 is twice as likely as class 0, then we have probabilities \\(66\\%\\) vs. \\(33\\%\\)). The odds are defined as the probability of \\(Y\\) belonging to class 1 divided by the probabiity of belonging to class 0, and relates to the model parameters as \\[\\frac{P(Y_i=1)}{P(Y_i=0)} = \\exp(\\beta_0+\\beta_1 X_i).\\] So the log-odds are \\[\\log\\left(\\frac{P(Y_i=1)}{P(Y_i=0)}\\right) = \\beta_0+\\beta_1 X_i.\\] Increases in the values of the predictors affect the odds multiplicatively and the log-odds linearly. It is easy to extend a logistic regression model to more than two classes by fitting models iteratively. For example, first you classify class 1 against classes 2 and 3; then another logistic regression classifies class 2 against 3. Nevertheless, logistic regression relies on statistical assumptions to fit the parameters and interpret the fitted parameters. Whenever these assumptions are not met, one must be careful with the conclusions drawn. Other machine learning methods, that will be covered in Chapters 9 and 10, can also be used for classification tasks. These offer more flexibility than logistic regression, are not necessarily linear, and don’t need to satisfy strict statistical assumptions. 8.3.1.2 Metrics for classification Measuring the quality of a classification model is based on counting how many observations were correctly classified, rather than the distance between the values predicted by a regression and the true observed values. These can be represented in a confusion matrix: \\(Y = 1\\) \\(Y = 0\\) \\(\\hat{Y} = 1\\) True positives (TP) False positives (FP) \\(\\hat{Y} = 0\\) False negatives (FN) True negatives (TN) In a confusion matrix, correctly classified observations are on the diagonal and off-diagonal values correspond to different types of errors. Some of these error types are more relevant for certain applications. Imagine that you want to classify whether the water of a river is safe to drink based on measurements of certain particles or chemicals in the water (Y=1 means safe, Y=0 means unsafe). It’s much worse to tag as “safe” a polluted river than to tag as “unsafe” a potable water source, one must be conservative. In this case, we would prioritize avoiding false positives and wouldn’t care so much about false negatives. The following metrics are widely used and highlight different aspects of our modeling goals. Accuracy is simply the proportion of outputs that were correctly classified: \\[ \\text{Accuracy}=\\frac{\\text{TP} + \\text{TN}}{N},\\] where \\(N\\) is the number of observations. This is a very common metric for training ML models and treats both classes as equally important. It’s naturally extended to multi-class classification and usually compared to the value \\(\\frac{1}{C}\\) where \\(C\\) is the number of classes. Classification models are usually compared to randomness: How much better is our model compared to throwing a coin for classification? At random, we would assign each class \\(50\\%\\) of the time. So if we assume that both classes are as likely to appear, that is, they are balanced, the accuracy of a random guess would be around \\(0.5\\). Hence, we want the accuracy to be “better than random”. If there are \\(C\\) different classes and the observations are balanced, we want the accuracy to be above \\(1-1/C\\). A challenge is posed by imbalanced classes. For a dataset where \\(90\\%\\) of the observations are from class 1 and \\(10\\%\\) from class 0, always predicting 1 would lead to a accuracy of \\(0.9\\). This value may sound good, but that model is not informative because it doesn’t use any information from predictors. Therefore, be careful when working with imbalanced classes and interpreting your results. Precision measures how often our “positive” predictions are correct: \\[\\text{precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}.\\] The true positive rate (TPR), also called recall or sensitivity measures the proportion of real “positives” (\\(Y = 1\\)) we are able to capture: \\[ \\text{TPR} = \\frac{\\text{TP}}{\\text{TP}+\\text{FN}}.\\] The false positive rate (FPR) is defined by \\[\\text{FPR} = \\frac{\\text{FP}}{\\text{FP}+\\text{TN}}.\\] and is related to another metric called specificity by \\(\\text{FPR} = 1 - \\text{specificity}\\). Receiver operating characteristic (ROC) curve: To evaluate the performance of a binary classification model, it’s common to plot the ROC curve, where the TPR is plotted against the FPR, for varying values of the threshold \\(\\tau\\) used in the classification rule. When we decrease the threshold, we get more positive values (more observations are classified as 1), increasing both the true positive and false positive rate. The following image describes clearly how to interpret a ROC curve plot: Figure 8.4: ROC curves and how they compare, from Wikimedia Commons. AUC: The “area under the curve” is defined as the area betwee the ROC curve and the x-axis. For a random classifier we would have AUC = 0.5 and for the perfect classifier, AUC = 1. The aim is to increase the AUC. Nevertheless, a visual inspection of the ROC curve can say even more. F1: The F1 score is a more sophisticated metric, defined as the harmonic mean of precision and sensitivity, or in terms of the confusion matrix values: \\[ F1= 2 \\times \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}} = \\frac{2 \\text{TP}}{2 \\text{TP} + \\text{FP} + \\text{FN}}. \\] This metric provides good results for both balanced and imbalanced datasets and takes into account both the model’s ability to capture positive cases (recall) and be correct with the cases it does capture (precision). It takes values between 0 and 1, with 1 being the best and values of 0.5 and below being bad. These metrics can be used to compare the quality of different classifiers but also to understand the behaviour of a single classifier from different perspectives. This was an introduction of the most basic classification metrics. For a more information on the topic, check out Kuhn &amp; Johnson (2019), Chapter 3.3.2. Implementation in R Let’s take a look at the previous metrics for the logistic regression model we fitted before. The confusionMatrix() function from the {caret} library provides most of the statistics introduced above. # Make classification predictions Y &lt;- logmod$data$Type x &lt;- as.factor(round(logmod$fitted.values)) # Use 0.5 as threshold # Change class names levels(Y) &lt;- levels(x) &lt;- c(&quot;Quebec&quot;, &quot;Mississippi&quot;) # plot confusion matrix conf_matrix &lt;- caret::confusionMatrix(data = x, reference = Y) conf_matrix ## Confusion Matrix and Statistics ## ## Reference ## Prediction Quebec Mississippi ## Quebec 32 13 ## Mississippi 10 29 ## ## Accuracy : 0.7262 ## 95% CI : (0.618, 0.8179) ## No Information Rate : 0.5 ## P-Value [Acc &gt; NIR] : 2.039e-05 ## ## Kappa : 0.4524 ## ## Mcnemar&#39;s Test P-Value : 0.6767 ## ## Sensitivity : 0.7619 ## Specificity : 0.6905 ## Pos Pred Value : 0.7111 ## Neg Pred Value : 0.7436 ## Prevalence : 0.5000 ## Detection Rate : 0.3810 ## Detection Prevalence : 0.5357 ## Balanced Accuracy : 0.7262 ## ## &#39;Positive&#39; Class : Quebec ## Now we can visualize the confusion matrix as a mosaic plot. This is quite helpful when we work with many classes. mosaicplot(conf_matrix$table, main = &quot;Confusion matrix&quot;) 8.4 Exercises There are no exercises with provided solutions for this Chapter. 8.5 Report Exercise Although there are a lot of helpful packages and user-friendly functions available, you will often have to write your own data analysis routine. This requires good understanding of statistical knowledge, algorithmic thinking and problem-solving skills. While writing your code, you will face many questions and bugs that you need to solve. And knowing where and how to ask for help properly are crucial parts of this process (see 2.2.3 for more on getting help). In this Report Exercise, stepwise forward regression is to be performed for the task of modelling GPP as a function of predictors available in the dataset of half-hourly ecosystem fluxes. To learn the skills mentioned above, you will (attempt to) write your own stepwise forward regression from scratch based on the algorithm description of stepwise forward regression in Section 8.2.3.1. The data of half-hourly fluxes can be downloaded from here. Deliverables for the report The complete workflow of reading the data, performing the stepwise forward regression, visualising and discussing the results should be implemented in an RMarkdown notebook file and added as file ./vignettes/re_stepwise.Rmd to your git repository. You will then point us to the URL of your repository. We will fork your repository and reproduce the workflow implemented in your notebook. The following aspects should be covered in your notebook: An evaluation of all bivariate models (single predictor), implementing just steps 1-3 of the algorithm described in 8.2.3.1. This should be complemented by a visualisation and a brief discussion of the results. An implementation of stepwise forward regression, and a visualisation and discussion of its results. If you face unsurmountable challenges and/or errors you didn’t manage to resolve, your notebook may implement a minimum reproducible example of your code that produces the error or of your attempted implementation of stepwise forward regression. Guide for your implementation Remember the structure of the loops implemented for the Exercise Nested loops in Chapter 2. It may be helpful to write a “pseudo-code” that defines the structure of loops and conditionals and defines the points at which certain evaluations and function calls are performed, but cannot be actually run. This Stackoverflow post may be helpful for dynamically creating formulas, given a character vector or variable names, When discussing results, consider why a certain variable was not chosen in the final model. How do non-included variables relate to variables that are included? How do the model metrics change when adding variables? Can you explain their patterns with your statistical knowledge? To better discuss your results, you might want to look up what each variable stands for. More information can be found here. Chose an appropriate visualisation for communicating results. If you hand in a minimum reproducible example of an unsuccessful implementation of stepwise regression, follow this guideline. Write a minimum reproducible example as if you were addressing an online audience; describe in detail what you goal is, where you got stuck, what the error message is, provide a code example that is runnable without needing any local files. Code errors can break knitting your RMarkdown to HTML. To prevent an entire RMarkdown from failing to render because of erroneous code, you can set the respective code chunk as error = TRUE to avoid render failure while including the error message in the output, or eval = FALSE to avoid running the chunk altogether. More can be found here. "],["supervisedmli.html", "Chapter 9 Supervised machine learning I 9.1 Learning objectives 9.2 Tutorial 9.3 Exercises 9.4 Report Exercises", " Chapter 9 Supervised machine learning I Chapter lead author: Benjamin Stocker 9.1 Learning objectives Machine learning may appear magical. The ability of machine learning algorithms to detect patterns and make predictions is fascinating. However, several challenges have to be met in the process of formulating, training, and evaluating the models. In this and the next chapter (Chapter 10), we will discuss some basics of supervised machine learning and how to achieve best predictive results. Basic steps of the implementation of supervised machine learning are introduced, including data splitting, pre-processing, model formulation, and the implementation of these steps using the {caret} and {recipes} R packages. A focus is put on learning the concept of the bias-variance trade-off and overfitting. Contents of this Chapter are inspired and partly adopted by the excellent book Hands-On Machine Learning in R by Boehmke &amp; Greenwell. 9.2 Tutorial 9.2.1 What is supervised machine learning? Supervised machine learning is a type of machine learning where the model is trained using labeled data and the goal is to predict the output for new, unseen data. This corresponds to the approach of model fitting that we’ve seen in Chapter 8. In contrast, unsupervised machine learning is a type of machine learning where the algorithms learn from data without being provided with labeled targets. The algorithms aim to identify patterns and relationships in the data without any guidance. Examples include clustering and dimensionality reduction. In supervised machine learning, we use a set of predictors \\(X\\) (also known as features, or independent variables) and observed values of a target variable \\(Y\\) that are recorded in parallel, to find a model \\(f(X) = \\hat{Y}\\) that yields a good match between \\(Y\\) and \\(\\hat{Y}\\) and that can be used for reliably predicting \\(Y\\) for new (“unseen”) data points \\(X_\\text{new}\\) - data that has not been used during model fitting/training. The hat on \\(\\hat{Y}\\) denotes an estimate. Some algorithms can even handle predictions of multiple target variables simultaneously (e.g., neural networks). From above definitions, we can note a few key ingredients of supervised machine learning: Input data (predictors) Target data recorded in parallel with predictors A model that estimates \\(f(X) = \\hat{Y}\\), made of mathematical operations relating \\(X\\) to \\(\\hat{Y}\\) and of model parameters (coefficients) that are calibrated to yield the best match of \\(Y\\) and \\(\\hat{Y}\\) A metric measuring how good the match between \\(Y\\) and \\(\\hat{Y}\\) is - the loss function An algorithm (the optimiser) to find the best set of parameters that minimize the loss (#fig:machine learningingredients)Supervised machine learning ingredients, adopted from Chollet and Allaire (2018). The type of modelling approach of supervised machine learning is very similar to fitting regression models as we did in Chapter 8. In a sense, supervised machine learning is just another empirical (or statistical) modelling approach. However, you may not want to call linear regression a machine learning algorithm because there is no iterative learning involved. Furthermore, machine learning differs from traditional statistical modelling methods in that it makes no assumptions regarding the data generation process and underlying distributions (Breiman, 2001). Nevertheless, contrasting a bivariate linear regression model with a complex machine learning algorithm is instructive. Also linear regression provides a prediction \\(\\hat{Y} = f(X)\\), just like other (proper) machine learning algorithms do. The functional form of a bivariate linear regression is not particularly flexible (just a straight line for the best fit between predictors and targets) and it has only two parameters (slope and intercept). At the other extreme are, for example, deep neural networks. They are extremely flexible, can learn highly non-linear relationships and deal with interactions between a large number of predictors. They also contain very large numbers of parameters (typically on the order of \\(10^4 - 10^7\\)). You can imagine that their high flexibility allows these types of algorithms to very effectively learn from the data, but also bears the risk of overfitting. What is overfitting? 9.2.2 Overfitting This example is based on this example from scikit-learn. Let’s assume that there is some true underlying relationship between a single predictor \\(X\\) and the target variable \\(Y\\). We don’t know this relationship and the observations contain a (normally distributed) error. Based on our training data, we fit three polynomial models that differ with respect to their complexity. We fit a polynomial of degree 1, 4, and 15 to the observations. A polynomial of degree \\(N\\) is given by: \\[ y = \\sum_{n=0}^N a_n x^n \\] \\(a_n\\) are the coefficients, i.e., model parameters. The goal of the training is to find the coefficients \\(a_n\\) so that the predicted \\(\\hat{Y}\\) fits observed \\(Y\\) best. From the above definition, the polynomial of degree 15 has 16 parameters, while the polynomial of degree 1 has two parameters (and corresponds to a simple bivariate linear regression). You can imagine that the polynomial of degree 15 is much more flexible and should thus yield the closest fit to the training data. This is indeed the case. We can use the same fitted models on data that was not used for model fitting - the test data. This is what’s done below. Again, the same true underlying relationship is used, but we sample a new set of data points \\(X_\\text{new}\\) and add a new sample of errors on top of the true relationship. You see that, using the test set, we find that “poly4” actually performs best - it has a much lower RMSE than “poly15”. Apparently, “poly15” was overfitted. Apparently, it used its flexibility to fit not only the shape of the true underlying relationship, but also the observation errors on top of it. This has the implication that, when this model is used for making predictions for data that was not used for training, it will yield misguided predictions that are affected by the errors in the training set. This is the reason why “poly15” performed worse on the test set than the other models. From the figures above, we can also conclude that “poly1” was underfitted - it performed worse than “poly4” also on the validation set. The out-of-sample performance of “poly15” gets even worse when applying the fitted polynomial models to data that extends beyond the range in \\(X\\) that was used for model training. Here, we’re extending just 20% to the right. You see that the RMSE for “poly15” literally explodes. The model is hopelessly overfitted and completely useless for prediction, although it looked like it fitted the data best when we considered only the training results. This is a fundamental challenge in machine learning - finding the model with the best generalisability. That is, a model that not only fits the training data well, but also performs well on unseen data. The phenomenon of fitting and overfitting as a function of the model complexity is also referred to as the bias-variance trade-off. The bias describes how well a model matches the training set (average error). A model with low bias will match the data set closely and vice versa. The variance describes how much a model changes when you train it using different portions of your data set. “poly15” has a high variance. On the other extreme, “poly1” has a high bias. It’s not affected by the noise in observations, but its predictions are also far off the observations. In machine learning (as in all statistical modelling), we are challenged to balance this trade-off. This Chapter and Chapter 10 introduce the methods for achieving the best model generalisability and find the sweet spot between high bias and high variance. One of the key steps of the machine learning modelling process is motivated by the example above: the separation of the data into a training and a testing set (data splitting). Only by withholding part of the data from the model training, we have a good basis for testing the model on that unseen data for evaluating its generalisability. Additional steps that may be required or beneficial for effective model training and their implementation in R are introduced in this and the next Chapter. Depending on your application or research question, it may also be of interest to evaluate the relationships embodied in \\(f(X)\\) or to quantify the importance of different predictors in our model. This is referred to as model interpretation and is not (currently) included in this book. Of course, a plethora of algorithms exist that do the job of \\(Y = f(X)\\). Each of them has its own strengths and limitations. It is beyond the scope of this course to introduce a larger number of machine learning algorithms. For illustration purposes in this chapter, we will use and introduce the K-nearest-Neighbors (KNN) algorithm and compare its performance to a multivariate linear regression for illustration purposes. Chapter 11 introduces Random Forest. 9.2.3 Data and the modelling challenge We’re returning to ecosystem flux data that we’ve used in Chapters 3 and 4. Here, we’re using daily data from the evergreen site in Davos, Switzerland (CH-Dav) to avoid effects of seasonally varying foliage cover for which the data does not contain information. To address such additional effects, we would have to, for example, combine the flux and meteorological data with remotely sensed surface greenness data. The data set FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv contains a time series of the ecosystem gross primary production (GPP) and a range of meteorological variables, measured in parallel. In this chapter, we formulate a model for predicting GPP from a set of covariates (other variables that vary in parallel, here the meteorological variables). This is to say that GPP_NT_VUT_REF is the target variable, and other variables that are available in our dataset are the predictors. Let’s read the data, select suitable variables, interpret missing value codes, and select only good-quality data (where at least 80% of the underlying half-hourly data was good quality measured data, and not gap-filled). daily_fluxes &lt;- read_csv(&quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot;) |&gt; # select only the variables we are interested in dplyr::select(TIMESTAMP, GPP_NT_VUT_REF, # the target ends_with(&quot;_QC&quot;), # quality control info ends_with(&quot;_F&quot;), # includes all all meteorological covariates -contains(&quot;JSB&quot;) # weird useless variable ) |&gt; # convert to a nice date object dplyr::mutate(TIMESTAMP = lubridate::ymd(TIMESTAMP)) |&gt; # set all -9999 to NA mutate(across(where(is.numeric), ~na_if(., -9999))) |&gt; # retain only data based on &gt;=80% good-quality measurements # overwrite bad data with NA (not dropping rows) dplyr::mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC &lt; 0.8, NA, GPP_NT_VUT_REF), TA_F = ifelse(TA_F_QC &lt; 0.8, NA, TA_F), SW_IN_F = ifelse(SW_IN_F_QC &lt; 0.8, NA, SW_IN_F), LW_IN_F = ifelse(LW_IN_F_QC &lt; 0.8, NA, LW_IN_F), VPD_F = ifelse(VPD_F_QC &lt; 0.8, NA, VPD_F), PA_F = ifelse(PA_F_QC &lt; 0.8, NA, PA_F), P_F = ifelse(P_F_QC &lt; 0.8, NA, P_F), WS_F = ifelse(WS_F_QC &lt; 0.8, NA, WS_F)) |&gt; # drop QC variables (no longer needed) dplyr::select(-ends_with(&quot;_QC&quot;)) To reproduce this code chunk, you can download the file FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv from here and read it from the local path where the file is stored on your machine. All data files used in this tutorials are stored here. The steps above are considered data wrangling and are not part of the modelling process. After completing this tutorial, you will understand this distinction. 9.2.4 K-nearest neighbours Before we start with the model training workflow, let’s introduce the K-nearest neighbour (KNN) algorithm. It serves the purpose of demonstrating the bias-variance trade-off. As the name suggests, KNN uses the \\(k\\) observations that are “nearest” to the new record for which we want to make a prediction. It then calculates their average (for regression) or most frequent value (for classification) and uses it as the prediction of the target value. “Nearest” is determined by some distance metric evaluated based on the values of the predictors. In our example (GPP_NT_VUT_REF ~ .), KNN would determine the \\(k\\) days (rows in a data frame) where conditions, given by our set of predictors, were most similar (nearest) to the day for which we seek a prediction. Then, it calculates the prediction as the average (mean) GPP value of these days. Determining “nearest” neighbors is commonly based on either the Euclidean or Manhattan distances between two data points \\(X_a\\) and \\(X_b\\), considering all \\(P\\) predictors \\(j\\). Euclidean distance: \\[ \\sqrt{ \\sum_{j=1}^P (X_{a,j} - X_{b,j})^2 } \\\\ \\] Manhattan distance: \\[ \\sum_{j=1}^P | X_{a,j} - X_{b,j} | \\] In two-dimensional space, the Euclidean distance measures the length of a straight line between two points (remember Pythagoras!). The Manhattan distance is called this way because it measures the distance you would have to walk to get from point \\(a\\) to point \\(b\\) in Manhattan, New York, where you cannot cut corners but have to follow a rectangular grid of streets. \\(|x|\\) is the absolute value of \\(X\\) ( \\(|-x| = x\\)). KNN is a simple algorithm that uses knowledge of the “local” data structure for prediction. A drawback is that the model “training” has to be done for each prediction step and the computation time of the training increases with \\(x \\times p\\). KNNs are often used, for example, to impute values (fill missing values, see also below) and have the advantage that predicted values are always within the range of observed values of the target variable. 9.2.5 Model formulation The aim of supervised machine learning is to find a model \\(\\hat{Y} = f(X)\\) so that \\(\\hat{Y}\\) agrees well with observations \\(Y\\). We typically start with a research question where \\(Y\\) is given - naturally - by the problem we are addressing and we have a data set at hand where one or multiple predictors (or “features”) \\(X\\) are recorded along with \\(Y\\). From our data, we have information about how GPP (ecosystem-level photosynthesis) depends on a set of abiotic factors, mostly meteorological measurements. 9.2.5.1 Formula notation In R, it is common to use the formula notation to specify the target and predictor variables. You have encountered formulas before, e.g., for a linear regression using the lm() function. To specify a linear regression model for GPP_NT_VUT_REF with three predictors SW_IN_F, VPD_F, and TA_F, to be fitted to data daily_fluxes, we write: lm(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = daily_fluxes) 9.2.5.2 The generic train() The way we formulate a model can be understood as being independent of the algorithm, or engine, that takes care of fitting \\(f(X)\\). The R package {caret} provides a unified interface for using different machine learning algorithms implemented in separate packages. In other words, it acts as a wrapper for multiple different model fitting, or machine learning algorithms. This has the advantage that it unifies the interface - the way arguments are provided and outputs are returned. {caret} also provides implementations for a set of commonly used tools for data processing, model training, and evaluation. We’ll use {caret} here for model training with the function train(). Note however, that using a specific algorithm, which is implemented in a specific package outside {caret}, also requires that the respective package be installed and loaded. Using {caret} for specifying the same linear regression model as above, the base-R lm() function, can be done with {caret} in a generalized form as: caret::train( form = GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = daily_fluxes |&gt; drop_na(), # drop missing values trControl = caret::trainControl(method = &quot;none&quot;), # no resampling method = &quot;lm&quot; ) ## Linear Regression ## ## 2729 samples ## 3 predictor ## ## No pre-processing ## Resampling: None Note the argument specified as trControl = trainControl(method = \"none\"). This suppresses the default approach to model fitting in {caret} - to resample using bootstrapping. More on that in Chapter 10. Note also that we dropped all rows that contained at least one missing value - necessary to apply the least squares method for the linear regression model fitting. It’s advisable to apply this data removal step only at the very last point of the data processing and modelling workflow. Alternative algorithms may be able to deal with missing values and we want to avoid losing information along the workflow. Of course, it is an overkill to write this as in the code chunk above compared to just writing lm(...). The advantage of the unified interface is that we can simply replace the method argument to use a different model fitting algorithm. For example, to use KNN, we just can write: caret::train( form = GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = daily_fluxes |&gt; drop_na(), trControl = caret::trainControl(method = &quot;none&quot;), method = &quot;knn&quot; ) ## k-Nearest Neighbors ## ## 2729 samples ## 3 predictor ## ## No pre-processing ## Resampling: None 9.2.6 Data splitting The introductory example demonstrated the importance of validating the fitted model with data that was not used for training. Thus, we can test the model’s generalisability to new (“unseen”) data. The essential step that enables us to assess the model’s generalization error is to hold out part of the data from training and set it aside (leaving it absolutely untouched!) for testing. There is no fixed rule for how much data are to be used for training and testing, respectively. We have to balance a trade-off: Spending too much data for training will leave us with too little data for testing and the test results may not be robust. In this case, the sample size for getting robust validation statistics is not sufficiently large and we don’t know for sure whether we are safe from an over-fit model. Spending too much data for validation will leave us with too little data for training. In this case, the machine learning algorithm may not be successful at finding real relationships due to insufficient amounts of training data. Typical splits are between 60-80% for training. However, in cases where the number of data points is very large, the gains from having more training data are marginal, but come at the cost of adding to the already high computational burden of model training. In environmental sciences, the number of predictors is often smaller than the sample size (\\(p &lt; n\\)), because it is typically easier to collect repeated observations of a particular variable than to expand the set of variables being observed. Nevertheless, in cases where the number \\(p\\) gets large, it is important, and for some algorithms mandatory, to maintain \\(p &lt; n\\) for model training. An important aspect to consider when splitting the data is to make sure that all “states” of the system for which we have data are well represented in training and testing sets. A particularly challenging case is posed when it is of particular interest that the algorithm learns relationships \\(f(X)\\) under rare conditions \\(X\\), for example meteorological extreme events. If not addressed with particular measures, model training tends to achieve good model performance for the most common conditions. A simple way to put more emphasis for model training on extreme conditions is to compensate by sampling overly proportional from such cases for the training data set. Several alternative functions for the data splitting step are available from different packages in R. We use the the rsample package here as it allows to additionally make sure that data from the full range of a given variable’s values (VPD_F in the example below) are well covered in both training and testing sets. set.seed(123) # for reproducibility split &lt;- rsample::initial_split(daily_fluxes, prop = 0.7, strata = &quot;VPD_F&quot;) daily_fluxes_train &lt;- rsample::training(split) daily_fluxes_test &lt;- rsample::testing(split) Plot the distribution of values in the training and testing sets. plot_data &lt;- daily_fluxes_train |&gt; dplyr::mutate(split = &quot;train&quot;) |&gt; dplyr::bind_rows(daily_fluxes_test |&gt; dplyr::mutate(split = &quot;test&quot;)) |&gt; tidyr::pivot_longer(cols = 2:9, names_to = &quot;variable&quot;, values_to = &quot;value&quot;) plot_data |&gt; ggplot(aes(x = value, y = ..density.., color = split)) + geom_density() + facet_wrap(~variable, scales = &quot;free&quot;) 9.2.7 Pre-processing Data pre-processing is aimed at preparing the data for use in a specific model fitting procedure and at improving the effectiveness of model training. The splitting of the data into a training and test set makes sure that no information from the test set is used during or before model training. It is important that absolutely no information from the test set finds its way into the training set (data leakage). In a general sense, pre-processing involve data transformations where the transformation functions use parameters that are determined on the data itself. Consider, for example, the standardization. That is, the linear transformation of a vector of values to have zero mean (data is centered, \\(\\mu = 0\\)) and a standard deviation of 1 (data is scaled to \\(\\sigma = 1\\)). In order to avoid data leakage, the mean and standard deviation have to be determined on the training set only. Then, the normalization of the training and the test sets both use the set of (\\(\\mu, \\sigma\\)) determined on the training set. Data leakage would occur if the (\\(\\mu, \\sigma\\)) would be determined on data containing values from the test set. Often, multiple splits of the data are considered during model training. Hence, an even larger number of data transformation parameters (\\(\\mu, \\sigma\\) in the example of normalization) have to be determined and transformations applied to the multiple splits of the data. {caret} deals with this for you and the transformations do not have to be “manually” applied before applying the train() function call. Instead, the data pre-processing is considered an integral step of model training and instructions are specified as part of the train() function call and along with the un-transformed data. The {recipes} package provides an even more powerful way for specifying the formula and pre-processing steps in one go. It is compatible with the train() function of {caret}. For the same formula as above, and an example where the data daily_fluxes_train is to be normalized (centered and scaled), we can specify a “recipe” using the pipe operator as: pp &lt;- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = daily_fluxes_train) |&gt; recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |&gt; recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes()) The first line with the recipe() function call assigns roles to the different variables. GPP_NT_VUT_REF is an outcome (in “{recipes} speak”). Then, we used selectors to apply the recipe step to several variables at once. The first selector, all_numeric(), selects all variables that are either integers or real values. The second selector, -all_outcomes() removes any outcome (target) variables from this recipe step. The returned object pp does not contain a normalized version of the data frame daily_fluxes_train, but rather the information that allows us to apply a specific set of pre-processing steps also to any other data set. The object pp can then be supplied to train() as its first argument: caret::train( pp, data = daily_fluxes_train, method = &quot;knn&quot;, trControl = caret::trainControl(method = &quot;none&quot;) ) The example above showed data standardization as a data pre-processing step. Data pre-processing may be done with different aims, as described in sub-sections below. 9.2.7.1 Standardization Several algorithms explicitly require data to be standardized so that values of all predictors vary within a comparable range. The necessity of this step becomes obvious when considering KNN, where the magnitude of the distance is strongly influenced by the order of magnitude of the predictor values. Here are the ranges and quantiles of the available variables. daily_fluxes |&gt; summarise(across(where(is.numeric), ~quantile(.x, probs = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE))) |&gt; t() |&gt; as_tibble(rownames = &quot;variable&quot;) |&gt; setNames(c(&quot;variable&quot;, &quot;min&quot;, &quot;q25&quot;, &quot;q50&quot;, &quot;q75&quot;, &quot;max&quot;)) ## # A tibble: 8 × 6 ## variable min q25 q50 q75 max ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 GPP_NT_VUT_REF -4.23 0.773 2.87 5.45 12.3 ## 2 TA_F -21.9 -1.47 3.51 8.72 20.7 ## 3 SW_IN_F 3.30 77.8 135. 214. 366. ## 4 LW_IN_F 138. 243. 279. 308. 365. ## 5 VPD_F 0.001 0.959 2.23 4.06 16.6 ## 6 PA_F 80.4 83.2 83.7 84.1 85.6 ## 7 P_F 0 0 0 1.6 92.1 ## 8 WS_F 0.405 1.56 1.93 2.34 6.54 We see for example, that typical values of LW_IN_F are by a factor 100 larger than values of VPD_F. A distance calculated based on these raw values would therefore be strongly dominated by the difference in LW_IN_F values, and differences in VPD_F would hardly affect the distance. Therefore, the data must be standardized before using it with the KNN algorithm (and other algorithms, including Neural Networks). Standardization is done to each variable separately, by centering and scaling each to have \\(\\mu = 0\\) and \\(\\sigma = 1\\). The steps for centering and scaling using the recipes package are described above. Standardization can be done not only by centering and scaling (as described above), but also by scaling to within range, where values are scaled such that the minimum value within each variable (column) is 0 and the maximum is 1. As seen above for the feature engineering example, the object pp does not contain a standardized version of the data frame daily_fluxes_train, but rather the information that allows us to apply the same standardization also to other data. In other words, recipe(..., data = daily_fluxes_train) |&gt; step_center(...) |&gt; step_scale(...) doesn’t actually transform daily_fluxes_train. There are two more steps involved to get there. This might seem bothersome at first but their separation is critical in the context of model training and data leakage, and translates the conception of the pre-processing as a “recipe” into the way we write the code. To actually transform the data, we first have to “prepare” the recipe: pp_prep &lt;- recipes::prep(pp, training = daily_fluxes_train) Finally we can actually transform the data. That is, “juice” the prepared recipe. daily_fluxes_juiced &lt;- recipes::juice(pp_prep) Note, if we are to apply the prepared recipe to new data, we’ll have to bake() it. daily_fluxes_baked &lt;- recipes::bake(pp_prep, new_data = daily_fluxes_train) # confirm that juice and bake return identical objects when given the same data all_equal(daily_fluxes_juiced, daily_fluxes_baked) ## Warning: `all_equal()` was deprecated in dplyr 1.1.0. ## ℹ Please use `all.equal()` instead. ## ℹ And manually order the rows/cols as needed ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. ## [1] TRUE The effect is of standardization is illustrated by comparing original and transformed variables: # prepare data for plotting plot_data_original &lt;- daily_fluxes_train |&gt; dplyr::select(one_of(c(&quot;SW_IN_F&quot;, &quot;VPD_F&quot;, &quot;TA_F&quot;))) |&gt; tidyr::pivot_longer(cols = c(SW_IN_F, VPD_F, TA_F), names_to = &quot;var&quot;, values_to = &quot;val&quot;) plot_data_juiced &lt;- daily_fluxes_juiced |&gt; dplyr::select(one_of(c(&quot;SW_IN_F&quot;, &quot;VPD_F&quot;, &quot;TA_F&quot;))) |&gt; tidyr::pivot_longer(cols = c(SW_IN_F, VPD_F, TA_F), names_to = &quot;var&quot;, values_to = &quot;val&quot;) # plot density plot_1 &lt;- ggplot(data = plot_data_original, aes(val, ..density..)) + geom_density() + facet_wrap(~var) # plot density by var plot_2 &lt;- ggplot(data = plot_data_juiced, aes(val, ..density..)) + geom_density() + facet_wrap(~var) # combine both plots cowplot::plot_grid(plot_1, plot_2, nrow = 2) 9.2.7.2 Handling missing data Several machine learning algorithms require missing values to be removed. That is, if any of the cells in one row has a missing value, the entire cell gets removed. This can lead to severe data loss. In cases where missing values appear predominantly in only a few variables, it may be advantageous to drop the affected variable from the data for modelling. In other cases, it may be advantageous to fill missing values (data imputation, see next section). Although such imputed data is “fake”, it may be preferred to impute values than to drop entire rows and thus get the benefit of being able to use the information contained in available (real) values of affected rows. Whether or not imputation is preferred should be determined based on the model skill for an an out-of-sample test (more on that later). Visualizing missing data is the essential first step in making decisions about dropping rows with missing data versus removing predictors from the model (which would imply too much data removal). visdat::vis_miss( daily_fluxes, cluster = FALSE, warn_large_data = FALSE ) Here, the variable LW_IN_F (longwave radiation) if affected by a lot of missing data. Note that we applied a data cleaning step along with the data read-in at the very top of this Chapter. There, we applied a filtering criterion where values are only retained if at least 80% of the underlying half-hourly data is actual measured (and not gap-filled) data. Whether to drop the variable for further modelling should be informed also by our understanding of the data and the processes relevant for the modelling task. Here, the modelling target is GPP and the carbon cycle specialists among the readers may know that longwave radiation is not a known important control on GPP (ecosystem photosynthesis). Therefore, we may consider dropping this variable from the dataset for our modelling task. The remaining variables are affected by less frequent missingness with which we will deal otherwise. 9.2.7.3 Imputation Imputation refers to the replacement of missing values with with a “best guess” value (Boehmke and Greenwell). Different approaches exist for determining that best guess. The most basic approach is to impute missing values with the mean or median of the available values of the same variable, which can be implemented using step_impute_*() from the {recipes} package. For example, to impute the median for all predictors separately: pp |&gt; step_impute_median(all_predictors()) Imputing by the mean or median is “uninformative”. We may use information about the co-variation of multiple variables for imputing missing values. For example, for imputing missing VPD values, we may consider the fact that VPD tends to be high when air temperature is high. Therefore, missing VPD values can be modeled as a function of other co-variates (predictors). Several approaches to modelling missing values are available through the {recipes} package (see here). For example, we can use KNN with five neighbors as: pp |&gt; step_impute_knn(all_predictors(), neighbors = 5) 9.2.7.4 One-hot encoding For machine learning algorithms that require that all predictors be numerical (e.g., neural networks, or KNN), categorical predictors have to be pre-processed and converted into new numerical predictors. The most common such transformation is one-hot encoding, where a categorical predictor variable that has \\(N\\) levels is replaced by \\(N\\) new variables that contain either zeros or ones depending whether the value of the categorical feature corresponds to the respective column. Because this creates perfect collinearity between these new column, we can also drop one of them. This is referred to as dummy encoding. The example below demonstrates what one-hot encoding does. # original data frame df &lt;- tibble(id = 1:4, color = c(&quot;red&quot;, &quot;red&quot;, &quot;green&quot;, &quot;blue&quot;)) df ## # A tibble: 4 × 2 ## id color ## &lt;int&gt; &lt;chr&gt; ## 1 1 red ## 2 2 red ## 3 3 green ## 4 4 blue # after one-hot encoding dmy &lt;- dummyVars(&quot;~ .&quot;, data = df, sep = &quot;_&quot;) data.frame(predict(dmy, newdata = df)) ## id colorblue colorgreen colorred ## 1 1 0 0 1 ## 2 2 0 0 1 ## 3 3 0 1 0 ## 4 4 1 0 0 Note that in a case where color is strictly one of c(\"red\", \"red\", \"green\", \"blue\") (and not, for example, \"yellow\"), then one of the columns added by dummyVars() is obsolete (if it’s neither \"red\", nor \"green\", it must be \"blue\") - columns are collinear. This can be avoided by setting fullRank = FALSE. Using the recipes package, one-hot encoding is implemented by: recipe(GPP_NT_VUT_REF ~ ., data = daily_fluxes) |&gt; step_dummy(all_nominal(), one_hot = TRUE) 9.2.7.5 Zero-variance predictors Sometimes, the data generation process yields variables that have the same value in each observation. And sometimes this is due to failure of the measurement device or some other bug in the data collection pipeline. Either way, this may cause some algorithms to crash or become unstable. Such “zero-variance” predictors are usually removed altogether. The same applies also to variables with “near-zero variance”. That is, variables where only a few unique values occur with a high frequency in the entire data set. The danger is that, when data is split into training and testing sets, the variable may effectively become a “zero-variance” variable within the training subset. We can test for zero-variance or near-zero variance predictors by quantifying the following metrics: Frequency ratio: Ratio of the frequency of the most common predictor over the second most common predictor. This should be near 1 for well-behaved predictors and get very large for problematic ones. Percent unique values: The number of unique values divided by the total number of rows in the data set (times 100). For problematic variables, this ratio gets small (approaches 1/100). The function nearZeroVar of the {caret} package flags suspicious variables (zeroVar = TRUE or nzv = TRUE). In our data set, we don’t find any: caret::nearZeroVar(daily_fluxes, saveMetrics = TRUE) ## freqRatio percentUnique zeroVar nzv ## TIMESTAMP 1.000000 100.000000 FALSE FALSE ## GPP_NT_VUT_REF 1.000000 93.732887 FALSE FALSE ## TA_F 1.000000 83.951932 FALSE FALSE ## SW_IN_F 1.500000 95.375723 FALSE FALSE ## LW_IN_F 1.000000 43.170064 FALSE FALSE ## VPD_F 1.142857 60.450259 FALSE FALSE ## PA_F 1.090909 37.906906 FALSE FALSE ## P_F 10.268072 5.978096 FALSE FALSE ## WS_F 1.083333 34.758138 FALSE FALSE Using the recipes package, we can add a step that removes zero-variance predictors by: pp |&gt; step_zv(all_predictors()) 9.2.7.6 Target engineering Target engineering refers to pre-processing of the target variable. Its application can enable improved predictions, particularly for models that make assumptions about prediction errors or when the target variable follows a “special” distribution (e.g., heavily skewed distribution, or where the target variable is a fraction that is naturally bounded by 0 and 1). A simple log-transformation of the target variable can often resolve issues with skewed distributions. An implication of a log-transformation is that errors in predicting values in the upper end of the observed range are “discounted” in their weight compared to errors in the lower range. In our data set, the variable WS_F (wind speed) is skewed. The target variable that we have considered so far (GPP_NT_VUT_REF) is not skewed. In a case where we would consider WS_F to be our target variable, we would thus consider applying a log-transformation. plot_1 &lt;- ggplot(data = daily_fluxes, aes(x = WS_F, y = ..density..)) + geom_histogram() + labs(title = &quot;Original&quot;) plot_2 &lt;- ggplot(data = daily_fluxes, aes(x = log(WS_F), y = ..density..)) + geom_histogram() + labs(title = &quot;Log-transformed&quot;) cowplot::plot_grid(plot_1, plot_2) Log transformation as part of the pre-processing is specified using the step_log() function, here applied to the model target variable (all_outcomes()). recipes::recipe(WS_F ~ ., data = daily_fluxes) |&gt; # it&#39;s of course non-sense to model wind speed like this recipes::step_log(all_outcomes()) A log-transformation doesn’t necessarily result in a perfect normal distribution of transformed values. The Box-Cox can get us closer. It can be considered a generalization of the log-transformation. Values are transformed according to the following function: \\[ y(\\lambda) = \\begin{cases} \\frac{Y^\\lambda-1}{\\lambda}, &amp;\\; y \\neq 0\\\\ \\log(Y), &amp;\\; y = 0 \\end{cases} \\] \\(\\lambda\\) is treated as a parameter that is fitted such that the resulting distribution of values \\(Y\\) approaches the normal distribution. To specify a Box-Cox-transformation as part of the pre-processing, we can use step_BoxCox() from the {recipes} package. pp &lt;- recipes::recipe(WS_F ~ ., data = daily_fluxes_train) |&gt; recipes::step_BoxCox(all_outcomes()) How do transformed values look like? prep_pp &lt;- recipes::prep(pp, training = daily_fluxes_train |&gt; drop_na()) daily_fluxes_baked &lt;- bake(prep_pp, new_data = daily_fluxes_test |&gt; drop_na()) daily_fluxes_baked |&gt; ggplot(aes(x = WS_F, y = ..density..)) + geom_histogram() + labs(title = &quot;Box-Cox-transformed&quot;) Note that the Box-Cox-transformation can only be applied to values that are strictly positive. In our example, wind speed (WS_F) is. If this is not satisfied, a Yeo-Johnson transformation can be applied. recipes::recipe(WS_F ~ ., data = daily_fluxes) |&gt; recipes::step_YeoJohnson(all_outcomes()) 9.2.8 Putting it all together (half-way) Let’s recap. We have a dataset daily_fluxes and we want to predict ecosystem GPP (GPP_NT_VUT_REF) from a set of predictors - environmental covariates that were measured in parallel to GPP. Let’s compare the performance of a multivariate linear regression and KNN model in terms of its generalisation to data that was not used for model fitting. The following pieces are implemented: Missing data: We’ve seen that the predictor LW_IN_F has lots of missing values and - given a priori knowledge is not critical for predicting GPP and we’ll drop it. Data cleaning: Data (daily_fluxes) was cleaned based on quality control information upon reading the data at the beginning of this Chapter. Before modelling, we’re checking the distribution of the target value here again to make sure it is “well-behaved”. Imputation: We drop rows with missing data for model training, instead of imputing them. Some of the predictors are distintively not normally distributed. Let’s Box-Cox transform all predictors as a pre-processing step. We have to standardize the data in order to use it for KNN. We have no variable where zero-variance was detected and we have no categorical variables that have to be transformed by one-hot encoding to be used in KNN. We use a data split, whithholding 30% for testing. Fit two models: a linear regression model and KNN. Take \\(k=10\\) for the KNN model. Other choices are possible and will affect the prediction error on the training and the testing data in different manners. We’ll learn more about the optimal choice of \\(k\\) (hyperparameter tuning) in the next chapter. Fit models to minimize the root mean square error (RMSE) between predictions and observations. More on the choice of the metric argument in train() (loss function) in the next chapter. For the KNN model, use \\(k=8\\). These steps are implemented by the code below. # Data cleaning: looks ok, no obviously bad data # no long tail, therefore no further target engineering daily_fluxes |&gt; ggplot(aes(x = GPP_NT_VUT_REF, y = ..count..)) + geom_histogram() # Data splitting set.seed(1982) # for reproducibility split &lt;- rsample::initial_split(daily_fluxes, prop = 0.7, strata = &quot;VPD_F&quot;) daily_fluxes_train &lt;- rsample::training(split) daily_fluxes_test &lt;- rsample::testing(split) # Model and pre-processing formulation, use all variables but LW_IN_F pp &lt;- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = daily_fluxes_train |&gt; drop_na()) |&gt; recipes::step_BoxCox(recipes::all_predictors()) |&gt; recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |&gt; recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes()) # Fit linear regression model mod_lm &lt;- caret::train( pp, data = daily_fluxes_train |&gt; drop_na(), method = &quot;lm&quot;, trControl = caret::trainControl(method = &quot;none&quot;), metric = &quot;RMSE&quot; ) # Fit KNN model mod_knn &lt;- caret::train( pp, data = daily_fluxes_train |&gt; drop_na(), method = &quot;knn&quot;, trControl = caret::trainControl(method = &quot;none&quot;), tuneGrid = data.frame(k = 8), metric = &quot;RMSE&quot; ) We can use the model objects mod_lm and mod_knn to add the fitted values to the training and the test data, both using the generic function predict(..., newdata = ...). The code below implements the prediction step, the measuring of the prediction skill, and the visualisation of predicted versus observed values on the test and training sets, bundled into one function - eval_model() - which we will re-use for each fitted model object. # make model evaluation into a function to reuse code eval_model &lt;- function(mod, df_train, df_test){ # add predictions to the data frames df_train &lt;- df_train |&gt; drop_na() df_train$fitted &lt;- predict(mod, newdata = df_train) df_test &lt;- df_test |&gt; drop_na() df_test$fitted &lt;- predict(mod, newdata = df_test) # get metrics tables metrics_train &lt;- df_train |&gt; yardstick::metrics(GPP_NT_VUT_REF, fitted) metrics_test &lt;- df_test |&gt; yardstick::metrics(GPP_NT_VUT_REF, fitted) # extract values from metrics tables rmse_train &lt;- metrics_train |&gt; filter(.metric == &quot;rmse&quot;) |&gt; pull(.estimate) rsq_train &lt;- metrics_train |&gt; filter(.metric == &quot;rsq&quot;) |&gt; pull(.estimate) rmse_test &lt;- metrics_test |&gt; filter(.metric == &quot;rmse&quot;) |&gt; pull(.estimate) rsq_test &lt;- metrics_test |&gt; filter(.metric == &quot;rsq&quot;) |&gt; pull(.estimate) # visualise as a scatterplot # adding information of metrics as sub-titles plot_1 &lt;- ggplot(data = df_train, aes(GPP_NT_VUT_REF, fitted)) + geom_point(alpha = 0.3) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;red&quot;) + geom_abline(slope = 1, intercept = 0, linetype = &quot;dotted&quot;) + labs(subtitle = bquote( italic(R)^2 == .(format(rsq_train, digits = 2)) ~~ RMSE == .(format(rmse_train, digits = 3))), title = &quot;Training set&quot;) + theme_classic() plot_2 &lt;- ggplot(data = df_test, aes(GPP_NT_VUT_REF, fitted)) + geom_point(alpha = 0.3) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;red&quot;) + geom_abline(slope = 1, intercept = 0, linetype = &quot;dotted&quot;) + labs(subtitle = bquote( italic(R)^2 == .(format(rsq_test, digits = 2)) ~~ RMSE == .(format(rmse_test, digits = 3))), title = &quot;Test set&quot;) + theme_classic() out &lt;- cowplot::plot_grid(plot_1, plot_2) return(out) } # linear regression model eval_model(mod = mod_lm, df_train = daily_fluxes_train, df_test = daily_fluxes_test) Figure 9.1: Evaluation of the linear regression and the KNN models on the training and the test set. # KNN eval_model(mod = mod_knn, df_train = daily_fluxes_train, df_test = daily_fluxes_test) Figure 9.2: Evaluation of the linear regression and the KNN models on the training and the test set. It is advisable to keep workflow notebooks (this RMarkdown file) light and legible. Therefore, code chunks should not be excessively long and functions should be kept in a ./R/*.R file, which can be loaded. This also facilitates debugging code inside the function. Here, the function eval_model() is part of the book’s git repository, stored in the sub-directory ./R/, and used also in later chapters. 9.3 Exercises There are no exercises with provided solutions for this Chapter. 9.4 Report Exercises Comparison of the linear regression and KNN models The figures above show the evaluation of the model performances of the linear regression and the KNN model, evaluated on the training and test set. This exercise is to interpret and understand the observed differences. Implement the following points: Adopt the code from this Chapter for fitting and evaluating the linear regression model and the KNN into your own RMarkdown file. Name the file ./vignettes/re_ml_01.Rmd. Keep larger functions in a separate file in an appropriate directory and load the function definition as part of the RMarkdown. Interpret observed differences in the context of the bias-variance trade-off: Why is the difference between the evaluation on the training and the test set larger for the KNN model than for the linear regression model? Why does the evaluation on the test set indicate a better model performance of the KNN model than the linear regression model? How would you position the KNN and the linear regression model along the spectrum of the bias-variance trade-off? Visualise temporal variations of observed and modelled GPP for both models, covering all available dates. The role of k Let’s look at the role of \\(k\\) in a KNN. Answer the following questions: Based on your understanding of KNN (and without running code), state a hypothesis for how the \\(R^2\\) and the MAE evaluated on the test and on the training set would change for \\(k\\) approaching 1 and for \\(k\\) approaching \\(N\\) (the number of observations in the data). Explain your hypothesis, referring to the bias-variance trade-off. Put your hypothesis to the test! Write code that splits the data into a training and a test set and repeats model fitting and evaluation for different values for \\(k\\). Visualise results, showing model generalisability as a function of model complexity. Describe how a “region” of overfitting and underfitting can be determined in your visualisation. Write (some of your) code into a function that takes \\(k\\) as an input and and returns the MAE determined on the test set. Is there an “optimal” \\(k\\) in terms of model generalisability? Edit your code to determine an optimal \\(k\\). Add code and text for addressing this exercise to the file ./vignettes/re_ml_01.Rmd and give the notebook a suitable structure for easy navigation with a table of content (toc) by modifying its YAML header: Important: to find an optimal \\(k\\), you will have to use daily data and not half-hourly data! Hint: Do not produce various of the “Training - Test” Figures shown above to find an optimal \\(k\\). Find a suitable plot that shows the optimal \\(k\\) (maybe you can find one in this or another Chapter…). --- title: &quot;Report Exercise Chapter 10&quot; author: &quot;Ziggy Stardust&quot; output: html_document: toc: true --- Deliverables for the report Present your solutions in a file called re_ml01.Rmd, save it in your vignettes folder alongside the HTML version, and make sure that your code is reproducible (make sure your .rmd is knittable, that all data is available, that paths to that data work, etc.). "],["supervisedmlii.html", "Chapter 10 Supervised machine learning II 10.1 Learning objectives 10.2 Tutorial 10.3 Exercises 10.4 Report Exercise", " Chapter 10 Supervised machine learning II Chapter lead author: Benjamin Stocker 10.1 Learning objectives In the Chapter 9, you learned how the data are pre-processed, the model fitted, and how the model’s generasbility to unseen data is tested. In the exercises of Chapter 9, you learned how the bias-variance trade-off of a model, a KNN, can be controlled and that the choice of model complexity has different implications of the model’s performance on the training and the test sets. A “good” model generalises well. That is, it performs well on unseen data. In this chapter, you will learn more about the process of model training, the concept of the loss, and how we can chose the right level of model complexity for optimal model generalisability as part of the model training step. This completes your set of skills for your first implementations a supervised machine learning workflow. 10.2 Tutorial 10.2.1 Data and the modelling challenge We’re using the same data and address the same modelling challenge as in Chapter 9. Let’s load the data, wrangle a bit, specify the same model formulation, and the same pre-processing steps as in the previous Chapter 9. daily_fluxes &lt;- readr::read_csv(&quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot;) |&gt; # select only the variables we are interested in dplyr::select(TIMESTAMP, GPP_NT_VUT_REF, # the target ends_with(&quot;_QC&quot;), # quality control info ends_with(&quot;_F&quot;), # includes all all meteorological covariates -contains(&quot;JSB&quot;) # weird useless variable ) |&gt; # convert to a nice date object dplyr::mutate(TIMESTAMP = lubridate::ymd(TIMESTAMP)) |&gt; # set all -9999 to NA dplyr::mutate(across(where(is.numeric), ~na_if(., -9999))) |&gt; # retain only data based on &gt;=80% good-quality measurements # overwrite bad data with NA (not dropping rows) dplyr::mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC &lt; 0.8, NA, GPP_NT_VUT_REF), TA_F = ifelse(TA_F_QC &lt; 0.8, NA, TA_F), SW_IN_F = ifelse(SW_IN_F_QC &lt; 0.8, NA, SW_IN_F), LW_IN_F = ifelse(LW_IN_F_QC &lt; 0.8, NA, LW_IN_F), VPD_F = ifelse(VPD_F_QC &lt; 0.8, NA, VPD_F), PA_F = ifelse(PA_F_QC &lt; 0.8, NA, PA_F), P_F = ifelse(P_F_QC &lt; 0.8, NA, P_F), WS_F = ifelse(WS_F_QC &lt; 0.8, NA, WS_F)) |&gt; # drop QC variables (no longer needed) dplyr::select(-ends_with(&quot;_QC&quot;)) # Data splitting set.seed(123) # for reproducibility split &lt;- rsample::initial_split(daily_fluxes, prop = 0.7, strata = &quot;VPD_F&quot;) daily_fluxes_train &lt;- rsample::training(split) daily_fluxes_test &lt;- rsample::testing(split) # The same model formulation is in the previous chapter pp &lt;- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = daily_fluxes_train) |&gt; recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |&gt; recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes()) To reproduce this code chunk, you can download the file FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv from here and read it from the local path where the file is stored on your machine. All data files used in this tutorials are stored here. 10.2.2 Loss function Model training in supervised machine learning is guided by the match (or mismatch) between the predicted and observed target variable(s), that is, between \\(\\hat{Y}\\) and \\(Y\\). The loss function quantifies this mismatch (\\(L(\\hat{Y}, Y)\\)), and the algorithm (“optimiser” in Fig. 10.1 of Chapter 9) takes care of progressively reducing the loss during model training. Let’s say the machine learning model contains two parameters and predictions can be considered a function of the two (\\(\\hat{Y}(w_1, w_2)\\)). \\(Y\\) is actually constant. Thus, the loss function is effectively a function \\(L(w_1, w_2)\\). Therefore, we can consider the model training as a search of the parameter space to find the minimum of the loss. The parameter space spanned by all possible combinations of \\((w_1, w_2)\\). Common loss functions are the root mean square error (RMSE), or the mean square error (MSE), or the mean absolute error (MAE). Loss minimization is a general feature of ML model training. Figure 10.1: Visualization of a loss function. Model training is implemented in R for different machine learning algorithms in different packages. Some algorithms are even implemented by multiple packages. As described in Chapter @ref(#supervisedmli), the {caret} package provides “wrappers” that handle a large selection of different machine learning model implementations in different packages with a unified interface (see here for an overview of available models). The {caret} function train() is the center piece also in terms of specifying the loss function as the argument metric. It defaults to the RMSE for regression models and the accuracy for classification. 10.2.3 Hyperparameters Practically all machine learning algorithms have some “knobs” to turn for controlling a model’s complexity and other features of the model training. The optimal choice of these “knobs” is to be found for efficient model performance. Such “knobs” are the hyperparameters. Each algorithm comes with its own, specific hyperparameters. For KNN, k is the (only) hyperparameter. It specifies the number of neighbours to consider for determining distances. There is always an optimum \\(k\\). Obviously, if \\(k = n\\), we consider all observations as neighbours and each prediction is simply the mean of all observed target values \\(Y\\), irrespective of the predictor values. This cannot be optimal and such a model is likely underfit. On the other extreme, with \\(k = 1\\), the model will be strongly affected by the noise in the single nearest neighbour and its generalisability will suffer. This should be reflected in a poor performance on the validation data. Hyperparameters usually have to be “tuned”. The optimal setting depends on the data and can therefore not be known a priori. Below is a visualisation of the loss (MAE) on the training and on the test set for different choices of \\(k\\). This illustrates that the model performance on the training set keeps improving as the model variance (as opposed to bias) increases - here as we go towards smaller \\(k\\). However, what counts for measuring out-of-sample model performance is the evaluation on the test set, which deteriorates with increasing model variance beyond a certain point. Although decisive for the generalisability of the model, we cannot evaluate its performance on the test set during model training. We have set that data aside and must leave it untouched to have a basis for evaluating the model performance on unseen data after training. What can we do to determine the optimal hyperparameter choice during model training, estimating its performance on the test set? 10.2.4 Resampling The solution is to split the training data again - now into a training and a validation set. Using the validation set, we can “mimick” out-of-sample predictions during training by determining the loss on the validation set and use that for guiding the model training. However, depending on the volume of data we have access to, evaluation metrics determined on the validation set may not be robust. Results may vary depending on the split into training and validation data. This makes it challenging for reliably estimating out-of-sample performance. A solution for this problem is to resample the training data into a number training-validation splits, yielding several pairs of training and validation data. Model training is then guided by minimising the average loss determined on the different resamples. Having multiple resamples (multiple folds of training-validation splits) avoids the loss minimization from being misguided by random peculiarities in the training and/or validation data. Whether or not a resampling is applied, depends on the data volume and computational costs of the model training which increase linearly with the number of resamples. For models that take days-weeks to train, resampling is not a realistic option. However, for many machine learning applications in Geography and Environmental Sciences, models are much less costly and resampling is viable and desirable approach to model training. The most important methods of resampling are bootstrapping (not explained here, but see Boehmke and Greenwell (2020)) and k-fold cross validation. An advantage of bootstrapping is that it provides an estimation of the distribution of the training error (without the need for data distribution assumptions because it’s non parametric), which informs not only the magnitude of the training error but also the variance of this estimate. Nevertheless, this statistical approach can become very computationally intensive because it needs many resamples with replacement and model runs. Hence cross validation lends itself more to model training. In k-fold cross validation, the training data is split into k equally sized subsets (folds). (Don’t confuse this k with the k in KNN.) Then, there will be k iterations, where each fold is used for validation once (while the remaining folds are used for training). An extreme case is leave-one-out cross validation, where k corresponds to the number of data points. Figure 10.2: K-fold cross validation. From Boehmke and Greenwell (2020). To do a k-fold cross validation during model training in R, we don’t have to implement the loops around folds ourselves. The resampling procedure can be specified in the {caret} function train() with the argument trControl. The object that this argument takes is the output of a function call to trainControl(). This can be implemented in two steps. For example, to do a 10-fold cross-validation, we can write: set.seed(1982) mod_cv &lt;- caret::train(pp, data = daily_fluxes_train |&gt; drop_na(), method = &quot;knn&quot;, trControl = caret::trainControl(method = &quot;cv&quot;, number = 10), tuneGrid = data.frame(k = c(2, 5, 10, 15, 20, 25, 30, 35, 40, 60, 100)), metric = &quot;MAE&quot;) # generic plot of the caret model object ggplot(mod_cv) # generic print print(mod_cv) ## k-Nearest Neighbors ## ## 1910 samples ## 8 predictor ## ## Recipe steps: center, scale ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 1719, 1718, 1718, 1719, 1720, 1718, ... ## Resampling results across tuning parameters: ## ## k RMSE Rsquared MAE ## 2 1.758390 0.5643266 1.331563 ## 5 1.579795 0.6321707 1.199857 ## 10 1.539660 0.6480432 1.163468 ## 15 1.522924 0.6549458 1.155488 ## 20 1.518210 0.6568319 1.152947 ## 25 1.516940 0.6574409 1.151223 ## 30 1.517698 0.6570899 1.153631 ## 35 1.517916 0.6570457 1.153421 ## 40 1.517559 0.6573507 1.153832 ## 60 1.522451 0.6556145 1.161452 ## 100 1.535410 0.6508217 1.177483 ## ## MAE was used to select the optimal model using the smallest value. ## The final value used for the model was k = 25. From the output of print(mod_cv), we get information about model performance for each hyperparameter choice. The values reported are means of respective metrics determined across the ten folds. Also the optimal choice of \\(k\\) (25) is reported. Does this correspond to the \\(k\\) with the best performance determined on the test set? If resampling was a good approach to estimating out-of-sample model performance, then it should! df_mae |&gt; # see hint filter(set == &quot;test&quot;) |&gt; filter(.estimate == min(.estimate)) ## # A tibble: 1 × 6 ## idx .metric .estimator .estimate k set ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 7 mae standard 1.15 30 test The object df_mae is created above, recording metrics determined on the training and the validation set for differen choices of \\(k\\). The respective code is hidden for the book. Explore the source code here. The evaluation on the test set suggests that \\(k=30\\) is optimal, while 10-fold cross-validation yielded an optimal \\(k = 25\\). Apparently, cross-validation suggested a model that is slightly more on the variance side along the bias-variance trade-off than the evaluation on the test set did. This (relatively small) mismatch is probably a result of randomness in the data. Let’s look at the results as we did in Chapter 9. The model object mod_cv contains information about the whole hyperparameter search and also about the choice of the best hyperparameter value. When using the object in the predict() function (as used inside eval_model()), it automatically uses the model trained with the optimal \\(k\\). eval_model(mod = mod_cv, df_train = daily_fluxes_train, df_test = daily_fluxes_test) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; Remember that in Chapter 9, we used \\(k=8\\) and got an \\(R^2=0.72\\) on the training set and \\(R^2=0.64\\) on the test set. The results with the optimal choice of \\(k=25\\) yield an \\(R^2=0.69\\) on the training set and \\(R^2=0.65\\) on the test set - poorer than with \\(k=8\\) on the training set but slightly better on the test set. 10.2.5 Validation versus testing data A source of confusion can be the distinction of validation and testing data. They are different things. The validation data is used during model training. The model fitting and the selection of the optimal hyperparameters is based on comparing predictions with the validation data. Hence, evaluations of true out-of-sample predictions should be done with a portion of the data that has never been used during the model training process (see Figure 10.3). Figure 10.3: Figure adopted form Google Machine Learning Crash Course. 10.2.6 Modeling with structured data A fundamental assumption underlying many machine learning algorithms and their training setup is that the data are independent and identically distributed (iid). This means that each observation is generated by the same process, follows the same distribution, and is independent from its neighboring point or any other data point. This assumption is often made in statistical models to simplify the analysis and to ensure the validity of various mathematical results used in machine learning algorithms. In reality, this is often not satisfied. In Chapter 3, we dealt with structure in the data in the context of formatting and aggregating data frames. Such structures are often also relevant for modelling and what it means for a model to “generalize well”. Remember that structure in data arises from similarity of the subjects generating the data. Such structures and their relation to the modelling task should be considered when choosing the model algorithm, formulating the model, and when implementing the training a testing setup. Consider, for example, the time series of ecosystem fluxes and meterological covariates in our dataset daily_fluxes. When using this data to train a KNN or a linear regression model, we implicitly assume that the data is iid. We assumed that there is a true function \\(f(X)\\) that generates the target data \\(Y\\) and that can be used to predict under any novel condition \\(X_\\text{new}\\). However, in reality, this may not be the case. \\(f(X)\\) may change over time. For example, over the course of a season, the physiology of plants changes (think phenology) and may lead to temporally varying relationships between \\(X\\) and \\(Y\\) that are not captured by temporal variations in \\(X\\) itself - the relationships are not stationary. Working with geographic and environmental data, we often deal with temporal dependencies between predictors and the target data. In our data daily_fluxes of ecosystem fluxes and meteorological covariates, this may be, as mentioned, arising from phenological changes in plant physiology and structure, or caused by a lasting effects weather extremes (e.g., a late frost event in spring). In hydrological data, temporal dependencies between weather and streamflow are generated by catchment characteristics and the runoff generation processes. Such temporal dependencies violate the independence assumption. Certain machine learning algorithms (e.g., recurrent neural networks) offer a solution for such cases and are suited for modelling temporal dependencies or, in general, sequential data where the order of the records matters (e.g., language models that consider the order of words in a text). Training-testing and cross-validation splits for sequential data have to preserve the order of the data in the subsets. In this case, the splits have to be done by blocks. That is, model generalisability is to be assessed by training on one block of the time series and testing on the remaining block. Note that the splitting method introduced in Chapter 9 using rsample::initial_split() assumes that the data is iid. In the function, data points are randomly drawn and allocated to either the test or the training subset. It is therefore not applicable for splitting data with temporal dependencies. Modelling temporal dependencies will be dealt in future (not currently available) chapters of this book. Other dependencies may arise from the spatial context. For example, a model for classifying an atmospheric pressure field as a high or a low pressure system uses information about the spatial arrangement of the data - in this case raster data. A model predicts one value (‘high pressure system’ or ‘low pressure system’) per pressure field. Such modelling tasks are dealt with yet another class of algorithms (e.g., convolutional neural networks). Spatial or group-related structure in the data may arise if, in general, the processes generating the data, cannot be assumed to be identical and lead to identically distributed data across groups. For example, in the data daily_fluxes_allsites_nested_joined from Chapter 3, time series of ecosystem fluxes and meteorological covariates are provided for a set of different sites. There, the group structure is linked to site identity. Similarly, streamflow data may be available for multiple catchments. However, considering the between-catchment variations in soil, terrain, vegetation, and geology, a model may not yield accurate predictions when trained by data from one catchment and applied to a different catchment. To evaluate model generalisability to a new site or catchment (not just a new time step within a single site or catchment), this structure has to be taken into consideration. In this, case, data splits of training and validation or testing subsets are to be separated along blocks, delineated by the similar groups of data points (by sites, or by catchments). That is, training data from a given site (or catchment) should be either in the training set or in the test (or validation) set, but not in both. This illustrates that the data structure and the modelling aim (generalisability in what respect?) have to be accounted for when designing the data split and resampling strategy. The {caret} function groupKFold() offers the solution for such cases, creating folds for cross-validation that respect group delineations. In other cases, such grouping structure may not be evident and may not be reflected by information accessible for modelling. For example, we may be able to separate time series from different sites but we don’t know whether sites are sufficiently independent to be able to consider the test metric to reflect the true uncertainty in predicting to an entirely new location which is neither in the test nor training set. In such cases, creative solutions have to be found and appropriate cross-validations have to be implemented with a view to the data structure and modelling aim. 10.3 Exercises Cross-validation by hand In the tutorial we “built on shoulder of giants” - people that went through the struggle of writing a robust package to implement a cross validation. Although we can and should use such packages, we still have to understand how a cross validation works in detail. Write a function that implements n-fold cross-validation for KNN with \\(k=30\\). (We write ‘n-fold CV’ here to avoid confusion with the k in KNN, but mean the same as described in Section 10.2.4.) The function should take as arguments the training data object, n for specifying the number of folds (use \\(n=60\\)), the name of the target variable, the names of the predictor variables as a character vector, and the \\(k\\) for KNN. The function should return a vector of length n, containing the MAE evaluated on the n folds. To randomize the split of data points into folds, first “re-shuffle” the rows in the training data as part of the function. Centering and scaling of the training and validation data should be applied manually before getting the KNN model object within each fold. As data, use daily ecosystem flux data from here. As target variable, use \"GPP_NT_VUT_REF\". As predictor variables, use c(\"TA_F\", \"SW_IN_F\", \"VPD_F\"). Visualise the distribution of the validation errors (MAE) across folds. Hint: Center and scale the predictors (but not the target data) within each fold. Hint: Use read.csv() to obtain a “nude” data frame (not a tibble) and use the syntax df[,\"variable_name\"] to extract a vector from a data frame. This makes your life easier. Hint: To determine the row indices to be allocated to each fold, you may find a solution using the base-R function split() or you may use caret::createFolds(). Hint: Write two functions: One as asked for above and one that implements steps that are carried out repeatedly for each fold, given row indices of the validation set of the respective fold. Hint: Since you are doing the cross-validation “by hand” and perform no hyperparameter tuning, we don’t need the caret::train() function. Instead, use caret::knnreg(x, y, ...) for creating the model object on the training data (with x and y being data frames) and the generic predict() for predicting on the validation set within each fold. 10.3.1 Cross-validation vs. test error Now, you can use the user-friendly caret::train() for KNN with 60-fold cross-validation and tuning the hyperparameter k. Use the MAE as the loss metric. Use the same data as in the Exercise above and withhold 20% of the data for the test set. Visually compare the reported mean MAE from the cross-validation folds with the MAE determined on a test set. In your visual comparison, add a plot layer showing the distribution of validation errors from you manual implementation of cross-validation (Exercise above). Hint: The returned model object from a caret::train() call is a list with fold-wise metrics in its element results. 10.4 Report Exercise Here, we explore the role of structure in the data for model generalisability and how to best estimate a “true” out-of-sample error that corresponds to the prediction task. The task here is to train a KNN-model on ecosystem flux observations from one site and predict to another site. In previous examples and exercises, we always trained and predicted within the same site. How well is a model generalisable to a new site; can we use our model for spatial upscaling? Solve these task and save your reproducible solution in ./vignettes/re_ml02.rmd (don`t forget to upload the data and html): To investigate the concept of generalisability, we use flux data from two sites: Davos (CH-Dav) (FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv), and Laegern (CH-Lae) (FLX_CH-Lae_FLUXNET2015_FULLSET_DD_2004-2014_1-4.csv). Gather information on the characteristics of these sites - how do they differ regarding climate, vegetation, altitude, etc. Train three KNN-models on each dataset: Davos only, Laegern only, both together. For each model: Use 80% of the data to train a KNN-model with an optimal \\(k\\) and set aside the remaining 20% for testing. Test your models on each of the three test datasets that you set aside. Create three tables (see example below), one for each model and display the evaluation metrics on all test sets. Example table: Model trained on data from [insert sitename/s] \\(R^2\\) \\(RMSE\\) Evaluation against Laegern test set Evaluation against Davos test set Evaluation against Laegern &amp; Davos test set Use your knowledge on statistical learning and on the different sites to answer the following questions: What are the patterns that you see in the tables? How well do models trained on one site perform on another site? Why is this the case? To answer this think of the differences between the sites! Interpret the models’ biases on the out-of-sample prediction with respect to the site’s charactersitics. Perhaps you could identify such biases by comparing the time-series of both models on the same test set? How does the model trained on both sites perform on the three test sets? Why is this the case? When training and testing on both sites, is this a true ‘out-of-sample’ setup? What would you expect if your model was tested against data from a site in Spain? Hint: To make the models comparable, make sure that you are using the same variables during training. Check your data before you start modelling! Hint: Do not produce multiple “Training-Test” plots shown throughout this course. Find a way to convey your message in concise way and pick these plots only when you really need them to support you answer. Hint: Consult the RMarkdown cheatsheet or other online resources to learn how to create a Markdown table. Deliverables for the report Present your solutions in a file called re_ml02.Rmd, save it in your vignettes folder alongside the HTML version, and make sure that your code is reproducible (make sure your .rmd is knittable, that all data is available, that paths to that data work, etc.). "],["randomforest.html", "Chapter 11 Random Forest 11.1 Learning objectives 11.2 Tutorial 11.3 Exercises", " Chapter 11 Random Forest Chapter lead author: Benjamin Stocker 11.1 Learning objectives In this chapter you will learn how to implement a Random Forest model. You will learn: the principles of a decision tree, and purpose of bagging how decision trees make up a Random Forest 11.2 Tutorial 11.2.1 Decision trees Just as a forest consists of trees, a Random Forest model consists of decision trees. Therefore, let’s start with a decision tree, also known as CART (classification and regression tree). Consider a similar example as in Chapter 9 where we fitted a function \\(Y = f(X)\\) with polynomials. Instead, here we fit it with a decision tree. The tree grows by successively splitting (branching) the predictor range (\\(X\\)) into binary classes (two regions along \\(X\\)) and predicting a different and constant value \\(c\\) for the target variable on either side of the split. The location of the split is chosen such that the overall error between the observed response (\\(Y_i\\)) and the predicted constant (\\(c_i\\)) is minimized. The error is determined based on whether we’re dealing with a regression or a classification problem. For regression problems, the sum of square errors is minimized. \\[ \\text{SSE} = \\sum_i{(c_i-Y_i)^2} \\] For classification problems, the cross-entropy or the Gini index are typically maximized. Both are measures of the difference between probability distributions (main ideas in this blogpost). Figure 11.1: A (very) shallow decision tree. Figure 11.2: A (very) shallow decision tree. As a tree grows, splits are recursively added. In our example, only one predictor variable is available. The splits are therefore performed always on the same variable, splitting up \\(X\\) further. Figure 11.3: A decision tree of depth 3. Figure 11.4: A decision tree of depth 3. In the visualisation of the decision tree above, the uppermost decision is the root node. From there, two branches connect to internal nodes, and at the bottom of the tree are the leaf nodes. (The nature-aware reader may note that leaves are typically at the top, and roots at the bottom of a tree. Nevermind.) Typically, multiple predictor variables are available. For each split, the variable and the location of the split along that variable is determined to satisfy the respective criteria for regression and classification. Decision trees are high variance learners. That is, as the maximum tree depth is increased, the variance of predictions increases continuously. In other words, the depth of a tree controls the model complexity and the bias-variance trade-off. With excessive depth, decision trees tend to overfit. Figure 11.5: An overfitting decision tree. Figure 11.6: An overfitting decision tree. An advantage of decision trees is that they require minimal pre-processing of the data and they are robust to outliers. This is thanks to their approach of converting continuous variables into binary classes for predictions. Hence, they can naturally handle a mix of continuous and categorical predictor variables. Furthermore, predictions are not sensitive to the distance of a predictor variable’s value to a respective variable’s split location. This makes decision trees robust to outliers. It also implies that predictions to unseen data points that lie outside the range of values in the training data (extrapolation) are conservative. The disadvantage is, as demonstrated above, the tendency towards high variance of predictions when models get complex (deep trees). And thus, decision trees tend to be outperformed by other algorithms. 11.2.2 Bagging The approach of bagging is to smooth out the high variance of decision tree predictions by averaging over multiple, slightly differet trees. Differences between the trees are introduced by bagging, that is, by training each individual tree only on a bootstrapped sample of the full training data. Here, a decision tree has the role of a base learner and bagging takes an ensemble approach. Final predictions are then taken as the average (for regression) or the most frequent class (for classification) across all trees’ predictions. Bagging is particularly effective when the base learner tends to have a high variance. The variance of predictions is continuously reduced with an increasing number of decision trees over which averages are taken and no tendency to overfit results from increasing the number of trees. However, the computational cost linearly increases with the number of trees and the gain in model performance levels out relatively quickly. Bagging also limits the interpretability. We can no longer visualise the fitted model with an intuitive graphical decision tree as done above. 11.2.3 From trees to a forest While bagging reduces the variance in predictions, limits to predictive performance remain. This is linked to the fact that, although a certain degree of randomness is introduced by sub-sampling the training data for each tree, individual trees often remain relatively similar. This is particularly expressed if variations in the target variable are controlled primarily by variations in a small number of predictor variables. In this case, decision trees tend to be built by splits on the same variable, irrespective of which bootstrapped sample the individual tree is trained with. Random Forest solves this problem by introducing an additional source of randomness: Only a subset of the predictor variables are considered at each split. This strongly reduces the similarity of individual trees (and also reduces computational costs) and leads to improved predictive performance. The number of variables to consider at each split is a hyperparameter of the Random Forest algorithm, commonly named \\(m_\\text{try}\\). In the example below, we use the implementation in the {ranger} package (wrapped with {caret}), where the respective model fitting function has a hyperparameter mtry. Common default values are \\(m_\\text{try} = p/3\\) for regression and \\(m_\\text{try} = \\sqrt{p}\\) for classification, where \\(p\\) is the number of predictors. Model complexity is controlled by the depth of the trees. Depending on the implementation of the Random Forest algorithm, this is governed not by explicitly specifying the tree depth, but by setting the number of observations in the leaf node. In the {ranger} package, the respective hyperparamter is min.node.size. The number of trees is another hyperparameter and affects predictions similarly as described above for bagging. A great strength of Random Forest is, inherited by the characteristics of its underlying decision trees, its minimal requirement of data pre-processing, its capability of dealing with continuous and categorical variables, and its robustness to outliers. With the default choices of \\(m_\\text{try}\\), Random Forest provides very good out-of-the-box performance. However, the hyperparameters of Random Forest have interactive effects and should be searched systematically. # Data loading and cleaning source(&quot;R/eval_model.R&quot;) # read daily fluxes daily_fluxes &lt;- readr::read_csv( &quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot; ) # select only the variables we are interested in # such as the target (GPP_NT_VUT_REF), the # quality control parameters ending in QC, # the meteorological covariates ending in F, # while removing (-) a weird useless variable JSB # # Subsequently convert dates to a nice date format # set all -9999 to NA and only retain values with # QC value of &gt; 0.8 daily_fluxes &lt;- daily_fluxes |&gt; # select only the variables we are interested in dplyr::select(TIMESTAMP, GPP_NT_VUT_REF, # the target ends_with(&quot;_QC&quot;), # quality control info ends_with(&quot;_F&quot;), # includes all all meteorological covariates -contains(&quot;JSB&quot;) # weird useless variable ) |&gt; # convert to a nice date object dplyr::mutate(TIMESTAMP = ymd(TIMESTAMP)) |&gt; # set all -9999 to NA dplyr::mutate(across(where(is.numeric), ~na_if(., -9999))) |&gt; # retain only data based on &gt;=80% good-quality measurements # overwrite bad data with NA (not dropping rows) dplyr::mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC &lt; 0.8, NA, GPP_NT_VUT_REF), TA_F = ifelse(TA_F_QC &lt; 0.8, NA, TA_F), SW_IN_F = ifelse(SW_IN_F_QC &lt; 0.8, NA, SW_IN_F), LW_IN_F = ifelse(LW_IN_F_QC &lt; 0.8, NA, LW_IN_F), VPD_F = ifelse(VPD_F_QC &lt; 0.8, NA, VPD_F), PA_F = ifelse(PA_F_QC &lt; 0.8, NA, PA_F), P_F = ifelse(P_F_QC &lt; 0.8, NA, P_F), WS_F = ifelse(WS_F_QC &lt; 0.8, NA, WS_F)) |&gt; # drop QC variables (no longer needed) dplyr::select(-ends_with(&quot;_QC&quot;)) # Data splitting set.seed(123) # for reproducibility split &lt;- rsample::initial_split(daily_fluxes, prop = 0.7, strata = &quot;VPD_F&quot;) ddf_train &lt;- rsample::training(split) ddf_test &lt;- rsample::testing(split) # The same model formulation is in the previous chapter pp &lt;- recipes::recipe(GPP_NT_VUT_REF ~ TA_F + SW_IN_F + LW_IN_F + VPD_F + P_F + WS_F, data = ddf_train) |&gt; recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |&gt; recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes()) mod &lt;- train( pp, data = ddf_train %&gt;% drop_na(), method = &quot;ranger&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 5, savePredictions = &quot;final&quot;), tuneGrid = expand.grid( .mtry = floor(6 / 3), .min.node.size = 5, .splitrule = &quot;variance&quot;), metric = &quot;RMSE&quot;, replace = FALSE, sample.fraction = 0.5, num.trees = 2000, # high number ok since no hperparam tuning seed = 1982 # for reproducibility ) # generic print print(mod) ## Random Forest ## ## 1910 samples ## 8 predictor ## ## Recipe steps: center, scale ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 1528, 1528, 1529, 1527, 1528 ## Resampling results: ## ## RMSE Rsquared MAE ## 1.411155 0.7047382 1.070285 ## ## Tuning parameter &#39;mtry&#39; was held constant at a value of 2 ## Tuning ## parameter &#39;splitrule&#39; was held constant at a value of variance ## ## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 5 eval_model(mod = mod, df_train = ddf_train, df_test = ddf_test) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; With this, you are equipped with a powerful method and the fundamental understanding of concepts and tools for supervised machine learning applications. This is the basis for your journey in Applied Geodata Science. Congratulations and have fun! 11.3 Exercises Fitting a Random Forest Fit a Random Forest model to the flux data used in the examples of this chapter. Implement bagging 12 decision trees (num.trees), each with a minimum number of observations per leaf of 5 (min.node.size). You can consult the respective arguments for the \"ranger\" method typing ?ranger. Repeat the fitting with 1000 decision trees and minimum node size of 5, then with 12 decision trees and a minimum node size of 1. Then, discuss the role that the number of decision trees and the minimum number of leaf observations of a tree play in the bias-variance trade-off and in the computation time. Hyperparameter tuning In a previous tutorial, you learned how to tune the hyperparameter \\(k\\) in a KNN by hand. Now you will do the hyperparameter tuning for a Random Forest model. The task gets more complicated because there are more hyperparameters in a Random Forest. The {caret} package allows to vary three hyperparameters: mtry: The number of variables to consider to make decisions at each node, often taken as \\(p/3\\) for regression, where \\(p\\) is the number of predictors. min.node.size: The number of data points at the “bottom” of each decision tree, i.e. the leaves. splitrule: The function applied to data in each branch of a tree, used for determining the goodness of a decision. Answer the following questions, giving a reason for your responses: Check the help for the ranger() function and identify which values each of the three hyperparameters/arguments can take. Select a sensible range of values for each hyperparameter, that you will use in the hyperparameter search. In the previous exercise, you have seen how the minimum node size regulates fit quality and overfitting. How does the minimum node size relate to tree depth? What happens at the edge cases, when min.node.size = 1 and when min.node.size = n (n being the number of observations)? Note that it’s not necessary to provide the max.depth argument to train() because min.node.size is already limiting the size of the trees in the Random Forests. Greedy hyperparameter tuning: Sequentially optimize the choice of each hyperparameter, one at a time and keeping the other two constant. Take the code from the tutorial as a starting point, and those hyperparameter values as an initial point for the search. Implement the optimization routine yourself, using loops. Tip: Keep the number of trees low, otherwise it takes too long to fit each Random Forest model. Grid hyperparameter tuning: Starting with the same range of values for each hyperparameter as before, look for the combination that leads to the best model performance among all combinations of hyperparameter values. This time, use the expand.grid() function to create a data.frame of hyperparameter value combinations. This grid will be passed to train() via the tuneGrid argument (see example in the tutorial). This will automatically do the hyperparameter search for you. Comment the output of train() and the results of the hyperparameter search. Compare the results from the two hyperparameter tuning approaches. Do the optimal hyperparameters coincide? Are the corresponding RMSE estimates similar? What are the advantages and disadvantages of the greedy and the grid approaches? Model performance You have trained several Random Forest models. Evaluate the model performance on the best model (the one for the tuned hyperparameters) and on one of your worse models. If you compare the RMSE and \\(R^2\\) on the training and the test set, does it show overfitting? "],["interpretableml.html", "Chapter 12 Interpretable Machine Learning 12.1 Partial dependence plots 12.2 Variable importance from permutation", " Chapter 12 Interpretable Machine Learning A great advantage of machine learning models such as Random Forests or Neural Networks is that they can capture non-linear relationships and faint but relevant patterns in the dataset. However, this complexity comes with the trade-off that models turn into black-box models that cannot be interpreted easily. Therefore, model interpretation is crucial to demystify the decision-making processes of complex algorithms. In this chapter, we introduce you to a few key techniques to investigate the inner workings of your models. To give examples for model interpretation, we re-use the Random Forest model that we created in Chapter 11. As a reminder, we predicted GPP from different environmental variables such as temperature, short-wave radiation, vapor pressure deficit, and others. # The Random Forest model requires the following models to be loaded: require(caret) require(ranger) rf_mod &lt;- readRDS(&quot;data/tutorials/rf_mod.rds&quot;) rf_mod ## Random Forest ## ## 1910 samples ## 8 predictor ## ## Recipe steps: center, scale ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 1528, 1528, 1529, 1527, 1528 ## Resampling results: ## ## RMSE Rsquared MAE ## 1.411155 0.7047382 1.070285 ## ## Tuning parameter &#39;mtry&#39; was held constant at a value of 2 ## Tuning ## parameter &#39;splitrule&#39; was held constant at a value of variance ## ## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 5 Sidenote: There are two types of model interpretation: model-specific and model-agnostic interpretation. Here, we will focus on the latter as they can be applied to most machine learning models. 12.1 Partial dependence plots Partial dependence plots (PDP) give insight on the marginal effect of a single predictor variable on the response when all other predictors are kept constant. The algorithm to create PDPs goes as follows (adapted from Boehmke &amp; Greenwell (2019)): For a selected predictor (x) 1. Construct a grid of j evenly spaced values across the range of x: {x1, x2, ..., xj} 2. For i in {1,...,j} do | Copy the training data and replace the original values of x with the constant xi | Apply the fitted ML model to obtain vector of predictions for each data point. | Average predictions across all data points. End 3. Plot the averaged predictions against x1, x2, ..., xj Written-out, this means that we create a vector \\(j\\) of length \\(i\\) that holds evenly spaced values of our variable of interest \\(x\\). E.g., if the temperature in our dataset varies from 1 to 20, and we choose \\(i=20\\), we get a vector \\([j_1 = 1, j_2 =2, ..., j_{20} = 20 ]\\). Now, we create \\(i\\) copies of our training dataset and for each copy, we over-write the temperature data with the respective value in our vector. The first copy will have all temperature values set to 1, the second to 2, etc. Then, we use the model to calculate the response value for each entry in all copies. Per copy, we take the average response (\\(\\text{mean}(\\hat{Y})\\)) and plot that average against the value of the variable of interest. This gives us the response across a range of our variable of interest, whilst all other variables to not change. Therefore, we get the partial dependence of the response on the variable of interest. Here’s an illustration to make this clearer: Figure 12.1: Visualisation of Partial Dependence Plot algorithm from Boehmke &amp; Greenwell (2019). Here, Gr_Liv_Area is the variable of interest \\(x\\). Luckily, we do not have to write the algorithm ourselves but can directly use the {pdp} packages: # The predictor variables are saved in our model&#39;s recipe preds &lt;- rf_mod$recipe$var_info |&gt; dplyr::filter(role == &quot;predictor&quot;) |&gt; dplyr::pull(variable) # The partial() function can take n=3 predictors at max and will try to create # a n-dimensional visulaisation to show interactive effects. However, # this is computational intensive, so we only look at the simple # response-predictor plots all_plots &lt;- list() for (p in preds) { all_plots[[p]] &lt;- pdp::partial( rf_mod, # Model to use p, # Predictor to assess plot = TRUE # Whether output should be a plot or dataframe ) } pdps &lt;- cowplot::plot_grid(all_plots[[1]], all_plots[[2]], all_plots[[3]], all_plots[[4]], all_plots[[5]], all_plots[[6]]) pdps These PDPs show that the variables VPD_F, P_F, and WS_F have a relatively small marginal effect as indicated by the small range in yhat. The other three variables however have quite an influence. For example, between 0 and 10 \\(^\\circ\\)C, the temperature variable TA_F causes a rapid increase in yhat, so the model predicts that temperature drives GPP strongly within this range but not much below 0 or above 10 \\(^\\circ\\)C. The pattern is relatively similar for LW_IN_F, which is sensible because long-wave radiation is highly correlated with temperature. For the short-wave radiation SW_IN_F, we see the saturating effect of light on GPP that we saw in previous chapters. 12.2 Variable importance from permutation The PDPs discussed above give us a general feeling of how important a variable is in our model but they do not quantify this importance directly (but see measures for the “flatness” of a PDP here). However, we can measure variable importance directly through a permutation procedure. Put simply, this means that we replace values in our training dataset with random values (i.e., we permute the dataset) and assess how this permutation affects the model’s performance. Permuting an important variable with random values will destroy any relationship between that variable and the response variable. The model’s performance given by a loss function, e.g. its RMSE, will be compared between the non-permuted and permuted model to assess how influential the permuted variable is. A variable is considered to be important, when its permutation increases the model error relative to other variables. Vice versa, permuting an unimportant variable does not lead to a (strong) increase in model error. The algorithm works as follows (taken from Boehmke &amp; Greenwell (2019)): For any given loss function do the following: 1. Compute loss function for original model 2. For variable i in {1,...,p} do | randomize values | apply given ML model | estimate loss function | compute feature importance (some difference/ratio measure between permuted loss &amp; original loss) End 3. Sort variables by descending feature importance Again, we can rely on others who have already implemented this algorithm in the {vip} package. Note that the {vip} package has model-specific algorithms implemented but also takes model-agnostic arguments as done below. vip::vip(rf_mod, # Model to use train = rf_mod$trainingData, # Training data used in the model method = &quot;permute&quot;, # VIP method target = &quot;GPP_NT_VUT_REF&quot;, # Target variable nsim = 5, # Number of simulations metric = &quot;RMSE&quot;, # Metric to assess quantify permutation sample_frac = 0.75, # Fraction of training data to use pred_wrapper = predict # Prediction function to use ) In line with the results from the PDPs, we see that the variables SW_IN_F, TA_F, and LW_IN_F are most influential. "],["solutions.html", "A Solutions A.1 Getting Started A.2 Programming primers A.3 Data wrangling A.4 Data Visualisation A.5 Data Variety A.6 Open Science A.7 Code Management A.8 Regression and Classification A.9 Supervised ML I A.10 Supervised ML II A.11 Random Forest", " A Solutions A.1 Getting Started Dimensions of a circle Given the radius of a circle write a few lines of code that calculates its area and its circumference. Run your code with different values assigned of the radius. radius &lt;- 1 area &lt;- pi * radius^2 circum &lt;- 2 * pi * radius Print the solution as text. print(paste(&quot;Radius:&quot;, radius, &quot; Circumference:&quot;, circum)) ## [1] &quot;Radius: 1 Circumference: 6.28318530717959&quot; Sequence of numbers Generate a sequence of numbers from 0 and \\(\\pi\\) as a vector with length 5. seq(0, pi, length.out = 5) ## [1] 0.0000000 0.7853982 1.5707963 2.3561945 3.1415927 Gauss sum Rumors have it that young Carl Friedrich Gauss was asked in primary school to calculate the sum of all natural numbers between 1 and 100. He did it in his head in no time. We’re very likely not as intelligent as young Gauss. But we have R. What’s the solution? sum(1:100) ## [1] 5050 Gauss calculated the sum with a trick. The sum of 100 and 1 is 101. The sum of 99 and 2 is 101. You do this 50 times, and you get \\(50 \\times 101\\). Demonstrate Gauss’ trick with vectors in R. vec_a &lt;- 1:50 vec_b &lt;- 100:51 vec_c &lt;- vec_a + vec_b # each element is 101 vec_c ## [1] 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 ## [20] 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 101 ## [39] 101 101 101 101 101 101 101 101 101 101 101 101 # the length of vectors is fifty. 50 * 101 sum(vec_c) ## [1] 5050 Magic trick algorithm Define a variable named x that contains an integer value and perform the following operations in sequence: Redefine x by adding 1. Double the resulting number, over-writing x. Add 4 to x and save the result as x. Redefine x as half of the previous value of x. Subtract the originally chosen arbitrary number from x. Print x. Restart the algorithm defined above by choosing a new arbitrary natural number. x &lt;- -999 # arbitrary integer x_save &lt;- x # save for the last step x &lt;- x + 1 x &lt;- x * 2 x &lt;- x + 4 x &lt;- x / 2 x - x_save ## [1] 3 Vectors Print the object datasets::rivers and consult the manual of this object. What is the class of the object? What is the length of the object? Calculate the mean, median, minimum, maximum, and the 33%-quantile across all values. class(datasets::rivers) ## [1] &quot;numeric&quot; length(datasets::rivers) ## [1] 141 mean(datasets::rivers) ## [1] 591.1844 quantile(datasets::rivers, probs = 0.33) ## 33% ## 342 Data frames Print the object datasets::quakes and consult the manual of this object. Determine the dimensions of the data frame using the respective function in R. Extract the vector of values in the data frame that contain information about the Richter Magnitude. Determine the value largest value in the vector of event magnitudes. Determine the geographic position of the epicenter of the largest event. dim(datasets::quakes) ## [1] 1000 5 vec &lt;- datasets::quakes$mag max(vec) ## [1] 6.4 idx &lt;- which.max(vec) # index of largest value # geographic positions defined by longitude and latitude (columns long and lat) datasets::quakes$long[idx] ## [1] 167.62 datasets::quakes$lat[idx] ## [1] -15.56 Workspace No solutions provided. A.2 Programming primers Gauss variations # for-loop to compute sum from 1 - 100 sum &lt;- 0 for (i in 1:100){ sum &lt;- sum + i # for-loop iterating from 1 to 100 } print(sum) ## [1] 5050 # while-loop to compute sum from 1 - 100 loop_status &lt;- TRUE counter &lt;- 0 sum &lt;- 0 while (loop_status) { # while-loop is repeated as long as loop_status is true counter &lt;- counter + 1 sum &lt;- sum + counter if (counter == 100) loop_status &lt;- FALSE } print(sum) ## [1] 5050 # Initiate sum variable sum &lt;- 0 # Go through loop from 1 to 100 for (i in seq(100)) { # Check if the current number a muliple of three and seven # The modulo operator &#39;%%&#39; returns the remainder of a division if (i %% 3 == 0 &amp;&amp; i %% 7 == 0 ) { sum &lt;- sum + i } } print(paste0(&quot;The sum of multiples of 3 and 7 within 1-100 is: &quot;, sum)) ## [1] &quot;The sum of multiples of 3 and 7 within 1-100 is: 210&quot; Nested loops mymat &lt;- matrix(c(6, 7, 3, NA, 15, 6, 7, NA, 9, 12, 6, 11, NA, 3, 9, 4, 7, 3, 21, NA, 6, rep(NA, 7)), nrow = 4, byrow = TRUE) myvec &lt;- c(8, 4, 12, 9, 15, 6) # Loop over the rows in `mymat`. for (i in 1:nrow(mymat)){ # Loop over the columns in `mymat`. for (j in 1:ncol(mymat)){ # Check if current value is missing, if so overwrite with max in &#39;myvec&#39; if (is.na(mymat[i,j])){ mymat[i,j] &lt;- max(myvec) } } myvec &lt;- myvec[-which.max(myvec)] # update the vector removing the maximum value } mymat ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 6 7 3 15 15 6 7 ## [2,] 12 9 12 6 11 12 3 ## [3,] 9 4 7 3 21 9 6 ## [4,] 8 8 8 8 8 8 8 Interpolation # Set up vector as required in the exercise vec &lt;- rep(NA, 100) # initialize vector of length 100 with NA vec[1:25] &lt;- 6 # populate first 25 elements of &#39;vec&#39; with 6. vec[66:100] &lt;- -20 # populate elements 66:100 with -20. # Determine index of last non-missing value before gap last_non_na &lt;- 1 while (!is.na(vec[last_non_na+1])) last_non_na &lt;- last_non_na + 1 # determine index of first non-missing value after gap first_non_na &lt;- last_non_na + 1 while (is.na(vec[first_non_na])) first_non_na &lt;- first_non_na + 1 # Get the increment that is needed for interpolation last_value &lt;- vec[last_non_na] # Last non-NA value first_value &lt;- vec[first_non_na] # First non-NA value delta &lt;- (last_value - first_value) / (last_non_na - first_non_na) # Change in y over change in x # fill missing values incrementally for (i in 2:length(vec)){ if (is.na(vec[i])) vec[i] &lt;- vec[i-1] + delta } plot(vec) # or short using the approx() function: vec &lt;- rep(NA, 100) # initialize vector of length 100 with NA vec[1:25] &lt;- 6 # populate first 25 elements of &#39;vec&#39; with 6. vec[66:100] &lt;- -20 # populate elements 66:100 with -20. vec &lt;- approx(1:100, vec, xout = 1:100) plot(vec) A.3 Data wrangling Star wars {dplyr} comes with a toy dataset dplyr::starwars (just type it into the console to see its content). Have a look at the dataset with View(). Play around with the dataset to get familiar with the {tidyverse} coding style. Use (possibly among others) the functions dplyr::filter(), dplyr::arrange(), dplyr::pull(), dplyr::select(), dplyr::desc() and dplyr::slice() to answer the following question: How many pale characters come from the planets Ryloth or Naboo? dplyr::starwars |&gt; dplyr::filter( skin_color == &quot;pale&quot; &amp; (homeworld == &quot;Naboo&quot; | homeworld == &quot;Ryloth&quot;) ) |&gt; nrow() ## [1] 2 Who is the oldest among the tallest thirty characters? dplyr::starwars |&gt; arrange(desc(height)) |&gt; slice(1:30) |&gt; arrange(birth_year) |&gt; slice(1) |&gt; pull(name) ## [1] &quot;IG-88&quot; What is the name of the shortest character and their starship in “Return of the Jedi”? dplyr::starwars |&gt; unnest(films) |&gt; filter(films == &quot;Return of the Jedi&quot;) |&gt; unnest(starships) |&gt; arrange(height) |&gt; slice(1) |&gt; select(name, starships) ## # A tibble: 1 × 2 ## name starships ## &lt;chr&gt; &lt;chr&gt; ## 1 Nien Nunb Millennium Falcon Aggregating You have learned about aggregating in the {tidyverse}. Let’s put it in practice. Reuse the code in the tutorial to read, reduce, and aggregate the half_hourly_fluxes dataset to the daily scale, calculating the following metrics across half-hourly VPD_F values within each day: mean, 25% quantile, and 75% quantile. # read half hourly fluxes half_hourly_fluxes &lt;- readr::read_csv( &quot;data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv&quot; ) # Select only variables that we are interested in half_hourly_fluxes &lt;- half_hourly_fluxes |&gt; dplyr::select( starts_with(&quot;TIMESTAMP&quot;), ends_with(&quot;_F&quot;), GPP_NT_VUT_REF, NEE_VUT_REF_QC, -starts_with(&quot;SWC_F_MDS_&quot;), -contains(&quot;JSB&quot;) ) # Clean the datetime objects # and aggregate to daily scale daily_fluxes &lt;- half_hourly_fluxes |&gt; dplyr::mutate( date_time = lubridate::ymd_hm(TIMESTAMP_START), date = lubridate::date(date_time)) |&gt; dplyr::group_by(date) |&gt; dplyr::summarise( mean = mean(VPD_F), q25 = quantile(VPD_F, probs = 0.25), q75 = quantile(VPD_F, probs = 0.75) ) Retain only the daily data for which the daily mean VPD is among the upper or the lower 10% quantiles. # In two steps. First, get thresholds of the quantiles thresholds &lt;- quantile(daily_fluxes$mean, probs = c(0.1, 0.9)) # Then, filter data to be above/below the upper/lower quantiles and combine daily_fluxes_sub &lt;- daily_fluxes |&gt; # in lower 10% quantile filter(mean &lt; thresholds[1]) |&gt; mutate(qq = &quot;lower&quot;) |&gt; # add label # combine bind_rows( daily_fluxes |&gt; # in upper 90% quantile filter(mean &gt; thresholds[2]) |&gt; mutate(qq = &quot;upper&quot;) ) Calculate the mean of the 25% and the mean of the 75% quantiles of half-hourly VPD within the upper and lower 10% quantiles of mean daily VPD. daily_fluxes_sub |&gt; group_by(qq) |&gt; summarise( q25_mean = mean(q25), q75_mean = mean(q75) ) ## # A tibble: 2 × 3 ## qq q25_mean q75_mean ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 lower 0.0989 0.149 ## 2 upper 6.56 15.2 Patterns in data quality The uncleaned dataset FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv holds half-hourly data that is sometimes of poor quality. Investigate whether NEE data quality is randomly spread across hours in a day by calculating the proportion of (i) actually measured data, (ii) good quality gap-filled data, (iii) medium quality data, and (iv) poor quality data within each hour-of-day (24 hours per day). # using half_hourly_fluxes read above daily_fluxes &lt;- half_hourly_fluxes |&gt; mutate(TIMESTAMP_START = lubridate::ymd_hm(TIMESTAMP_START)) |&gt; mutate(hour_of_day = lubridate::hour(TIMESTAMP_START)) |&gt; group_by(hour_of_day) |&gt; summarise(n_measured = sum(NEE_VUT_REF_QC == 0), n_good = sum(NEE_VUT_REF_QC == 1), n_medium = sum(NEE_VUT_REF_QC == 2), n_poor = sum(NEE_VUT_REF_QC == 3), n_total = n() ) |&gt; mutate(f_measured = n_measured / n_total, f_good = n_good / n_total, f_medium = n_medium / n_total, f_poor = n_poor / n_total, ) Interpret your findings: Are the proportions evenly spread across hours in a day? # this is not asked for but interesting. More on data visualisation in Chapter 5 # you can also just look at values of df$f_measured over the course of a day (hod) daily_fluxes |&gt; pivot_longer(c(f_measured, f_good, f_medium, f_poor), names_to = &quot;quality&quot;, values_to = &quot;fraction&quot;) |&gt; ggplot(aes(x = hour_of_day, y = fraction * 100, # *100 to get percentages color = quality)) + geom_line(linewidth = 1.5) + # make lines bit bigger theme_classic() + # Pick a nice theme scale_color_brewer( # Pick a nice color palette &quot;Quality&quot;, # Give legend a title labels = c(&quot;Good gap filled data&quot;, &quot;Measured data&quot;, &quot;Medium gap filled data&quot;, &quot;Poor gap filled data&quot;), # Give legend levels a label palette = 3, # Pick color palette direction = -1 # Inverse order of color palette ) + labs( title = &quot;Temporal pattern of GPP quality&quot;, y = &quot;Fraction of total GPP entries [%]&quot;, x = &quot;Hour of Day&quot; ) Perform an aggregation of the half-hourly GPP data (variable GPP_NT_VUT_REF) to daily means of the unmodified data read from file FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv, and from cleaned data where only measured (not gap-filled) half-hourly data is kept and aggregated. This yields two data frames with daily GPP data. Calculate the overall mean GPP for the two data frames (across all days in the data frame). Are the overall mean GPP values equal? If not, why? daily_fluxes_all &lt;- half_hourly_fluxes |&gt; dplyr::mutate( date_time = lubridate::ymd_hm(TIMESTAMP_START), date = lubridate::date(date_time) ) |&gt; dplyr::group_by(date) |&gt; dplyr::summarise( GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF, na.rm = TRUE) ) daily_fluxes_cleaned &lt;- half_hourly_fluxes |&gt; dplyr::mutate( date_time = lubridate::ymd_hm(TIMESTAMP_START), date = lubridate::date(date_time) ) |&gt; dplyr::mutate( GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC == 0, GPP_NT_VUT_REF, NA) ) |&gt; dplyr::group_by(date) |&gt; dplyr::summarise( GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF, na.rm = TRUE) ) # overall means daily_fluxes_all |&gt; summarise( GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF, na.rm = TRUE) ) ## # A tibble: 1 × 1 ## GPP_NT_VUT_REF ## &lt;dbl&gt; ## 1 4.20 daily_fluxes_cleaned |&gt; summarise( GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF, na.rm = TRUE) ) ## # A tibble: 1 × 1 ## GPP_NT_VUT_REF ## &lt;dbl&gt; ## 1 7.07 A.4 Data Visualisation Spurious data In Section 3.2.10.2, we discovered that certain values of GPP_NT_VUT_REF in the half-hourly data half_hourly_fluxes (to be read from file data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv) are repeated with a spuriously high frequency. Determine all values of GPP_NT_VUT_REF that appear more than once in half_hourly_fluxes and label them as being “spurious”. Visualise the time series of the first two years of half-hourly GPP, mapping the information whether the data is spurious or not to the color aesthetic. # Read and wrangle data half_hourly_fluxes &lt;- readr::read_csv(&quot;data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv&quot;) |&gt; # set all -9999 to NA dplyr::mutate(dplyr::across(dplyr::where(is.numeric), ~dplyr::na_if(., -9999))) |&gt; # interpret all variables starting with TIMESTAMP as a date-time object dplyr::mutate_at(vars(starts_with(&quot;TIMESTAMP_&quot;)), lubridate::ymd_hm) ## Rows: 52608 Columns: 235 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (235): TIMESTAMP_START, TIMESTAMP_END, TA_F_MDS, TA_F_MDS_QC, TA_ERA, TA... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # determine spurious GPP_NT_VUT_REF values as those that are duplicated # this creates a logical vector specifying whether the respective row has a # duplicate vec_spurious &lt;- half_hourly_fluxes |&gt; # by keeping only one column, duplicated() determines duplications in # that variable only select(GPP_NT_VUT_REF) |&gt; duplicated() # label spurious half-hourly data half_hourly_fluxes &lt;- half_hourly_fluxes |&gt; mutate(spurious = vec_spurious) # visualise ggplot( data = half_hourly_fluxes |&gt; slice(1:(48*365)), aes(x = TIMESTAMP_START, y = GPP_NT_VUT_REF)) + geom_line() + geom_point(aes(color = spurious), size = 0.9) + labs(x = &quot;Time&quot;, y = expression(paste(&quot;GPP (&quot;, mu,&quot;mol CO&quot;[2], &quot; m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + scale_color_viridis_d() + # inverse color scale is more intuitive here theme_classic() Then aggregate half-hourly to daily data, taking the mean of GPP_NT_VUT_REF and recording the proportion of underlying half-hourly data points that are “spurious”. Visualise the time series of daily GPP_NT_VUT_REF with the color scale indicating the proportion of spurious half-hourly data that was used for determining the respective date’s mean GPP. # aggregate daily_fluxes &lt;- half_hourly_fluxes |&gt; mutate(date = lubridate::date(TIMESTAMP_START)) |&gt; group_by(date) |&gt; summarise(frac_spurious = sum(spurious)/48, GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF)) # visualise ggplot( data = daily_fluxes, aes(x = date, y = GPP_NT_VUT_REF)) + geom_line() + geom_point(aes(color = frac_spurious), size = 0.9) + labs(x = &quot;Time&quot;, y = expression(paste(&quot;GPP (&quot;, mu,&quot;mol CO&quot;[2], &quot; m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + scale_color_viridis_c(direction = -1) + # inverse color scale is more intuitive here theme_classic() Identifying Outliers A key part of data cleaning is to detect and understand outliers. Visualisations can help. Your task here is to find outliers in GPP_NT_VUT_REF. First, using the half-hourly fluxes data, determine “outliers” as those values of GPP_NT_VUT_REF that fall outside \\(( Q_1 - 1.5 (Q_3 - Q_1)\\) to \\(Q_3 + 1.5 (Q_3 - Q_1)\\). Plot GPP_NT_VUT_REF versus shortwave radiation and highlight outliers in red. Hint: Use boxplot.stats() to return a list containing a vector of the data points which lie beyond the extremes of the whiskers of the boxplot. Hint: Use scale_color_manual() to mannually define the color scale. vec_outliers &lt;- boxplot.stats(half_hourly_fluxes$GPP_NT_VUT_REF)$out plot_data &lt;- half_hourly_fluxes |&gt; mutate(outlier = GPP_NT_VUT_REF %in% vec_outliers) plot_data |&gt; ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = outlier)) + geom_point() + scale_color_manual(&quot;Outlier?&quot;, # Set title of legend values = c(&quot;black&quot;, &quot;red&quot;), # Highlight in red labels = c(&quot;No&quot;, &quot;Yes&quot;) # Add labels to the legend ) + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (&quot;, mu,&quot;mol CO&quot;[2], &quot; m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() Now, we want to “control” for the influence of shortwave radiation on GPP and define outliers with respect to the distribution of residuals of the linear regression between the two variables. Relax the definition of what is considered an outlier by setting adjusting their definition to falling outside \\(( Q_1 - 5 (Q_3 - Q_1)\\) to \\(Q_3 + 5 (Q_3 - Q_1)\\). Again, plot GPP_NT_VUT_REF versus shortwave radiation and highlight outliers in red. Hint: Fit the linear regression model as lm(GPP_NT_VUT_REF ~ SW_IN_F, data = half_hourly_fluxes) and obtain the residuals from the object returned by the lm() function (see ‘Value’ in its help page). Hint: The output of boxplot.stats(x) is a list, containing an element out. out is a named vector of the oulier values with names referring to the row numbers of x. Use as.integer(names(boxplot.stats(x)$out)) to get row numbers. residuals &lt;- lm(GPP_NT_VUT_REF ~ SW_IN_F, data = half_hourly_fluxes)$residuals # # unclear why this doesn&#39;t work: # vec_outliers &lt;- boxplot.stats(residuals, coef = 5)$out # plot_data &lt;- half_hourly_fluxes |&gt; # mutate(outlier = GPP_NT_VUT_REF %in% vec_outliers) # this works: rowindex_outliers &lt;- as.integer(names(boxplot.stats(residuals, coef = 5)$out)) plot_data &lt;- half_hourly_fluxes |&gt; mutate(rowindex = dplyr::row_number()) |&gt; mutate(outlier = rowindex %in% rowindex_outliers) plot_data |&gt; ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = outlier)) + geom_point() + scale_color_manual(&quot;Outlier?&quot;, # Set title of legend values = c(&quot;black&quot;, &quot;red&quot;), # Highlight in red labels = c(&quot;No&quot;, &quot;Yes&quot;) # Add labels to the legend ) + labs(x = expression(paste(&quot;Shortwave radiation (W m&quot;^-2, &quot;)&quot;)), y = expression(paste(&quot;GPP (&quot;, mu,&quot;mol CO&quot;[2], &quot; m&quot;^-2, &quot;s&quot;^-1, &quot;)&quot;))) + theme_classic() What do we see in this plot? We see that the red points, the outliers, fall outside the main point cloud of green points. The distribution of these outliers seems without systematic pattern or deviation. Nonetheless, it is good practice to go a step further and look at these data points in detail to find out whether they should be removed or not in your analysis. In later Chapters you will learn more on what disproportionate role outliers can play and how they may affect your statistical model and analysis. Visualising diurnal and seasonal cycles of GPP As explored in the previous Chapter’s exercises, GPP varies over diurnal and seasonal cycles. Create a publication-ready figure that visualises the mean diurnal cycle of GPP for each day-of-year (mean across multiple years). Make sure that the figure is properly labelled, and legible for readers with a color vision deficiency. Hint: To get the diurnal and seasonal cycles, summarise the half-hourly data by the hour of the day and the day of the year simultaneously using multiple grouping variables within group_by() and calculate mean values for GPP for each group. Hint: Chose an appropriate visualisation that maps the hour-of-day to the x-axis and the day-of-year to the y-axis. # Aggregate to hours-in-day for each day-in-year fluxes_per_hod_doy &lt;- half_hourly_fluxes |&gt; # df from previous exercise dplyr::mutate( hour_day = lubridate::hour(TIMESTAMP_START), # hour of the day day_year = lubridate::yday(TIMESTAMP_START)) |&gt; # day of the year dplyr::group_by(hour_day, day_year) |&gt; # multiple grouping dplyr::summarise(gpp = mean(GPP_NT_VUT_REF)) # Publication-ready raster plot fluxes_per_hod_doy |&gt; # Specify aesthetics ggplot(aes(x = hour_day, y = day_year, fill = gpp)) + # fill color of the raster geom_raster() + # Use a color scale that works also for color-blind people scale_fill_viridis_c(option = &quot;magma&quot;) + # adjust the aspect ratio of the plotting region coord_fixed(ratio = 0.18) + # labels of each mapping axis, \\n is a line break labs(title = &quot;Gross primary production&quot;, subtitle = &quot;Diurnal and seasonal cycle&quot;, x = &quot;Hour of day&quot;, y = &quot;Day of year&quot;, fill = expression(paste(mu,&quot;mol CO&quot;[2], &quot; m&quot;^-2, &quot;s&quot;^-1))) + # avoid having a small padding from the lowest values to the end of axes scale_x_continuous(expand = c(0,0)) + scale_y_continuous(expand = c(0,0)) Beautiful, isn’t it? We nicely see that on practically all days of the year we have the diurnal cycle of GPP which follows the sun’s cycle. And throughout a year, we see a rapid increase in GPP in the spring when all trees put out their leaves at once. After a highly productive summer, temperatures drop, sensescence kicks in and GPP gradually drops into its winter low. Trend in carbon dioxide concentrations This exercise explores the longest available atmospheric CO\\(_2\\) record, obtained at the Mauna Loa observatory in Hawaii. Atmospheric CO\\(_2\\) in the northern hemisphere is characterised by seasonal swings, caused by the seasonal course of CO\\(_2\\) uptake and release by the terrestrial biosphere. We’ve explored the seasonality of the CO\\(_2\\) uptake measured at one site (in Switzerland) extensively in this an previous chapters. Your task here is to calculate and visualise the long-term trend of CO\\(_2\\). Follow these steps: Download and read the monthly mean CO2\\(_2\\) data as a CSV file from here and read it into R. # download the file download.file( &quot;https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.csv&quot;, &quot;data/co2_mm_mlo.csv&quot; ) # read in the data ml_co2 &lt;- readr::read_csv( &quot;data/co2_mm_mlo.csv&quot;, skip = 40 ) # wrangle the data # interpret missing values # rename to avoid white space in column name ml_co2 &lt;- ml_co2 |&gt; dplyr::mutate(dplyr::across(dplyr::where(is.numeric), ~dplyr::na_if(., -9.99))) |&gt; dplyr::mutate(dplyr::across(dplyr::where(is.numeric), ~dplyr::na_if(., -0.99))) |&gt; dplyr::mutate(dplyr::across(dplyr::where(is.numeric), ~dplyr::na_if(., -1))) |&gt; dplyr::rename(year_dec = `decimal date`) Make a simple graph for visualizing the monthly CO\\(_2\\) time series. ml_co2 |&gt; ggplot2::ggplot() + ggplot2::geom_line(aes(year_dec, average)) Write a function that computes a 12-month running mean of the CO\\(_2\\) time series. The running mean for month \\(m\\) should consider values of \\(m-5\\) to \\(m+6\\). Define arguments for the function that let the user specify the width of the running mean “box” (i.e., setting the \\(5\\) and \\(6\\) to any other integer of choice) # Write the running mean function # variables are defined as: # Input vector (vec) # Number of elements to the left (left) # Number of elements to the right (right) running_mean &lt;- function( vec, left, right ) { # Create an empty vector of the same length as the input data vec_out &lt;- rep(NA, length(vec)) # Loop over each position in the vector for (idx in (left+1):(length(vec)-right)){ # Define start and end of the box to average over startbox &lt;- idx - left endbox &lt;- idx + right vec_out[idx] &lt;- mean(vec[startbox:endbox], na.rm = TRUE) } return(vec_out) } ml_co2 &lt;- ml_co2 |&gt; mutate( average_12m = running_mean(average, left = 5, right = 6) ) Make a publication-ready figure that shows the monthly and the 12-month running mean time series of the CO\\(_2\\) record. Hint: To automatically render the time axis with ggplot, you can create a time object by combining the year and month columns: lubridate::ymd(paste(as.character(year), \"-\", as.character(month), \"-15\")) # create a date object for nice plotting plot_data &lt;- ml_co2 |&gt; mutate( date = lubridate::ymd( paste(as.character(year), &quot;-&quot;, as.character(month), &quot;-15&quot;) # centering monthly mean on the 15th of each month ) ) plot_data |&gt; ggplot() + # monthly means geom_line( aes( date, average, color = &quot;Monthly mean&quot; ) ) + # running mean geom_line( aes( date, average_12m, color = &quot;12-month running mean&quot; ) ) + # Style the plot theme_classic() + theme( legend.position = c(0.25, 0.75) # Move legend into the plot ) + scale_color_manual( &quot;&quot;, # Omit legend title values = c(&quot;tomato&quot;, &quot;black&quot;), labels = c(&quot;12-month running mean&quot;, &quot;Monthly mean&quot;) ) + labs( title = expression( paste(&quot;Atmospheric CO&quot;[2], &quot; concentrations on Manua Lao, Hawaii&quot;) ), y = expression(paste(&quot;CO&quot;[2], &quot; (ppm)&quot;)), x = &quot;Year&quot; ) ## Warning: Removed 11 rows containing missing values (`geom_line()`). A.5 Data Variety Files and file formats Reading and writing human readable files While not leaving your R session, download and open the files at the following locations: The below code shows how to read in the different demo data sets (CSV files). You will note that they all need separate settings, and that a given file extension isn’t necessarily a reflection of the content the file. Inspection of your read in data is therefore key. # read in the first demo demo_01 &lt;- read.table( &quot;https://raw.githubusercontent.com/geco-bern/agds/main/data/demo_1.csv&quot;, sep = &quot;,&quot;, header = TRUE ) # read in second demo demo_02 &lt;- read.table( &quot;https://raw.githubusercontent.com/geco-bern/agds/main/data/demo_2.csv&quot;, sep = &quot; &quot;, header = TRUE ) demo_03 &lt;- read.table( &quot;https://raw.githubusercontent.com/geco-bern/agds/main/data/demo_3.csv&quot;, sep = &quot;;&quot;, comment.char = &quot;|&quot;, header = TRUE, ) All the demo data sets are equal, except for their formatting. We can test if the content is identical by using the identical() function in R. # compare 1 with 2 identical(demo_01, demo_02) ## [1] TRUE # compare 2 with 3 identical(demo_02, demo_03) ## [1] TRUE # Given transitive properties, demo_01 is identical to demo_03 Once loaded into your R environment, combine and save all data as a temporary CSV file. Read in the new temporary CSV file, and save it as a JSON file in your current working directory. You can combine the three datasets using the {dplyr} bind_rows() function. # combining all demo datasets demo_all &lt;- dplyr::bind_rows(demo_01, demo_02, demo_03) # writing the data to a temporary CSV file write.table( demo_all, file = file.path(tempdir(), &quot;tmp_csv_file.csv&quot;), col.names = TRUE, row.names = FALSE, sep = &quot;,&quot; ) # or... write.csv( demo_all, file.path(tempdir(), &quot;tmp_csv_file.csv&quot;), row.names = FALSE ) # read in the previous CSV file demo_all_new &lt;-read.table( file.path(tempdir(), &quot;tmp_csv_file.csv&quot;), header = TRUE, sep = &quot;,&quot; ) # writing the data to a JSON file jsonlite::write_json(demo_all_new, path = &quot;./my_json_file.json&quot;) Reading and writing binary files Download and open the following file: https://raw.githubusercontent.com/geco-bern/agds/main/data/demo_data.nc What file format are we dealing with? It is a NetCDF file (ending .nc, see Table in Chapter 5) What library would you use to read this kind of data? Different libraries are available, including {terra}, {raster} (the predecessor of {terra}), and {ncdf4}, see Table in Chapter 5. What does this file contain? In R: # read unknown netcdf file using the {terra} library library(terra) unknown_netcdf &lt;- terra::rast( &quot;https://raw.githubusercontent.com/geco-bern/agds/main/data/demo_data.nc&quot; ) # print the meta-data by calling the variable unknown_netcdf # visually plot the data terra::plot(unknown_netcdf) From your terminal (when the file is located in the current working directory: ncdump -h demo_data.nc When printing the object in R, we get: varname : t2m (2 metre temperature). Write this file to disk in a different geospatial format you desire (use the R documentation of the library used to read the file and the chapter information). # write the data as a geotiff (other options are possible as well in writeRaster) terra::writeRaster( unknown_netcdf, filename = &quot;./test.tif&quot;, overwrite = TRUE ) Download and open the following file: https://raw.githubusercontent.com/geco-bern/agds/main/data/demo_data.tif. Does this data seem familiar, and how can you tell? What are your conclusions? # read unknown tif file using the {terra} library library(terra) unknown_tif &lt;- terra::rast( &quot;https://raw.githubusercontent.com/geco-bern/agds/main/data/demo_data.tif&quot; ) # print the meta-data by calling the variable unknown_tif # visually plot the data terra::plot(unknown_tif) # Are they exactly the same terra::plot(unknown_tif - unknown_netcdf) # or... identical(unknown_netcdf, unknown_tif) Looks similar to the NetCDF data, but temperature appears to be given in Celsius in the GeoTIFF file and in Kelvin in the NetCDF file. A.5.1 API Use A.5.1.1 Get We can get the total sand content using the tutorial using new coodinates outlining Switzerland. # set API URL endpoint # for the total sand content url &lt;- &quot;https://thredds.daac.ornl.gov/thredds/ncss/ornldaac/1247/T_SAND.nc4&quot; # formulate query to pass to httr query &lt;- list( &quot;var&quot; = &quot;T_SAND&quot;, &quot;south&quot; = 45.5, &quot;west&quot; = 5.9, &quot;east&quot; = 10.7, &quot;north&quot; = 48, &quot;disableProjSubset&quot; = &quot;on&quot;, &quot;horizStride&quot; = 1, &quot;accept&quot; = &quot;netcdf4&quot; ) # download data using the # API endpoint and query data status &lt;- httr::GET( url = url, query = query, httr::write_disk( path = file.path(tempdir(), &quot;T_SAND.nc&quot;), overwrite = TRUE ) ) # to visualize the data # we need to load the {terra} # library sand &lt;- terra::rast(file.path(tempdir(), &quot;T_SAND.nc&quot;)) terra::plot(sand) Consulting the original data pages or the package help files one can determine that the parameter “T_SAND” needs to be replaced by “T_SILT” in both the URL and the query. # set API URL endpoint # for the total sand content url &lt;- &quot;https://thredds.daac.ornl.gov/thredds/ncss/ornldaac/1247/T_SILT.nc4&quot; # formulate query to pass to httr query &lt;- list( &quot;var&quot; = &quot;T_SILT&quot;, &quot;south&quot; = 45.5, &quot;west&quot; = 5.9, &quot;east&quot; = 10.7, &quot;north&quot; = 48, &quot;disableProjSubset&quot; = &quot;on&quot;, &quot;horizStride&quot; = 1, &quot;accept&quot; = &quot;netcdf4&quot; ) # download data using the # API endpoint and query data status &lt;- httr::GET( url = url, query = query, httr::write_disk( path = file.path(tempdir(), &quot;T_SAND.nc&quot;), overwrite = TRUE ) ) # to visualize the data # we need to load the {terra} # library sand &lt;- terra::rast(file.path(tempdir(), &quot;T_SAND.nc&quot;)) terra::plot(sand) A.5.1.2 Dedicated libraries Using the {hwsdr} package this simplifies to: # Download a soil fraction map # of sand for a given bounding box hwsdr::ws_subset( location = c(45.5, 5.9, 48, 10.7), param = &quot;T_SAND&quot;, path = tempdir() ) ## class : SpatRaster ## dimensions : 97, 51, 1 (nrow, ncol, nlyr) ## resolution : 0.05, 0.05 (x, y) ## extent : 45.5, 48.05, 5.9, 10.75 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 ## source : T_SAND.nc ## name : T_SAND ## unit : % # to visualize the data # we need to load the {terra} # library sand &lt;- terra::rast(file.path(tempdir(), &quot;T_SAND.nc&quot;)) terra::plot(sand) You can easily list all {MODISTools} products using: # list all products products &lt;- MODISTools::mt_products() # We count nrow(products) ## [1] 47 You can use the {MODISTools} package to easily query the land cover data. Use the MODISTools::mt_products() and MODISTools::mt_bands() functions to determine products to use (i.e. MCD12Q1). Note that MODISTools does not use a bounding box but km left/right and top/bottom - an approximation is therefore good enough. # download land cover data (for a single year to speed things up) land_cover &lt;- MODISTools::mt_subset( product = &quot;MCD12Q1&quot;, site_name = &quot;Swiss&quot;, lat = 46.6756, lon = 7.85480, band = &quot;LC_Type1&quot;, start = &quot;2012-01-01&quot;, end = &quot;2012-12-31&quot;, km_lr = 50, km_ab = 50, internal = TRUE, progress = TRUE ) ## Downloading chunks: ## | | | 0% | |======================================================================| 100% # convert to a raster map for plotting land_cover &lt;- MODISTools::mt_to_terra(land_cover) terra::plot(land_cover) A.6 Open Science External data The project data is stored in one folder without folders to sort data from code to give it structure. The project can be re-organized using a simple project structure as such: ~/project/ ├─ data/ ├─ 00_convert_data.R ├─ survey.xlsx # the original ├─ survey.csv # from (xls conversion (copy 1).csv) ├─ R/ ├─ my_functions.R ├─ analysis/ ├─ 00_model_fits.R # from Model-test-final.R ├─ 01_model_plots.R # from Plots.R ├─ vignettes/ ├─ Report.Rmd ├─ manuscript/ ├─ Report.html ├─ Figure 1.png Note that duplicate files are removed, code to cleanup data is numbered and stored with the data, functions which are accessible for analysis are stored in the ./R/ folder, Rmarkdown files are stored in the vignettes folder and the results of the full analysis is stored in a manuscript folder. Some variations on naming is possible. A new project This exercise trains your ability to access and wrangle data yourself in a reproducible way. The best solution to test whether you successfully did so is by letting a friend run all of your code on their machine. Resolving the errors you may encounter helps you to improve your workflow and ensures a streamlined submission of your final report. Tracking the state of your project For your new project created above run {renv} by following the tutorial outline. In short, in the main project run all project code (or load all require libraries) and execute: # Initiate a local index of used libraries renv::init() # Take a snapshot of all used libraries renv::snapshot() You should now find an renv folder in your project as well as an renv.lock file. A.7 Code Management Location based code management No solutions provided. A.8 Regression and Classification A.9 Supervised ML I No solutions provided A.10 Supervised ML II Cross-validation by hand In the tutorial we “built on shoulder of giants” - people that went through the struggle of writing a robust package to implement a cross validation. Although we can and should use such packages, we still have to understand how a cross validation works in detail. Write a function that implements n-fold cross-validation for KNN with \\(k=30\\). (We write ‘n-fold CV’ here to avoid confusion with the k in KNN, but mean the same as described in Section 10.2.4.) The function should take as arguments the training data object, n for specifying the number of folds (use \\(n=60\\)), the name of the target variable, the names of the predictor variables as a character vector, and the \\(k\\) for KNN. The function should return a vector of length n, containing the MAE evaluated on the n folds. To randomize the split of data points into folds, first “re-shuffle” the rows in the training data as part of the function. Centering and scaling of the training and validation data should be applied manually before getting the KNN model object within each fold. As data, use daily ecosystem flux data from here. As target variable, use \"GPP_NT_VUT_REF\". As predictor variables, use c(\"TA_F\", \"SW_IN_F\", \"VPD_F\"). Visualise the distribution of the validation errors (MAE) across folds. # read data provided for this exercise daily_fluxes &lt;- read.csv(&quot;data/df_daily_exercise_supervisedmlii.csv&quot;) nam_target &lt;- &quot;GPP_NT_VUT_REF&quot; nams_predictors &lt;- c(&quot;TA_F&quot;, &quot;SW_IN_F&quot;, &quot;VPD_F&quot;) # function returning the MAE for each fold, given the indices of the # rows to be used as the validation set. get_mae_byfold &lt;- function(df, idx_fold, # row indices for validation set nam_target, # character string for target variables nams_predictors, # vector of character strings for predictors use_k # k for KNN ){ # validation set df_valid &lt;- df[idx_fold, c(nam_target, nams_predictors)] # remaining training set df_train &lt;- df[-idx_fold, c(nam_target, nams_predictors)] # center and scale based on training data parameters mean_byvar &lt;- c() sd_byvar &lt;- c() df_train_cs &lt;- df_train * NA df_valid_cs &lt;- df_valid * NA # center and scale each predictor for (ivar in nams_predictors){ # determine mean and sd for centering and scaling mean_byvar[ivar] &lt;- mean(df_train[,ivar], na.rm = TRUE) sd_byvar[ivar] &lt;- sd(df_train[,ivar], na.rm = TRUE) # center and scale training data df_train_cs[,ivar] &lt;- df_train[,ivar] - mean_byvar[ivar] df_train_cs[,ivar] &lt;- df_train_cs[,ivar] / sd_byvar[ivar] # center and scale validation data # important: use parameters (mean and sd) determined on training data df_valid_cs[,ivar] &lt;- df_valid[,ivar] - mean_byvar[ivar] df_valid_cs[,ivar] &lt;- df_valid_cs[,ivar] / sd_byvar[ivar] } # add unmodified target variable df_valid_cs[,nam_target] &lt;- df_valid[,nam_target] df_train_cs[,nam_target] &lt;- df_train[,nam_target] # train using the scaled training data mod &lt;- caret::knnreg(df_train_cs[,nams_predictors], df_train_cs[,nam_target], k = use_k ) # predict using the scaled validation data df_valid_cs$pred &lt;- predict(mod, newdata = df_valid_cs[,nams_predictors]) # calculate MAE on validation data out &lt;- mean(abs(df_valid_cs$pred - df_valid_cs[,nam_target])) return(out) } # function reshuffling data, creating folds (list of row indices), and # calling function to calculate MAE on each fold. Returns vector of MAE for each fold. get_mae_cv &lt;- function(df, nam_target, # character string for target variables nams_predictors, # vector of character strings for predictors n_folds, # number of folds for cross-validation use_k # k for KNN ){ # re-shuffle rows in data frame df &lt;- df[sample(nrow(df)),] # determine row indices to be allocated to each fold # each fold takes in 1/n_folds of the total number of rows nrows_per_fold &lt;- ceiling(nrow(df) / n_folds) idx &lt;- rep(seq(1:n_folds), each = nrows_per_fold) folds_1 &lt;- split(1:nrow(df), idx[1:nrow(df)]) # alternative option n_folds &lt;- 5 idx &lt;- (1:nrow(df) - 1) %/% (nrow(df) / n_folds) folds_2 &lt;- split(1:nrow(df), idx) # using caret built-in function folds_3 &lt;- caret::createFolds(1:nrow(df), k = n_folds) # loop over folds and get MAE determined on each validation set mae_list &lt;- purrr::map( folds_1, ~get_mae_byfold(df, ., nam_target, nams_predictors, use_k) ) # return a vector of MAE in each fold return(unlist(mae_list)) } # get MAE for each fold of the cross-validation vec_mae_byhand &lt;- get_mae_cv(daily_fluxes, nam_target, nams_predictors, n_folds = 60, use_k = 30 ) tibble(mae = vec_mae_byhand) |&gt; ggplot(aes(mae, ..count..)) + geom_histogram(fill = &quot;grey70&quot;, color = &quot;black&quot;, bins = 10) + theme_classic() Cross-validation vs. test error Now, you can use the user-friendly caret::train() for KNN with 60-fold cross-validation and tuning the hyperparameter k. Use the MAE as the loss metric. Use the same data as in the Exercise above and withhold 20% of the data for the test set. Visually compare the reported mean MAE from the cross-validation folds with the MAE determined on a test set. In your visual comparison, add a plot layer showing the distribution of validation errors from you manual implementation of cross-validation (Exercise above). daily_fluxes &lt;- read.csv(&quot;data/df_daily_exercise_supervisedmlii.csv&quot;) set.seed(1982) # for reproducibility split &lt;- rsample::initial_split(daily_fluxes, prop = 0.8) daily_fluxes_train &lt;- rsample::training(split) daily_fluxes_test &lt;- rsample::testing(split) # The same model formulation is in the previous chapter pp &lt;- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = daily_fluxes_train) |&gt; recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |&gt; recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes()) mod &lt;- caret::train(pp, data = daily_fluxes_train |&gt; drop_na(), method = &quot;knn&quot;, trControl = caret::trainControl(method = &quot;cv&quot;, number = 60 ), tuneGrid = data.frame(k = c(2, 5, 10, 15, 20, 25, 30, 35, 40, 60, 100)), metric = &quot;MAE&quot; ) # data frame containing metrics on validation set by fold metrics_byfold &lt;- mod$resample # MAE on test set daily_fluxes_test &lt;- daily_fluxes_test |&gt; drop_na() %&gt;% # use magrittr-pipe here for the dot evaluation mutate(fitted = predict(mod, newdata = .)) mae_test &lt;- mean(abs(daily_fluxes_test$fitted - daily_fluxes_test$GPP_NT_VUT_REF)) plot_data &lt;- metrics_byfold |&gt; select(caret = MAE) |&gt; bind_cols( tibble(byhand = vec_mae_byhand) ) |&gt; pivot_longer(cols = c(caret, byhand), names_to = &quot;implementation&quot;, values_to = &quot;MAE&quot;) plot_data |&gt; ggplot(aes(x = MAE, y = ..count.., fill = implementation)) + geom_density(alpha = 0.5) + # test error geom_vline(xintercept = mae_test, color = &quot;red&quot;, size = 2) + scale_fill_manual(values = c(&quot;darkgoldenrod&quot;, &quot;royalblue&quot;)) + theme_classic() A.11 Random Forest Fitting a Random Forest Fit a Random Forest model to the flux data used in the examples of this chapter. Implement bagging 12 decision trees (num.trees), each with a minimum number of observations per leaf of 5 (min.node.size). You can consult the respective arguments for the \"ranger\" method typing ?ranger. # Data loading and cleaning daily_fluxes &lt;- read_csv(&quot;./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv&quot;) |&gt; # select only the variables we are interested in dplyr::select(TIMESTAMP, GPP_NT_VUT_REF, # the target ends_with(&quot;_QC&quot;), # quality control info ends_with(&quot;_F&quot;), # includes all all meteorological covariates -contains(&quot;JSB&quot;) # weird useless variable ) |&gt; # convert to a nice date object dplyr::mutate(TIMESTAMP = lubridate::ymd(TIMESTAMP)) |&gt; # set all -9999 to NA dplyr::mutate(dplyr::across(dplyr::where(is.numeric), ~dplyr::na_if(., -9999))) |&gt; # dplyr::na_if(-9999) |&gt; xxxxx # retain only data based on &gt;=80% good-quality measurements # overwrite bad data with NA (not dropping rows) dplyr::mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC &lt; 0.8, NA, GPP_NT_VUT_REF), TA_F = ifelse(TA_F_QC &lt; 0.8, NA, TA_F), SW_IN_F = ifelse(SW_IN_F_QC &lt; 0.8, NA, SW_IN_F), LW_IN_F = ifelse(LW_IN_F_QC &lt; 0.8, NA, LW_IN_F), VPD_F = ifelse(VPD_F_QC &lt; 0.8, NA, VPD_F), PA_F = ifelse(PA_F_QC &lt; 0.8, NA, PA_F), P_F = ifelse(P_F_QC &lt; 0.8, NA, P_F), WS_F = ifelse(WS_F_QC &lt; 0.8, NA, WS_F)) |&gt; # drop QC variables (no longer needed) dplyr::select(-ends_with(&quot;_QC&quot;)) # Data splitting set.seed(123) # for reproducibility split &lt;- rsample::initial_split(daily_fluxes, prop = 0.7, strata = &quot;VPD_F&quot;) daily_fluxes_train &lt;- rsample::training(split) daily_fluxes_test &lt;- rsample::testing(split) # The same model formulation is in the previous chapter pp &lt;- recipes::recipe(GPP_NT_VUT_REF ~ TA_F + SW_IN_F + LW_IN_F + VPD_F + P_F + WS_F, data = daily_fluxes_train) |&gt; recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |&gt; recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes()) # Fit the first model of 12 trees and maximum depth 4 rf_12_5 &lt;- caret::train( pp, data = daily_fluxes_train %&gt;% drop_na(), method = &quot;ranger&quot;, metric = &quot;RMSE&quot;, trControl = trainControl( method = &quot;cv&quot;, number = 5, savePredictions = &quot;final&quot; ), tuneGrid = expand.grid( .mtry = floor(6 / 3), # default p/3 .min.node.size = 5, # set to 5 .splitrule = &quot;variance&quot; # default &quot;variance&quot; ), # arguments specific to &quot;ranger&quot; method replace = FALSE, sample.fraction = 0.5, num.trees = 12, seed = 1982 # for reproducibility ) # generic print print(rf_12_5) ## Random Forest ## ## 1910 samples ## 8 predictor ## ## Recipe steps: center, scale ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 1528, 1528, 1529, 1527, 1528 ## Resampling results: ## ## RMSE Rsquared MAE ## 1.468118 0.6796483 1.116388 ## ## Tuning parameter &#39;mtry&#39; was held constant at a value of 2 ## Tuning ## parameter &#39;splitrule&#39; was held constant at a value of variance ## ## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 5 Repeat the fitting with 1000 decision trees and minimum node size of 5, then with 12 decision trees and a minimum node size of 1. Then, discuss the role that the number of decision trees and the minimum number of leaf observations of a tree play in the bias-variance trade-off and in the computation time. # Directly fit the model again, with same data and model formulation # Now train with 1000 trees and maximum depth 4 rf_1000_5 &lt;- caret::train( pp, data = daily_fluxes_train %&gt;% drop_na(), method = &quot;ranger&quot;, metric = &quot;RMSE&quot;, trControl = trainControl( method = &quot;cv&quot;, number = 5, savePredictions = &quot;final&quot; ), tuneGrid = expand.grid( .mtry = floor(6 / 3), # default p/3 .min.node.size = 5, # set to 5 .splitrule = &quot;variance&quot; # default variance ), # arguments specific to &quot;ranger&quot; method replace = FALSE, sample.fraction = 0.5, num.trees = 1000, seed = 1982 ) # generic print print(rf_1000_5) ## Random Forest ## ## 1910 samples ## 8 predictor ## ## Recipe steps: center, scale ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 1527, 1528, 1529, 1529, 1527 ## Resampling results: ## ## RMSE Rsquared MAE ## 1.415667 0.701975 1.078454 ## ## Tuning parameter &#39;mtry&#39; was held constant at a value of 2 ## Tuning ## parameter &#39;splitrule&#39; was held constant at a value of variance ## ## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 5 # Repeat model fit with 12 trees and maximum depth 6 rf_12_1 &lt;- caret::train( pp, data = daily_fluxes_train %&gt;% drop_na(), method = &quot;ranger&quot;, metric = &quot;RMSE&quot;, trControl = trainControl( method = &quot;cv&quot;, number = 5, savePredictions = &quot;final&quot; ), tuneGrid = expand.grid( .mtry = floor(6 / 3), # default p/3 .min.node.size = 1, # set to 1 .splitrule = &quot;variance&quot; # default &quot;variance&quot; ), # arguments specific to &quot;ranger&quot; method replace = FALSE, sample.fraction = 0.5, num.trees = 12, seed = 1982 ) # generic print print(rf_12_1) ## Random Forest ## ## 1910 samples ## 8 predictor ## ## Recipe steps: center, scale ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 1527, 1528, 1529, 1528, 1528 ## Resampling results: ## ## RMSE Rsquared MAE ## 1.470344 0.678954 1.11868 ## ## Tuning parameter &#39;mtry&#39; was held constant at a value of 2 ## Tuning ## parameter &#39;splitrule&#39; was held constant at a value of variance ## ## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 1 Interpretation of results # check results rbind( rf_12_5$results |&gt; mutate(Setup = &quot;12 Trees with at least 5 obs per leaf&quot;) |&gt; relocate(Setup), rf_1000_5$results |&gt; mutate(Setup = &quot;1000 Trees with at least 5 obs per leaf&quot;) |&gt; relocate(Setup), rf_12_1$results |&gt; mutate(Setup = &quot;12 Trees with at least 1 obs per leaf&quot;) |&gt; relocate(Setup) )[,- c(2:4)] |&gt; knitr::kable(caption = &quot;Comparison of cross-validated metrics across Random Forest setups.&quot;) Table A.1: Comparison of cross-validated metrics across Random Forest setups. Setup RMSE Rsquared MAE RMSESD RsquaredSD MAESD 12 Trees with at least 5 obs per leaf 1.468118 0.6796483 1.116388 0.0631198 0.0243411 0.0436952 1000 Trees with at least 5 obs per leaf 1.415667 0.7019750 1.078454 0.0661862 0.0228750 0.0308386 12 Trees with at least 1 obs per leaf 1.470344 0.6789540 1.118680 0.0766487 0.0244763 0.0588226 Increasing the number of decision trees leads to a decrease in error (e.g. RMSE) and in error variance in the random forest results, making the model more accurate, robust and generalisable. This can be seen by the smaller values of practically all the metrics above. In the bias-variance trade-off, higher num.trees shifts the balance towards both lower bias and lower variance. Decreasing the number of observations required to go to each split allows for a better fit of the data, but makes the model less generalisable. Actually, it would lead to a perfect fit of the training data. Since the error metrics are calcualted using cross-validation, the lack of generalisability is captured together with the model fit. This is seen by an increase in both the error metrics and their estimated standard deviation. Hence, using low values for min.node.size may lead to overfitting. Hyperparameter tuning In a previous tutorial, you learned how to tune the hyperparameter \\(k\\) in a KNN by hand. Now you will do the hyperparameter tuning for a Random Forest model. The task gets more complicated because there are more hyperparameters in a random forest. The {caret} package allows to vary three hyperparameters: mtry: The number of variables to consider to make decisions at each node, often taken as \\(p/3\\) for regression, where \\(p\\) is the number of predictors. min.node.size: The number of data points at the “bottom” of each decision tree, i.e. the leaves. splitrule: The function applied to data in each branch of a tree, used for determining the goodness of a decision. Answer the following questions, giving a reason for your responses: Check the help for the ranger() function and identify which values each of the three hyperparameters/arguments can take. Select a sensible range of values for each hyperparameter, that you will use in the hyperparameter search. mtry_values &lt;- c(2, 4, 6) min.node.size_values &lt;- c(2, 5, 10, 20) splitrule_values &lt;- c(&quot;variance&quot;, &quot;extratrees&quot;, &quot;maxstat&quot;) In the previous exercise, you have seen how the minimum node size regulates fit quality and overfitting. How does the minimum node size relate to tree depth? What happens at the edge cases, when min.node.size = 1 and when min.node.size = n (n being the number of observations)? Note that it’s not necessary to provide the max.depth argument to train() because min.node.size is already limiting the size of the trees in the Random Forests. Solution: The minimum node size is inversely related to the tree depth. The more nodes are required to be at the leaves, the shorter the tree will be, because not so many splits can be done. If we allowed each leave to correspond to a single observation, we would have a very large tree, such that each branch corresponds to one observation. If we force each leave to have at least \\(n\\) observations, then the tree will not “grow”, that is, it will never even split. Greedy hyperparameter tuning: Sequentially optimize the choice of each hyperparameter, one at a time and keeping the other two constant. Take the code from the tutorial as a starting point, and those hyperparameter values as an initial point for the search. Implement the optimization routine yourself, using loops. &gt; Tip: Keep the number of trees low, otherwise it takes too long to fit each Random Forest model. # Use data and model formulation created before results &lt;- c() # initialise results set.seed(1997) # for reproducibility # Train models in a loop, save metrics for (mtry_value in mtry_values){ mod &lt;- caret::train( pp, data = daily_fluxes_train %&gt;% drop_na(), method = &quot;ranger&quot;, metric = &quot;RMSE&quot;, trControl = trainControl( method = &quot;cv&quot;, number = 5, savePredictions = &quot;final&quot; ), tuneGrid = expand.grid( .mtry = mtry_value, # modify mtry .min.node.size = 5, # default 5 .splitrule = &quot;variance&quot; # default &quot;variance&quot; ), # arguments specific to &quot;ranger&quot; method replace = FALSE, sample.fraction = 0.5, num.trees = 100, seed = 1982 ) results &lt;- rbind(results, mod$results) } results ## mtry min.node.size splitrule RMSE Rsquared MAE RMSESD ## 1 2 5 variance 1.408401 0.7048393 1.079003 0.06273482 ## 2 4 5 variance 1.415472 0.7036540 1.078367 0.02791004 ## 3 6 5 variance 1.433248 0.6946220 1.094301 0.11539229 ## RsquaredSD MAESD ## 1 0.01596849 0.04919243 ## 2 0.01083131 0.03353947 ## 3 0.03200823 0.07272992 Based on the first round of hyperparameter tuning, we should take mtry = 2 because it leads to the smallest RMSE and MAE, and the highest Rsquared. That is, the best fit. Nevertheless, the difference between taking mtry = 2 or 4 is very small and the second actually leads to smaller variance in the metric estimates. Hence, the decision is not so clear. # Take the previous best model and tune next hyperparameter results &lt;- c() # initialise results set.seed(1997) # for reproducibility # Train models in a loop, save metrics for (min.node.size_value in min.node.size_values){ mod &lt;- caret::train( pp, data = daily_fluxes_train %&gt;% drop_na(), method = &quot;ranger&quot;, metric = &quot;RMSE&quot;, trControl = trainControl( method = &quot;cv&quot;, number = 5, savePredictions = &quot;final&quot; ), tuneGrid = expand.grid( .mtry = 2, # best mtry .min.node.size = min.node.size_value, # modify .splitrule = &quot;variance&quot; # default &quot;variance&quot; ), # arguments specific to &quot;ranger&quot; method # keep num.trees keep small for computation # for reproducibility set the seed value sample.fraction = 0.5, num.trees = 100, seed = 1982 ) results &lt;- rbind(results, mod$results) } results ## mtry min.node.size splitrule RMSE Rsquared MAE RMSESD ## 1 2 2 variance 1.414459 0.7021521 1.080694 0.06568570 ## 2 2 5 variance 1.416445 0.7033030 1.078525 0.03266685 ## 3 2 10 variance 1.429133 0.6971880 1.090143 0.12087515 ## 4 2 20 variance 1.426698 0.6983137 1.085333 0.05662594 ## RsquaredSD MAESD ## 1 0.016183122 0.05218533 ## 2 0.005366239 0.03448577 ## 3 0.034472910 0.07423081 ## 4 0.015027167 0.03858145 The second hyperparameter should be either min.node.size = 2 or 5 because they lead to the best metrics overall. Again, the differences are very small and each metric would lead to a different decision. By increasing min.node.size, we get more generalisability, but if it’s too high we will lose fit quality. If you change the random seed, you’ll see that the tuning results are not robust, so whichever hyperparameter we choose won’t make a big difference in the model fit (at least within the ranges searched). Let’s take min.node.size = 2 for the next loop. # Take the previous best models and tune last hyperparameter results &lt;- c() # Train models in a loop, save metrics for (splitrule_value in splitrule_values){ mod &lt;- caret::train( pp, data = daily_fluxes_train %&gt;% drop_na(), method = &quot;ranger&quot;, metric = &quot;RMSE&quot;, trControl = trainControl( method = &quot;cv&quot;, number = 5, savePredictions = &quot;final&quot; ), tuneGrid = expand.grid( .mtry = 2, # best mtry .min.node.size = 2, # best min.node.size .splitrule = splitrule_value # modify ), # arguments specific to &quot;ranger&quot; method # keep num.trees keep small for computation # for reproducibility set the seed value replace = FALSE, sample.fraction = 0.5, num.trees = 100, seed = 1982 ) results &lt;- rbind(results, mod$results) } results ## mtry min.node.size splitrule RMSE Rsquared MAE RMSESD ## 1 2 2 variance 1.430041 0.6960092 1.089296 0.07545037 ## 2 2 2 extratrees 1.406141 0.7084745 1.067863 0.08168255 ## 3 2 2 maxstat 1.450796 0.6908389 1.109260 0.06738257 ## RsquaredSD MAESD ## 1 0.02332409 0.05434451 ## 2 0.01930827 0.04337092 ## 3 0.01935027 0.04709398 According to the last round of tuning, we should use splitrule = \"extratrees\". With that, we found the best model so far. Grid hyperparameter tuning: Starting with the same range of values for each hyperparameter as before, look for the combination that leads to the best model performance among all combinations of hyperparameter values. This time, use the expand.grid() function to create a data.frame of hyperparameter value combinations. This grid will be passed to train() via the tuneGrid argument (see example in the tutorial). This will automatically do the hyperparameter search for you. Comment the output of train() and the results of the hyperparameter search. set.seed(1403) # for reproducibility mod &lt;- caret::train( pp, data = daily_fluxes_train %&gt;% drop_na(), method = &quot;ranger&quot;, metric = &quot;RMSE&quot;, trControl = trainControl( method = &quot;cv&quot;, number = 5, savePredictions = &quot;final&quot; ), # expand grid of tunable hyperparameters tuneGrid = expand.grid( .mtry = mtry_values, .min.node.size = min.node.size_values, .splitrule = splitrule_values ), # arguments specific to &quot;ranger&quot; method # keep num.trees keep small for computation # for reproducibility set the seed value replace = FALSE, sample.fraction = 0.5, num.trees = 100, seed = 1982 ) plot(mod, metric = &quot;RMSE&quot;) plot(mod, metric = &quot;Rsquared&quot;) Compare the results from the two hyperparameter tuning approaches. Do the optimal hyperparameters coincide? Are the corresponding RMSE estimates similar? What are the advantages and disadvantages of the greedy and the grid approaches? The best model according to the grid search, with lowest RMSE and highest \\(R^2\\), is the one with mtry = 6, min.node.size = 10 and splitrule = \"extratrees\" . This is not the “best model” we found with the greedy approach (with mtry = 2 and min.node.size = 2) but actually this model is overall the second best model. The metric used for tuning matters, and looking at several of them at the same can help make decisions, if different metrics agree. All these hyperparameter tuning approaches agree that the best splitrule is \"extratrees\" (and if you change the random seed, this result is consistent). The best mtry value is different for each split rule used, so having started with \"variance\" in the greedy search lead the tuning in the wrong direction, moving us towards a local optimum rather than a global optimum. This highlights that hyperparameter values interact with each other and optimizing over grids is preferred (although it takes more time). Model performance You have trained several random forest models. Evaluate the model performance on the best model (the one for the tuned hyperparameters) and on one of your worse models. If you compare the RMSE and \\(R^2\\) on the training and the test set, does it show overfitting? # Train best model mod_best &lt;- caret::train( pp, data = daily_fluxes_train %&gt;% drop_na(), method = &quot;ranger&quot;, metric = &quot;RMSE&quot;, trControl = trainControl( method = &quot;cv&quot;, number = 5, savePredictions = &quot;final&quot; ), # expand grid of tunable hyperparameters tuneGrid = expand.grid( .mtry = 6, .min.node.size = 10, .splitrule = &quot;extratrees&quot; ), # arguments specific to &quot;ranger&quot; method # keep num.trees keep small for computation # for reproducibility set the seed value replace = FALSE, sample.fraction = 0.5, num.trees = 100, seed = 1982 ) # Get predictions # Train worst model mod_worst &lt;- caret::train( pp, data = daily_fluxes_train %&gt;% drop_na(), method = &quot;ranger&quot;, metric = &quot;RMSE&quot;, trControl = trainControl( method = &quot;cv&quot;, number = 5, savePredictions = &quot;final&quot; ), # expand grid of tunable hyperparameters tuneGrid = expand.grid( .mtry = 2, .min.node.size = 20, .splitrule = &quot;maxstat&quot; ), # arguments specific to &quot;ranger&quot; method # keep num.trees keep small for computation # for reproducibility set the seed value replace = FALSE, sample.fraction = 0.5, num.trees = 100, seed = 1982 ) source(&quot;R/eval_model.R&quot;) eval_model( mod_best, daily_fluxes_train, daily_fluxes_test ) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; eval_model( mod_worst, daily_fluxes_train, daily_fluxes_test ) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; The performance on the test set for the best model is still close to the performance on the training set, so the model doesn’t seem to overfit. The same goes for the worse model, which leads to worse \\(R^2\\) and RMSE and visually the fit is slightly worse. "],["references.html", "B References System information and package list", " B References System information and package list This book was compiled with the following environment: sessioninfo::session_info() ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.3.2 (2023-10-31) ## os Ubuntu 22.04.3 LTS ## system x86_64, linux-gnu ## ui X11 ## language (EN) ## collate C.UTF-8 ## ctype C.UTF-8 ## tz UTC ## date 2023-11-07 ## pandoc 2.19.2 @ /usr/bin/ (via rmarkdown) ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## ! package * version date (UTC) lib source ## P backports 1.4.1 2021-12-13 [?] RSPM (R 4.3.0) ## base64enc 0.1-3 2015-07-28 [1] CRAN (R 4.3.2) ## P bit 4.0.5 2022-11-15 [?] RSPM (R 4.3.0) ## P bit64 4.0.5 2020-08-30 [?] RSPM (R 4.3.0) ## bookdown 0.36 2023-10-16 [1] CRAN (R 4.3.2) ## P broom * 1.0.5 2023-06-09 [?] RSPM (R 4.3.0) ## bslib 0.5.1 2023-08-11 [1] CRAN (R 4.3.2) ## cachem 1.0.8 2023-05-01 [1] CRAN (R 4.3.2) ## P caret * 6.0-94 2023-03-21 [?] RSPM (R 4.3.0) ## P class 7.3-22 2023-05-03 [?] CRAN (R 4.3.2) ## P classInt 0.4-10 2023-09-05 [?] RSPM (R 4.3.0) ## cli 3.6.1 2023-03-23 [1] CRAN (R 4.3.2) ## P codetools 0.2-19 2023-02-01 [?] CRAN (R 4.3.2) ## P colorspace 2.1-0 2023-01-23 [?] RSPM (R 4.3.0) ## P cowplot * 1.1.1 2020-12-30 [?] RSPM (R 4.3.0) ## crayon 1.5.2 2022-09-29 [1] CRAN (R 4.3.2) ## curl 5.1.0 2023-10-02 [1] CRAN (R 4.3.2) ## P data.table 1.14.8 2023-02-17 [?] RSPM (R 4.3.0) ## P DBI 1.1.3 2022-06-18 [?] RSPM (R 4.3.0) ## digest 0.6.33 2023-07-07 [1] CRAN (R 4.3.2) ## P dplyr * 1.1.3 2023-09-03 [?] RSPM (R 4.3.0) ## P e1071 * 1.7-13 2023-02-01 [?] RSPM (R 4.3.0) ## ellipsis 0.3.2 2021-04-29 [1] CRAN (R 4.3.2) ## P evaluate 0.21 2023-05-05 [?] RSPM (R 4.3.0) ## fansi 1.0.5 2023-10-08 [1] CRAN (R 4.3.2) ## P farver 2.1.1 2022-07-06 [?] RSPM (R 4.3.0) ## fastmap 1.1.1 2023-02-24 [1] CRAN (R 4.3.2) ## P forcats * 1.0.0 2023-01-29 [?] RSPM (R 4.3.0) ## P foreach 1.5.2 2022-02-02 [?] RSPM (R 4.3.0) ## fs 1.6.3 2023-07-20 [1] CRAN (R 4.3.2) ## P furrr 0.3.1 2022-08-15 [?] RSPM (R 4.3.0) ## P future 1.33.0 2023-07-01 [?] RSPM (R 4.3.0) ## P future.apply 1.11.0 2023-05-21 [?] RSPM (R 4.3.0) ## P generics 0.1.3 2022-07-05 [?] RSPM (R 4.3.0) ## P ggplot2 * 3.4.4 2023-10-12 [?] RSPM (R 4.3.0) ## P globals 0.16.2 2022-11-21 [?] RSPM (R 4.3.0) ## glue 1.6.2 2022-02-24 [1] CRAN (R 4.3.2) ## P gower 1.0.1 2022-12-22 [?] RSPM (R 4.3.0) ## P gtable 0.3.4 2023-08-21 [?] RSPM (R 4.3.0) ## P hardhat 1.3.0 2023-03-30 [?] RSPM (R 4.3.0) ## P hexbin * 1.28.3 2023-03-21 [?] RSPM (R 4.3.0) ## highr 0.10 2022-12-22 [1] CRAN (R 4.3.2) ## P hms 1.1.3 2023-03-21 [?] RSPM (R 4.3.0) ## P htmltools 0.5.6 2023-08-10 [?] RSPM (R 4.3.0) ## P httr 1.4.7 2023-08-15 [?] RSPM (R 4.3.0) ## P hwsdr * 1.1 2023-09-16 [?] RSPM (R 4.3.0) ## P ipred 0.9-14 2023-03-09 [?] RSPM (R 4.3.0) ## P iterators 1.0.14 2022-02-05 [?] RSPM (R 4.3.0) ## jquerylib 0.1.4 2021-04-26 [1] CRAN (R 4.3.2) ## jsonlite * 1.8.7 2023-06-29 [1] CRAN (R 4.3.2) ## P KernSmooth 2.23-21 2023-05-03 [?] RSPM (R 4.3.0) ## P knitr 1.43 2023-05-25 [?] RSPM (R 4.3.0) ## P labeling 0.4.3 2023-08-29 [?] RSPM (R 4.3.0) ## P lattice * 0.21-8 2023-04-05 [?] RSPM (R 4.3.0) ## P lava 1.7.2.1 2023-02-27 [?] RSPM (R 4.3.0) ## lifecycle 1.0.3 2022-10-07 [1] CRAN (R 4.3.2) ## P listenv 0.9.0 2022-12-16 [?] RSPM (R 4.3.0) ## P lubridate * 1.9.3 2023-09-27 [?] RSPM (R 4.3.0) ## magrittr * 2.0.3 2022-03-30 [1] CRAN (R 4.3.2) ## P MASS 7.3-60 2023-05-04 [?] CRAN (R 4.3.2) ## P Matrix 1.5-4.1 2023-05-18 [?] RSPM (R 4.3.0) ## memoise 2.0.1 2021-11-26 [1] CRAN (R 4.3.2) ## P mgcv 1.8-42 2023-03-02 [?] RSPM (R 4.3.0) ## P ModelMetrics 1.2.2.2 2020-03-17 [?] RSPM (R 4.3.0) ## P modelr * 0.1.11 2023-03-22 [?] RSPM (R 4.3.0) ## P MODISTools * 1.1.5 2023-09-17 [?] RSPM (R 4.3.0) ## P munsell 0.5.0 2018-06-12 [?] RSPM (R 4.3.0) ## P ncdf4 * 1.21 2023-01-07 [?] RSPM (R 4.3.0) ## P nlme 3.1-162 2023-01-31 [?] RSPM (R 4.3.0) ## P nnet 7.3-19 2023-05-03 [?] CRAN (R 4.3.2) ## P parallelly 1.36.0 2023-05-26 [?] RSPM (R 4.3.0) ## P pdp 0.8.1 2022-06-07 [?] RSPM (R 4.3.0) ## pillar 1.9.0 2023-03-22 [1] CRAN (R 4.3.2) ## pkgconfig 2.0.3 2019-09-22 [1] CRAN (R 4.3.2) ## P plyr 1.8.9 2023-10-02 [?] RSPM (R 4.3.0) ## P pROC 1.18.5 2023-11-01 [?] RSPM (R 4.3.0) ## P prodlim 2023.08.28 2023-08-28 [?] RSPM (R 4.3.0) ## P proxy 0.4-27 2022-06-09 [?] RSPM (R 4.3.0) ## P purrr * 1.0.2 2023-08-10 [?] RSPM (R 4.3.0) ## R6 2.5.1 2021-08-19 [1] CRAN (R 4.3.2) ## P ranger * 0.15.1 2023-04-03 [?] RSPM (R 4.3.0) ## P RColorBrewer 1.1-3 2022-04-03 [?] RSPM (R 4.3.0) ## Rcpp 1.0.11 2023-07-06 [1] CRAN (R 4.3.2) ## P readr * 2.1.4 2023-02-10 [?] RSPM (R 4.3.0) ## P recipes * 1.0.8 2023-08-25 [?] RSPM (R 4.3.0) ## P renv * 0.16.0 2022-09-29 [?] RSPM (R 4.3.0) ## P repr 1.1.6 2023-01-26 [?] RSPM (R 4.3.0) ## P reshape2 1.4.4 2020-04-09 [?] RSPM (R 4.3.0) ## P rlang 1.1.1 2023-04-28 [?] RSPM (R 4.3.0) ## P rmarkdown * 2.24 2023-08-14 [?] RSPM (R 4.3.0) ## P rpart * 4.1.19 2022-10-21 [?] RSPM (R 4.3.0) ## P rpart.plot * 3.1.1 2022-05-21 [?] RSPM (R 4.3.0) ## P rsample * 1.2.0 2023-08-23 [?] RSPM (R 4.3.0) ## rstudioapi 0.15.0 2023-07-07 [1] CRAN (R 4.3.2) ## sass 0.4.7 2023-07-15 [1] CRAN (R 4.3.2) ## P scales 1.2.1 2022-08-20 [?] RSPM (R 4.3.0) ## P scico * 1.5.0 2023-08-14 [?] RSPM (R 4.3.0) ## P sessioninfo * 1.2.2 2021-12-06 [?] RSPM (R 4.3.0) ## P sf 1.0-14 2023-07-11 [?] RSPM (R 4.3.0) ## P skimr * 2.1.5 2022-12-23 [?] RSPM (R 4.3.0) ## P sp 2.1-1 2023-10-16 [?] RSPM (R 4.3.0) ## stringi 1.7.12 2023-01-11 [1] CRAN (R 4.3.2) ## stringr * 1.5.0 2022-12-02 [1] CRAN (R 4.3.2) ## P survival 3.5-5 2023-03-12 [?] RSPM (R 4.3.0) ## P terra * 1.7-46 2023-09-06 [?] RSPM (R 4.3.0) ## tibble * 3.2.1 2023-03-20 [1] CRAN (R 4.3.2) ## P tidyr * 1.3.0 2023-01-24 [?] RSPM (R 4.3.0) ## P tidyselect 1.2.0 2022-10-10 [?] RSPM (R 4.3.0) ## P tidyverse * 2.0.0 2023-02-22 [?] RSPM (R 4.3.0) ## P timechange 0.2.0 2023-01-11 [?] RSPM (R 4.3.0) ## P timeDate 4022.108 2023-01-07 [?] RSPM (R 4.3.0) ## P tzdb 0.4.0 2023-05-12 [?] RSPM (R 4.3.0) ## P units 0.8-4 2023-09-13 [?] RSPM (R 4.3.0) ## P usethis * 2.2.2 2023-07-06 [?] RSPM (R 4.3.0) ## utf8 1.2.4 2023-10-22 [1] CRAN (R 4.3.2) ## P vctrs 0.6.3 2023-06-14 [?] RSPM (R 4.3.0) ## P vip 0.4.1 2023-08-21 [?] RSPM (R 4.3.0) ## P viridisLite 0.4.2 2023-05-02 [?] RSPM (R 4.3.0) ## P visdat * 0.6.0 2023-02-02 [?] RSPM (R 4.3.0) ## P vroom 1.6.4 2023-10-02 [?] RSPM (R 4.3.0) ## withr 2.5.2 2023-10-30 [1] CRAN (R 4.3.2) ## P xfun 0.40 2023-08-09 [?] RSPM (R 4.3.0) ## yaml 2.3.7 2023-01-23 [1] CRAN (R 4.3.2) ## P yardstick * 1.2.0 2023-04-21 [?] RSPM (R 4.3.0) ## ## [1] /home/runner/work/agds/agds/renv/library/R-4.3/x86_64-pc-linux-gnu ## [2] /home/runner/work/agds/agds/renv/sandbox/R-4.3/x86_64-pc-linux-gnu/95f1b021 ## ## P ── Loaded and on-disk path mismatch. ## ## ────────────────────────────────────────────────────────────────────────────── "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
